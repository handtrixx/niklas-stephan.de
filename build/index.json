[{"date":"1746057600","url":"/blog/oss-mobiles-2025/","title":"Mobiltelefone 2025: Der King in Datenschutz und Nachhaltigkeit","summary":"Ein Ãœberblick Ã¼ber nachhaltige Mobiltelefone 2025, die die Daten des Benutzers schÃ¼tzen.","content":"Ein Ãœberblick Ã¼ber nachhaltige Mobiltelefone 2025, die die Daten des Benutzers schÃ¼tzen.\nStÃ¤ndig werden neue Handys auf den Markt geworfen und von unabhÃ¤ngigen Agenturen, Zeitschriften und Personen getestet. Der Fokus dieser Tests liegt jedoch meist auf der Hardwareleistung des GerÃ¤ts. Beim Betriebssystem wird in 99 % der FÃ¤lle einfach hingenommen, dass auf dem GerÃ¤t sowohl von Google als auch vom Hardwarehersteller eventuell unerwÃ¼nschte und vor allem datenhungrige Applikationen vorinstalliert werden. In vielen FÃ¤llen lassen sich diese dann nicht einmal deinstallieren.\nDeshalb konzentriert sich dieser unabhÃ¤ngige Test ganz auf den Aspekt des Datenschutzes und behandelt die anderen Kriterien nur beilÃ¤ufig. Zuvor rÃ¤umen wir aber noch mit falschen Vorstellungen Ã¼ber den Zusammenhang zwischen Open Source, Linux, Android und Google auf.\nFÃ¼r ungeduldige Leser: Das Vergleichsergebnis Hier die Ergebnisse des Vergleichs im Ãœberblick. Die genaue Beschreibung der einzelnen Handys und die Vergleichstabelle findest du weiter unten.\nGesamtsieger:\nFairphone 5 (30 Punkte) â€“ Beste Kombination aus Datenschutz, Nachhaltigkeit und Alltagstauglichkeit.\nTechnischer Sieger:\nGoogle Pixel 8 Pro (27 Punkte) â€“ Ãœberragende Kamera, Top-Hardware, lange Updates.\nPreis-Leistungs-Sieger:\nXiaomi Poco X3 NFC (24 Punkte) â€“ Sehr gutes Gesamtpaket zum gÃ¼nstigen Preis, mit LineageOS-UnterstÃ¼tzung.\nRanking (Punkte):\nFairphone 5 (30) Volla Phone X23 (28) Google Pixel 8 Pro (27) Volla Quintus (26) Google Pixel 9 (26) OnePlus 11 (25) Motorola Edge 40 Pro (25) Xiaomi Poco X3 NFC (24) Motorola Moto G32 (23) Jolla C2 Community Phone (19) Open Source, Linux, Android und Google Open Source bedeutet nicht automatisch Datenschutz oder Sicherheit. Viele Menschen setzen Open-Source-Software mit sicherer und datenschutzfreundlicher Software gleich, was jedoch nicht immer der RealitÃ¤t entspricht. Es ist wichtig, die Unterschiede und ZusammenhÃ¤nge zwischen Open Source, Linux, Android und den Diensten von Google zu verstehen.\nOpen Source bedeutet zunÃ¤chst einmal, dass der Quellcode einer Software Ã¶ffentlich einsehbar ist. Dadurch kÃ¶nnen Entwickler und Experten den Code Ã¼berprÃ¼fen, Schwachstellen identifizieren und zur Verbesserung beitragen.\nLinux ist ein Open-Source-Betriebssystem, das als Grundlage fÃ¼r viele andere Systeme dient, einschlieÃŸlich Android. Es bietet eine hohe FlexibilitÃ¤t und AnpassungsfÃ¤higkeit, jedoch hÃ¤ngen Datenschutz und Sicherheit stark von der jeweiligen Implementierung und den darauf aufbauenden Diensten ab. Da auch Android auf Linux basiert, erbt es zwar einige der Vorteile von Open Source, ist jedoch stark von den Entscheidungen und Diensten von Google geprÃ¤gt, was den Datenschutz beeinflussen kann.\nDas heiÃŸt: Ein Mobiltelefon, auf dem Android ohne Google-Dienste betrieben wird, kann als deutlich datenschutzfreundlicher betrachtet werden.\nAlternativen zu Android Es gibt natÃ¼rlich auch noch andere Optionen als Android, wie beispielsweise Apples iOS oder andere Linux-basierte Betriebssysteme.\niOS ist jedoch proprietÃ¤r (nicht Open Source) und stark in das Apple-Ã–kosystem eingebunden. Auf Apple-Mobiltelefonen ist es auÃŸerdem nicht mÃ¶glich, alternative Betriebssysteme zu installieren.\nDeshalb stellen in diesem Test Apples Telefone keine Option dar.\nEine weitere MÃ¶glichkeit sind Linux-basierte Betriebssysteme wie /e/OS, LineageOS oder Ubuntu Touch, die ohne Google-Dienste auskommen und den Fokus auf Datenschutz legen.\nHier muss man auch wieder unterscheiden, ob es sich beim Betriebssystem um ein von den Google-Diensten befreites Android handelt, wie das z. B. bei LineageOS der Fall ist, oder um eine eigenstÃ¤ndige Distribution wie z. B. Sailfish OS.\nWÃ¤hrend sich auf den Android-Systemen grundsÃ¤tzlich alle dafÃ¼r entwickelten Anwendungen ausfÃ¼hren lassen und man ggf. nur auf einen nicht von Google betriebenen App-Store ausweichen muss, sieht die Sache bei einer eigenstÃ¤ndigen Distribution anders aus.\nDa sich aktuell kaum Entwickler an den Entwicklungen zu eigenstÃ¤ndigen Distributionen beteiligen und diese auch nur ein winziges Zielpublikum bedienen, herrscht bei der Auswahl von Anwendungen grÃ¶ÃŸtenteils gÃ¤hnende Leere. An dieser Situation wird sich auch mittelfristig nichts Ã¤ndern, weshalb wir diese in unserem Test ebenfalls ignorieren werden.\nTestkriterien Die Bewertung basiert auf folgenden Kriterien (jeweils 0â€“5 Punkte):\nGoogle-freies Android ab Werk oder ohne Funktionsverlust installierbar ZeitgemÃ¤ÃŸe Hardware (CPU, RAM, Speicher) Nachhaltigkeit (Austauschbarkeit von Komponenten) KameraqualitÃ¤t AkkukapazitÃ¤t Maximal sind 30 Punkte erreichbar.\nDie Testkandidaten im Ãœberblick Fairphone 5 Das Fairphone 5 ist das nachhaltigste GerÃ¤t im Test. Es Ã¼berzeugt mit modularer Bauweise, langer ErsatzteilverfÃ¼gbarkeit und einem wechselbaren Akku. Die Hardware ist solide, die Kamera gut, und es gibt offiziellen Support fÃ¼r alternative Android-Versionen wie /e/OS und LineageOS. Wer Wert auf Datenschutz und Reparierbarkeit legt, findet hier die beste Option â€“ auch wenn der Preis im oberen Bereich liegt.\nTechnische Daten:\nBetriebssystem: Android (LineageOS-Support)\nDisplay: 6,46\u0026quot;, 2770x1224 Pixel\nKamera hinten: 50 MP\nKamera vorne: 50 MP\nSoC: Qualcomm QCM6490, 8 Kerne\nRAM: 8 GB\nSpeicher: 256 GB, microSD-Slot\nAkku: 4200 mAh, wechselbar\nBesonderheiten: IP55, MIL-STD-810H, Stereo-Lautsprecher\nPreis: 578 â‚¬\nVolla Phone X23 Das Volla Phone X23 kommt ab Werk mit einem Google-freien Android (Volla OS) und bietet einen wechselbaren Akku sowie robuste Bauweise (IP68, MIL-STD-810H). Die Hardware ist solide, die Kamera gut, und der Preis ist fair. FÃ¼r Datenschutz-Fans, die ein robustes und alltagstaugliches GerÃ¤t suchen, eine sehr gute Wahl.\nTechnische Daten:\nBetriebssystem: Volla OS\nDisplay: 6,1\u0026quot;, 1560x720 Pixel\nKamera hinten: 48 MP + 8 MP\nKamera vorne: 16 MP\nSoC: MediaTek Helio G99, 8 Kerne\nRAM: 6 GB\nSpeicher: 128 GB, microSD-Slot\nAkku: 5000 mAh, wechselbar\nBesonderheiten: IP68, MIL-STD-810H\nPreis: 564 â‚¬\nGoogle Pixel 8 Pro Das Pixel 8 Pro bietet Top-Hardware, exzellente Kamera und lange Update-Garantie. Dank LineageOS-Support lÃ¤sst sich ein Google-freies Android installieren, allerdings ist der Weg dahin nicht ganz trivial und der Akku ist nicht wechselbar. Wer Wert auf Kamera und Performance legt und bereit ist, das System umzubauen, bekommt hier ein sehr gutes Gesamtpaket.\nTechnische Daten:\nBetriebssystem: Android 15 (Update), LineageOS-Support\nDisplay: 6,7\u0026quot;, 2992x1344 Pixel\nKamera hinten: 50 MP + 48 MP + 48 MP\nKamera vorne: 10,5 MP\nSoC: Google Tensor G3, 9 Kerne\nRAM: 12 GB\nSpeicher: 128 GB\nAkku: 5050 mAh, fest verbaut\nBesonderheiten: IP68, 7 Jahre Updates\nPreis: 545 â‚¬\nVolla Quintus Das Volla Quintus ist das Flaggschiff von Volla, kommt ebenfalls mit Google-freiem Android und sehr guter Hardware. Der Akku ist wechselbar, das Display groÃŸ und hell. Der Preis ist allerdings recht hoch, was das Preis-Leistungs-VerhÃ¤ltnis etwas schmÃ¤lert.\nTechnische Daten:\nBetriebssystem: Volla OS (Android)\nDisplay: 6,78\u0026quot;, 2400x1080 Pixel, AMOLED, 120 Hz\nKamera hinten: 50 MP + 8 MP + 2 MP\nKamera vorne: 15,9 MP\nSoC: MediaTek Dimensity 7050, 8 Kerne\nRAM: 8 GB\nSpeicher: 256 GB\nAkku: 4600 mAh, wechselbar\nBesonderheiten: 5G, AMOLED, kabelloses Laden\nPreis: 719 â‚¬\nGoogle Pixel 9 Das Pixel 9 ist Ã¤hnlich stark wie das 8 Pro, etwas kompakter und gÃ¼nstiger. Auch hier gibt es lange Updates und LineageOS-Support. Die Kamera ist hervorragend, der Akku fest verbaut. FÃ¼r alle, die ein aktuelles, leistungsstarkes GerÃ¤t mit Custom-ROM-Option suchen, eine sehr gute Wahl.\nTechnische Daten:\nBetriebssystem: Android 15 (Update), LineageOS-Support\nDisplay: 6,3\u0026quot;, 2424x1080 Pixel, AMOLED\nKamera hinten: 50 MP + 48 MP\nKamera vorne: 10,5 MP\nSoC: Google Tensor G4, 8 Kerne\nRAM: 12 GB\nSpeicher: 128 GB\nAkku: 4700 mAh, fest verbaut\nBesonderheiten: IP68, 7 Jahre Updates\nPreis: 569 â‚¬\nOnePlus 11 Das OnePlus 11 bietet Top-Hardware und ist fÃ¼r Custom-ROMs wie LineageOS geeignet. Der Akku ist allerdings nicht wechselbar und die Nachhaltigkeit ist nur durchschnittlich. Wer Wert auf Performance und ein gutes Preis-Leistungs-VerhÃ¤ltnis legt, findet hier ein starkes GerÃ¤t.\nTechnische Daten:\nBetriebssystem: Android (LineageOS-Support)\nDisplay: 6,7\u0026quot;, 3216x1440 Pixel\nKamera hinten: 50 MP\nKamera vorne: 16 MP\nSoC: Snapdragon 8 Gen 2, 8 Kerne\nRAM: 8 GB\nSpeicher: 128 GB\nAkku: 5000 mAh, fest verbaut\nBesonderheiten: IP64, Stereo-Lautsprecher\nPreis: 538 â‚¬\nMotorola Edge 40 Pro Das Edge 40 Pro ist ein leistungsstarkes GerÃ¤t mit sehr guter Kamera und LineageOS-Support. Der Akku ist fest verbaut, die Reparierbarkeit eingeschrÃ¤nkt. FÃ¼r Power-User, die ein Custom-ROM nutzen wollen, eine interessante Option.\nTechnische Daten:\nBetriebssystem: Android (LineageOS-Support)\nDisplay: 6,67\u0026quot;, 2400x1080 Pixel\nKamera hinten: 50 MP\nKamera vorne: 60 MP\nSoC: Snapdragon 8 Gen 2, 8 Kerne\nRAM: 12 GB\nSpeicher: 256 GB\nAkku: 4600 mAh, fest verbaut\nBesonderheiten: IP68, kabelloses Laden\nPreis: 499 â‚¬\nXiaomi Poco X3 NFC Das Poco X3 NFC ist ein gÃ¼nstiges, aber leistungsfÃ¤higes GerÃ¤t mit LineageOS-Support. Die Installation eines Google-freien Systems ist mÃ¶glich, aber etwas umstÃ¤ndlich. Die Hardware ist solide, die Kamera gut, der Akku groÃŸ. FÃ¼r SparfÃ¼chse mit Datenschutz-Anspruch eine sehr gute Wahl.\nTechnische Daten:\nBetriebssystem: Android 11 (Update), LineageOS-Support\nDisplay: 6,67\u0026quot;, 2400x1080 Pixel, 120 Hz\nKamera hinten: 64 MP + 13 MP + 2 MP + 2 MP\nKamera vorne: 20 MP\nSoC: Snapdragon 732G, 8 Kerne\nRAM: 6 GB\nSpeicher: 128 GB, microSD-Slot\nAkku: 5160 mAh, fest verbaut\nBesonderheiten: IP53, Stereo-Lautsprecher\nPreis: ca. 250 â‚¬\nMotorola Moto G32 Das Moto G32 ist das gÃ¼nstigste GerÃ¤t im Test und bietet solide Hardware fÃ¼r den Alltag. Es gibt LineageOS-Support, aber die Kamera und Performance sind nur durchschnittlich, und der Akku ist nicht wechselbar. FÃ¼r SparfÃ¼chse, die Wert auf Datenschutz legen, dennoch eine brauchbare Wahl.\nTechnische Daten:\nBetriebssystem: Android 12 (ab Werk), LineageOS-Support\nDisplay: 6,5\u0026quot;, 2400x1080 Pixel\nKamera hinten: 50 MP + 8 MP + 2 MP\nKamera vorne: 16 MP\nSoC: Snapdragon 680, 8 Kerne\nRAM: 6 GB\nSpeicher: 128 GB, microSD-Slot\nAkku: 5000 mAh, fest verbaut\nBesonderheiten: UKW-Radio, IP52\nPreis: 421 â‚¬\nJolla C2 Community Phone Das Jolla C2 Community Phone setzt auf Sailfish OS, ein Linux-basiertes System. Die Hardware ist solide, aber die Akkulaufzeit schwach, kein 5G, und die App-Auswahl ist sehr eingeschrÃ¤nkt. FÃ¼r experimentierfreudige Nutzer mit Fokus auf Open Source, aber im Alltag nur bedingt empfehlenswert.\nTechnische Daten:\nBetriebssystem: Sailfish OS\nDisplay: 6,52\u0026quot;, 1600x720 Pixel\nKamera hinten: 64 MP\nKamera vorne: 16 MP\nSoC: ARM Cortex, 8 Kerne\nRAM: 8 GB\nSpeicher: 128 GB, microSD-Slot\nAkku: 4000 mAh, fest verbaut\nBesonderheiten: Kein GPS, kein 5G\nPreis: 284 â‚¬ + 60 â‚¬/Jahr ab dem 2. Jahr\nDie Bewertungstabelle im Detail Kriterium Fairphone 5 Volla X23 Pixel 8 Pro Volla Quintus Pixel 9 OnePlus 11 Edge 40 Pro Poco X3 NFC Moto G32 Jolla C2 Google-freies Android (ab Werk/Installierbar) 5 5 3 5 3 3 3 3 3 5 Hardware (CPU, RAM, Speicher) 4 4 5 5 5 5 5 4 3 3 Nachhaltigkeit (Austauschbarkeit) 5 4 2 3 2 2 2 2 2 2 KameraqualitÃ¤t 4 4 5 4 5 5 5 4 3 2 AkkukapazitÃ¤t 4 5 5 4 4 5 4 5 5 2 Gesamtpunkte 30 28 27 26 26 25 25 24 23 19 Legende:\n5 = erfÃ¼llt das Kriterium voll und ganz\n4 = sehr gut, kleine EinschrÃ¤nkungen\n3 = durchschnittlich / akzeptabel\n2 = unterdurchschnittlich / grÃ¶ÃŸere EinschrÃ¤nkungen\n1 = ungenÃ¼gend\nFazit Das Fairphone 5 ist die beste Wahl fÃ¼r alle, die Datenschutz, Nachhaltigkeit und Alltagstauglichkeit kombinieren wollen. Die Volla Phones sind fÃ¼r Puristen und Datenschutz-Fans sehr attraktiv. Die Pixel-GerÃ¤te bieten die beste Kamera und lange Updates, erfordern aber Eigeninitiative fÃ¼r ein Google-freies System. OnePlus und Motorola sind gute Allrounder, wenn Custom-ROMs und Preis-Leistung im Vordergrund stehen. Das Xiaomi Poco X3 NFC ist der Preis-Leistungs-Tipp fÃ¼r alle, die mit kleinen Abstrichen leben kÃ¶nnen. Das Jolla C2 ist ein spannendes Experiment, aber fÃ¼r den Alltag nur eingeschrÃ¤nkt geeignet.\nLinks:\nUbuntu Touch GerÃ¤teÃ¼bersicht\nLineageOS\n","tags":["Open Source","Hardware"],"section":"blog"},{"date":"1745280000","url":"/blog/perfect-website-2025/","title":"Herr Doktor, ich habe Wordpress, kann man da noch etwas machen?","summary":"Ein umfassender Vergleich moderner CMS und Static Site Generators fÃ¼r 2025, erstellt von Niklas Stephan und seinem KI-Assistenten","content":"Ein umfassender Vergleich moderner CMS und Static Site Generators fÃ¼r 2025, erstellt von Niklas Stephan und seinem KI-Assistenten\nDie Kurzfassung: Das Ranking der besten CMS und Static Site Generators Nach umfassender Analyse von 12 Kriterien haben wir klare Gewinner identifiziert:\nHugo (52 Punkte) - Der Performance-Champion Ghost (50 Punkte) - Die ausgewogene Alternative Jekyll (49 Punkte) - Der Sicherheits-Experte WordPress landet mit nur 38 Punkten auf Platz 9 - warum? Das erfahren Sie in unserer Detailanalyse.\nDie Vergleichstabelle im Detail Kriterium Hexo Grav Strapi Cockpit Ghost Publii WordPress Hugo Jekyll Joomla Typ(en) â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜… Backend â˜…â˜†â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜†â˜†â˜†â˜† â˜…â˜†â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜… Frontend â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜†â˜† Dateibasiert â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜†â˜†â˜†â˜† â˜…â˜…â˜†â˜†â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜†â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜†â˜†â˜†â˜† Docker ready â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜† PopularitÃ¤t â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† Open Source â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… Moderne Codebase â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜†â˜†â˜† Aktueller Code â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† Security â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜†â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜†â˜†â˜†â˜† Performance â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜†â˜†â˜† Clean Frontend â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜†â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜…â˜…â˜…â˜† â˜…â˜†â˜†â˜†â˜† â˜…â˜…â˜…â˜…â˜… â˜…â˜…â˜…â˜…â˜… â˜…â˜†â˜†â˜†â˜† Was bedeuten die Bewertungskriterien? Typ(en): Vielseitigkeit des Systems (SSG, Headless CMS, klassisches CMS) Backend: QualitÃ¤t des Admin-Panels, API und Benutzerverwaltung Frontend: FlexibilitÃ¤t bei Themes, Templates und API-Ausgabe Dateibasiert: Arbeitet mit Dateien statt Datenbank (Markdown, Flat-File) Docker ready: Einfachheit der Container-Bereitstellung PopularitÃ¤t: GrÃ¶ÃŸe der Community und Verbreitung Open Source: VollstÃ¤ndige Quelloffenheit Moderne Codebase: Einsatz aktueller Technologien und Best Practices Aktueller Code: AktivitÃ¤t der Entwicklung und Updates Security: Sicherheit gegen Angriffe und Exploits Performance: Ladegeschwindigkeit und Ressourceneffizienz Clean Frontend: QualitÃ¤t des generierten HTML/CSS/JS-Codes Die drei Systemkategorien im Vergleich Static Site Generators: Die Performance-Champions Vorteile:\nHerausragende Sicherheit (keine Datenbank = weniger AngriffsflÃ¤che) Blitzschnelle Ladezeiten durch statische Dateien Sauberer, schlanker Code ohne Bloat KostengÃ¼nstigeres Hosting mÃ¶glich Top-Empfehlungen:\nHugo (52 Punkte): Blitzschnell, sicher, mit sauberem Code-Output Jekyll (49 Punkte): Der Klassiker mit hervorragender Sicherheit Hexo (45 Punkte): Flexibler Generator mit Node.js-Basis Nachteile: SchwÃ¤cheres Backend, erfordert mehr technisches Know-how\nModerne CMS: Die ausgewogene Mitte Vorteile:\nBalance zwischen Benutzerfreundlichkeit und technischer QualitÃ¤t Moderne Codebasis mit aktuellen Technologien Bessere Sicherheit als traditionelle CMS Top-Empfehlungen:\nGhost (50 Punkte): Perfekt ausbalanciert, ohne signifikante SchwÃ¤chen Strapi (47 Punkte): FÃ¼hrendes Headless-CMS mit exzellentem Backend Grav (47 Punkte): Dateibasiertes CMS mit modernem Ansatz Nachteile: Teilweise geringere Verbreitung und kleineres Plugin-Ã–kosystem\nTraditionelle CMS: Die PopularitÃ¤ts-Riesen Vorteile:\nRiesiges Ã–kosystem an Plugins und Themes Umfangreiche Community und Support Einfacher Einstieg fÃ¼r AnfÃ¤nger Vertreter:\nWordPress (38 Punkte): MarktfÃ¼hrer mit groÃŸem Ã–kosystem Joomla (37 Punkte): Alternative mit Ã¤hnlichen Eigenschaften Nachteile: Erhebliche Sicherheitsprobleme, schlechte Performance, unsauberer Code\nDie StÃ¤rken und SchwÃ¤chen der Top-Systeme Hugo: Der technische Spitzenreiter StÃ¤rken:\nBlitzschnelle Generierung (bis zu 100x schneller als andere SSGs) Hervorragende Sicherheit durch statische Dateien Extrem sauberer Output-Code Keine Datenbank nÃ¶tig SchwÃ¤chen:\nMinimalistisches Backend (1â˜…) Steilere Lernkurve fÃ¼r Einsteiger Ghost: Die ausgewogene Alternative StÃ¤rken:\nModernes, intuitives Backend Hervorragende Editor-Erfahrung Gute Performance und Sicherheit Keine signifikanten SchwÃ¤chen (mindestens 3â˜… in allen Kategorien) SchwÃ¤chen:\nGeringere Verbreitung als WordPress Weniger Themes und Plugins verfÃ¼gbar WordPress: Der populÃ¤re Problemfall StÃ¤rken:\nGrÃ¶ÃŸtes Ã–kosystem an Plugins und Themes HÃ¶chste Verbreitung und Community Einfacher Einstieg fÃ¼r AnfÃ¤nger SchwÃ¤chen:\nKritische Sicherheitsprobleme (1â˜…) Schlechte Performance (2â˜…) Unsauberer Frontend-Code (1â˜…) Veraltete Codebasis (2â˜…) Fazit: Zeit fÃ¼r einen Wechsel? Die Analyse zeigt deutlich: Moderne Alternativen bieten erhebliche technische Vorteile gegenÃ¼ber WordPress und Joomla. Besonders in den kritischen Bereichen Sicherheit, Performance und Code-QualitÃ¤t fallen die traditionellen CMS deutlich ab.\nUnsere Empfehlungen:\nFÃ¼r neue Projekte: Setzen Sie auf Hugo, Jekyll oder Ghost FÃ¼r bestehende WordPress-Seiten: ErwÃ¤gen Sie eine Migration zu Ghost oder Grav FÃ¼r komplexe Anwendungen mit API-Bedarf: Strapi bietet ein hervorragendes Backend Die Entscheidung sollte letztlich von Ihren spezifischen Anforderungen abhÃ¤ngen - aber eines ist klar: Es gibt heute bessere Alternativen als WordPress. Ich selbst habe https://niklas-stephan.de inzwischen auf Hugo migriert und werde dazu eventuell in einem separaten Beitrag detailliert berichten.\nHaben Sie Fragen oder Anmkerungen zur Migration von WordPress zu einer modernen Alternative? Nutzen Sie die Kommentarfunktion um Ihre Gedanken mit uns zu teilen.\n","tags":["deutsch"],"section":"blog"},{"date":"1742342400","url":"/blog/distrobox/","title":"Distrobox - oder: \"Noch unabhÃ¤ngiger in Linux\"","summary":"Wenn du irgendwann genauso wie ich die Nase voll von MS Windows und Apple macOS hast, bleibt Linux als beliebteste und mittlerweile absolut konkurrenzfÃ¤hige Alternative. Und schon stehst du vor der nÃ¤chsten Entscheidung: Welches Linux darf es denn sein? Aber was wÃ¤re, wenn man einfach \u0026ldquo;jede\u0026rdquo; Art von Linux auf einmal laufen lassen kÃ¶nnte? Genau das verspricht Distrobox. Dieser Post geht kurz auf die Unterschiede zwischen Linux-Kernel, Distribution und Derivation ein und zeigt dann, wie man Distrobox installiert und konfiguriert.\n","content":"Wenn du irgendwann genauso wie ich die Nase voll von MS Windows und Apple macOS hast, bleibt Linux als beliebteste und mittlerweile absolut konkurrenzfÃ¤hige Alternative. Und schon stehst du vor der nÃ¤chsten Entscheidung: Welches Linux darf es denn sein? Aber was wÃ¤re, wenn man einfach \u0026ldquo;jede\u0026rdquo; Art von Linux auf einmal laufen lassen kÃ¶nnte? Genau das verspricht Distrobox. Dieser Post geht kurz auf die Unterschiede zwischen Linux-Kernel, Distribution und Derivation ein und zeigt dann, wie man Distrobox installiert und konfiguriert.\nKernel, Distribution und Derviation Hier die Unterschiede kurz erklÃ¤rt, was hilft, den Rest des Artikels besser zu verstehen (Quelle: GPT-4o mini, Prompt: \u0026ldquo;Was ist der Unterschied in Linux zwischen: Kernel, Distribution und Derivation?\u0026rdquo;, Datum: 27.03.2025).\nKernel Der Kernel ist der zentrale Bestandteil eines Betriebssystems. Er ist die Schnittstelle zwischen der Hardware und der Software und verwaltet die Systemressourcen wie CPU, Speicher und GerÃ¤te. Der Linux-Kernel ist der Kern des Linux-Betriebssystems und wird stÃ¤ndig weiterentwickelt. Er ist verantwortlich fÃ¼r die AusfÃ¼hrung von Prozessen, das Management von Speicher und die Kommunikation mit Hardwarekomponenten.\nDistribution Eine Linux-Distribution ist eine vollstÃ¤ndige Sammlung von Software, die auf dem Linux-Kernel basiert. Sie umfasst den Kernel selbst sowie eine Vielzahl von Softwarepaketen, die fÃ¼r die Nutzung des Systems erforderlich sind, wie z.B. Desktop-Umgebungen, Anwendungssoftware, Systemwerkzeuge und Treiber. Beispiele fÃ¼r beliebte Linux-Distributionen sind:\nUbuntu, Fedora, Debian, Arch Linux. Jede Distribution hat ihre eigenen Merkmale, Paketverwaltungssysteme und Zielgruppen.\nDerivation Derivation (oder abgeleitete Distribution) bezieht sich auf eine Distribution, die auf einer anderen Distribution basiert. Sie nutzt den Kernel und die Softwarepakete der ursprÃ¼nglichen Distribution, fÃ¼gt jedoch eigene Anpassungen, zusÃ¤tzliche Software oder spezifische Konfigurationen hinzu. Ein Beispiel fÃ¼r eine Derivation ist Linux Mint, das auf Ubuntu basiert, aber eine andere BenutzeroberflÃ¤che und zusÃ¤tzliche Funktionen bietet. Derivationen kÃ¶nnen oft als benutzerfreundlicher oder spezialisierter angesehen werden, da sie auf den StÃ¤rken der Basisdistribution aufbauen.\nUnd warum nun Distrobox? Distrobox zu nutzen, kann aus mehreren GrÃ¼nden nÃ¼tzlich oder sogar sinnvoll sein:\nIch verwende Distribution X, kenne mich aber in Distribution Y besser aus. Einige Tools in der Linux-Welt sind auf eine bestimmte Linux-Distribution \u0026ldquo;gemÃ¼nzt\u0026rdquo;. Ein prominentes Beispiel hierfÃ¼r sind die Paketmanager. In Fedora und Co. hat sich da dnf durchgesetzt, wÃ¤hrend unter Ubuntu und Co. apt Ã¼blich ist. Mit Distrobox kannst du einfach beide verwenden.\nDeine Wunsch-App gibt es nur fÃ¼r Distribution Y, aber du nutzt Distribution X. Dann installiere sie doch einfach Ã¼ber Distrobox ğŸ˜Š.\nEin Container oder gar eine VM wÃ¤re mit \u0026ldquo;Kanonen auf Spatzen\u0026rdquo; geschossen. Zum Ausprobieren empfiehlt es sich nach wie vor eher, mit einer vollstÃ¤ndigen Container-Management-Software wie Docker zu arbeiten, aber eine tiefgehende Integration in dein Betriebssystem lÃ¤sst sich einfacher mit Distrobox umsetzen.\nWeil es lustig ist. Ja, zumindest finde ich es lustig, auch auf einem Desktop zu sehen, dass alle Linux-Distributionen auf dem gleichen Kernel basieren und im Endeffekt nur ein GerÃ¼st darstellen.\nInstallation von Distrobox Die Installationspakete fÃ¼r Distrobox sind mittlerweile in allen groÃŸen Distributionen integriert und stellen die einfachste Variante dar, Distrobox zu installieren. Die Installation selbst kÃ¶nnt ihr entweder Ã¼ber die grafische OberflÃ¤che eurer Paketverwaltung durchfÃ¼hren oder wie folgt Ã¼ber die Kommandozeile.\nFÃ¼r Fedora / CentOs / RedHat:\n1sudo dnf install distrobox FÃ¼r Debian / Ubuntu:\n1sudo apt-get install distrobox Alternativ, z.B. wenn du eine Distribution nutzt, die nur eine Ã¤ltere Version von Distrobox anbietet, kannst du die Installation auch mit:\n1curl -s https://raw.githubusercontent.com/89luca89/distrobox/main/install | sudo sh starten. Denke dann aber daran, dass du fÃ¼r Updates selbst zustÃ¤ndig bist (diese startest du einfach durch einen erneuten Aufruf des zuvor genannten Befehls).\nErstellen eines neuen Containers Das Erstellen von Containern geht mit einem simplen Befehl auf der Kommandozeile vonstatten, z.B. so:\n1distrobox create --name ubuntu_container --image ubuntu:latest In diesem Fall wÃ¼rde ein neuer Container mit dem Namen \u0026ldquo;ubuntu_container\u0026rdquo; und der neuesten Ubuntu-Version angelegt werden.\nZwischen den Containern hin und her wechseln Um einen Container wie den im Beispiel zuvor erstellten \u0026ldquo;ubuntu_container\u0026rdquo; zu betreten, fÃ¼hrt man folgendes Kommando aus:\n1distrobox enter ubuntu_container Um einen Container wieder zu verlassen, reicht ein simples\n1exit Denke daran, den Container erst zu verlassen, bevor du in einen anderen springst.\nWeitere nÃ¼tzliche Befehle Alle vorhandenen Container kannst du dir mit folgendem Befehl anzeigen lassen:\n1distrobox list Einen laufenden Container wie unseren \u0026ldquo;ubuntu_container\u0026rdquo; kannst du mit folgendem Befehl stoppen:\n1distrobox stop ubuntu_container Und um einen Container komplett zu lÃ¶schen, benÃ¶tigen wir folgendes Kommando\n1distrobox rm ubuntu_container Fazit Distrobox macht es maximal einfach, zwischen verschiedenen Linux-Distributionen hin und her zu springen, ohne eine Dual-Boot-Installation, eine virtuelle Maschine oder Docker/Podman-Container anlegen zu mÃ¼ssen. Das macht sogar noch Sinn, wenn du z.B. schon Docker installiert hast, weil die dort verfÃ¼gbaren Images in der Regel fÃ¼r Serversysteme gedacht sind und deshalb fÃ¼r die interaktive Nutzung nicht wirklich taugen.\nBonustipp Wenn du beim betreten der Shell Kommandozeile auch ein cooles OS Logo mit Systeminformationen sehen willst: Das geht mit neofetch. Das Paket kann Ã¼ber alle gÃ¤ngigen Paketmanager installiert werden und heiÃŸt, rate mal, \u0026ldquo;neofetch\u0026rdquo;. FÃ¼r einen automatischen Start nach der Installation einfach neofetch ans ende eurer ~/.bashrc oder ~/.zshrc setzen.\n","tags":["deutsch","linux"],"section":"blog"},{"date":"1740096000","url":"/blog/docker-time-machine/","title":"Apple Time Machine Backup auf einen Netzwerkspeicher mit Linux und Docker Compose","summary":"Brauchen wir 2025 wirklich noch eine spezielle LÃ¶sung fÃ¼r das Backup unseres Rechners? Wir haben doch iCloud/OneDrive/â€¦! â€“ Korrekt, aber dabei handelt es sich um Dateisynchronisation und nicht um Backups! Den Unterschied und wie man mit Hilfe eines Raspberry Pi oder Ã¤hnlichem mit Docker Compose eine Backup LÃ¶sung fÃ¼r seinen Mac aufsetzt, die der original Apple TimeMachine in nichts nachsteht, erklÃ¤re ich in diesem Post.\nGrundbegriffe, Backup vs. Synchronisation Mit der sogenannten TimeMachine hatte Apple es wieder einmal geschafft, einen Service der bei anderen wahnsinnig kompliziert einzurichten ist/war, kinderleicht bedienbar zu machen. Leider scheinen Datensicherungen in das lokale Netz aus der Mode gekommen zu sein, so propagiert Apple selbst nur noch das Backup direkt auf einen USB-Speicher oder eben in die eigene iCloud. Dabei haben beide LÃ¶sungen grÃ¶ÃŸere Nachteile. FÃ¼r das lokale Backup muss ich immer daran denken, dass externe Speichermedium in den Rechner zu stecken und kommt es z.B. zu einem Kurzschluss wÃ¤hrend beide GerÃ¤te miteinander verbunden sind, sind auch beide zerstÃ¶rt. Dahin ist die Datensicherung, genau dann wenn man sie am meisten braucht. Die Alternativen iCloud und Co. begleiten ebenfalls mehrere Nachteile. Zum Einem vertrauen wir unsere Daten einem externen Anbieter an, der uns zwar versprechen kann dass diese dort sicher und geschÃ¼tzt aufbewahrt werden, aber garantieren kann uns das niemand. AuÃŸerdem handelt es sich bei iCloud und Ã¤hnlichen zunÃ¤chst nur um Services zur Dateisynchronisation. D.h. meine Daten werden parallel lokal und eben in der Cloud abgelegt. Das heisst auch, wenn ich eine Datei lokal lÃ¶sche, passiert das gleiche auch in der Cloud. Und wieder Adieu liebe Dateischerung. Dem entgegen wirk die Dateiversionierung die entweder StandardmÃ¤ÃŸig oder optional aktiviert werden kann, allerdings habe ich selbst die Erfahrung gemacht, dass diese auch genau dann wenn man sie brÃ¤uchte gerade mal nicht funktioniert.\n","content":"Brauchen wir 2025 wirklich noch eine spezielle LÃ¶sung fÃ¼r das Backup unseres Rechners? Wir haben doch iCloud/OneDrive/â€¦! â€“ Korrekt, aber dabei handelt es sich um Dateisynchronisation und nicht um Backups! Den Unterschied und wie man mit Hilfe eines Raspberry Pi oder Ã¤hnlichem mit Docker Compose eine Backup LÃ¶sung fÃ¼r seinen Mac aufsetzt, die der original Apple TimeMachine in nichts nachsteht, erklÃ¤re ich in diesem Post.\nGrundbegriffe, Backup vs. Synchronisation Mit der sogenannten TimeMachine hatte Apple es wieder einmal geschafft, einen Service der bei anderen wahnsinnig kompliziert einzurichten ist/war, kinderleicht bedienbar zu machen. Leider scheinen Datensicherungen in das lokale Netz aus der Mode gekommen zu sein, so propagiert Apple selbst nur noch das Backup direkt auf einen USB-Speicher oder eben in die eigene iCloud. Dabei haben beide LÃ¶sungen grÃ¶ÃŸere Nachteile. FÃ¼r das lokale Backup muss ich immer daran denken, dass externe Speichermedium in den Rechner zu stecken und kommt es z.B. zu einem Kurzschluss wÃ¤hrend beide GerÃ¤te miteinander verbunden sind, sind auch beide zerstÃ¶rt. Dahin ist die Datensicherung, genau dann wenn man sie am meisten braucht. Die Alternativen iCloud und Co. begleiten ebenfalls mehrere Nachteile. Zum Einem vertrauen wir unsere Daten einem externen Anbieter an, der uns zwar versprechen kann dass diese dort sicher und geschÃ¼tzt aufbewahrt werden, aber garantieren kann uns das niemand. AuÃŸerdem handelt es sich bei iCloud und Ã¤hnlichen zunÃ¤chst nur um Services zur Dateisynchronisation. D.h. meine Daten werden parallel lokal und eben in der Cloud abgelegt. Das heisst auch, wenn ich eine Datei lokal lÃ¶sche, passiert das gleiche auch in der Cloud. Und wieder Adieu liebe Dateischerung. Dem entgegen wirk die Dateiversionierung die entweder StandardmÃ¤ÃŸig oder optional aktiviert werden kann, allerdings habe ich selbst die Erfahrung gemacht, dass diese auch genau dann wenn man sie brÃ¤uchte gerade mal nicht funktioniert.\nAus diesen und anderen GrÃ¼nden hat Apple noch ein â€TÃ¼rchenâ€œ offen gelassen und stellt es anderen Herstelleren z.B. von Netzwerkspeichern wie QNAP frei, TimeMachine in ihre HardwarelsÃ¶ungen einzubinden, auch wenn Apple selbst keine entsprechende Hardware mehr vertreibt.\nalls ihr aber so wie ich sowieso schon einen Linux Server oder RaspberryPi mit Docker Compose habt, geht es auch noch einfacher: Wir konfigurieren einfach einen Container der als TimeMachine Server im Heimnetz dient!\nServer vorbereiten ZunÃ¤chst verbinden wir unseren externen Speicher, ich selbst verwende eine externe Festplatte, mit unserem Server. Falls nicht schon geschehen, formatieren wir unseren Speicher noch in einem fÃ¼r Linux nativen Dateisystem.\nFestplatte formatieren Um eine externe Festplatte in der Linux-Kommandozeile mit dem Dateisystem ext4 zu formatieren, kannst du die folgenden Schritte befolgen:\nSchlieÃŸe die externe Festplatte an deinen Computer an. Stelle sicher, dass sie erkannt wird und einen zugewiesenen GerÃ¤tepfad hat. Du kannst dies mit dem Befehl lsblk Ã¼berprÃ¼fen, der eine Liste der blockbasierten GerÃ¤te anzeigt. Ã–ffne ein Terminal oder eine Konsole, um die Linux-Kommandozeile zu Ã¶ffnen. Gib den Befehl sudo fdisk -l ein, um eine Liste der erkannten Festplatten und ihrer Partitionen anzuzeigen. Finde den GerÃ¤tepfad deiner externen Festplatte in der Liste. Normalerweise wird sie als â€/dev/sdXâ€œ bezeichnet, wobei â€Xâ€œ fÃ¼r einen Buchstaben steht (z. B. /dev/sdb). Stelle sicher, dass du den richtigen GerÃ¤tepfad auswÃ¤hlst und die Daten auf der Festplatte sicher gesichert hast. Das Formatieren einer Festplatte lÃ¶scht alle darauf befindlichen Daten unwiederbringlich. Gib den folgenden Befehl ein, um das Festplattenformat zu Ã¤ndern und das Dateisystem ext4 zu erstellen: 1sudo mkfs.ext4 /dev/sdX Ersetze â€/dev/sdXâ€œ durch den tatsÃ¤chlichen GerÃ¤tepfad deiner externen Festplatte. 6. Der Befehl wird dich fragen, ob du fortfahren mÃ¶chtest, da er alle Daten auf der Festplatte lÃ¶schen wird. BestÃ¤tige mit â€yâ€œ und drÃ¼cke die Eingabetaste. 7. Der Formatierungsvorgang beginnt und kann je nach GrÃ¶ÃŸe der Festplatte einige Zeit in Anspruch nehmen. 8. Sobald der Vorgang abgeschlossen ist, erhÃ¤ltst du eine BestÃ¤tigungsmeldung.\nDeine externe Festplatte sollte nun erfolgreich mit dem ext4-Dateisystem formatiert sein und bereit fÃ¼r die Verwendung unter Linux sein.\nFestplatte/Speicher dauerhaft mounten Als nÃ¤chstens wollen wir erreichen, dass der Speicher bzw. die Festplatte bei jedem Neustart des Systems automatisch â€gemountetâ€œ (an das System angehangen wird).\nUm eine mit ext4 formatierte Festplatte in der /etc/fstab-Datei zu mounten, kannst du die folgenden Schritte befolgen:\nÃ–ffne ein Terminal oder eine Konsole, um die Linux-Kommandozeile zu Ã¶ffnen. Gib den Befehl sudo blkid ein, um eine Liste der erkannten Festplatten und ihrer UUIDs anzuzeigen. Finde die UUID deiner ext4-formatierten Festplatte in der Liste. Die UUID sieht in etwa so aus: UUID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx. Erstelle einen Ordner, der als Mountpunkt fÃ¼r die Festplatte dienen soll. Du kannst dies mit dem Befehl sudo mkdir \u0026lt;mountpunkt\u0026gt; tun, wobei der Pfad zu dem gewÃ¼nschten Ordner ist. Zum Beispiel: sudo mkdir /mnt/backup . Ã–ffne die /etc/fstab Datei in einem Texteditor mit Root-Berechtigungen. Zum Beispiel: sudo nano /etc/fstab . FÃ¼ge eine neue Zeile am Ende der /etc/fstab Datei hinzu, um die Festplatte zu mounten. Die Syntax lautet: UUID=\u0026lt;UUID\u0026gt; \u0026lt;mountpunkt\u0026gt; ext4 defaults 0 2 . Ersetze durch die UUID deiner Festplatte und mit dem Pfad zum zuvor erstellten Ordner, z.B. UUID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx /mnt/backup ext4 defaults 0 2. Speichere die Ã„nderungen und schlieÃŸe den Texteditor. Um die Festplatte sofort zu mounten, ohne den Computer neu zu starten, gib den Befehl sudo mount -a ein. Dadurch werden die EintrÃ¤ge in der /etc/fstab Datei gelesen und die entsprechenden Festplatten gemountet. Die Festplatte wird nun jedes Mal automatisch beim Systemstart gemountet, indem die Informationen in der /etc/fstab-Datei verwendet werden. Du kannst auf die Dateien und Ordner in der Festplatte Ã¼ber den angegebenen Mountpunkt zugreifen.\nDocker Container mit Compose einrichten Falls ihr Docker und Docker Compose noch nicht installiert habt, ist das vorgehen hierzu hier beschrieben. Wir nutzen das Image mbentley/timemachine vom Docker Hub um die FunktionalitÃ¤t der TimeMachine so nachzustellen, wie es auch die Anbieter von professionellen BackuplÃ¶sungen tun.\nUm den Docker-Container â€mbentley/timemachine:smbâ€œ mit Docker Compose zu starten, erstelle bitte eine compose.yml-Datei mit den entsprechenden Konfigurationen. Hier ist ein Beispiel, das auf dem Image basiert und einen Dienst fÃ¼r den Container definiert:\n1version: \u0026#34;3.3\u0026#34; 2services: 3 timemachine: 4 image: mbentley/timemachine:smb 5 container_name: timemachine 6 hostname: timemachine 7 restart: unless-stopped 8 ports: 9 - 137:137/udp 10 - 138:138/udp 11 - 139:139 12 - 445:445 13 logging: 14 options: 15 max-size: \u0026#34;10m\u0026#34; 16 max-file: \u0026#34;3\u0026#34; 17 environment: 18 TZ: \u0026#39;Europe/Berlin\u0026#39; 19 CUSTOM_SMB_CONF: \u0026#34;false\u0026#34; 20 CUSTOM_USER: \u0026#34;false\u0026#34; 21 DEBUG_LEVEL: \u0026#34;1\u0026#34; 22 HIDE_SHARES: \u0026#34;no\u0026#34; 23 EXTERNAL_CONF: \u0026#34;\u0026#34; 24 MIMIC_MODEL: \u0026#34;TimeCapsule8,119\u0026#34; 25 TM_USERNAME: \u0026#34;timemachine\u0026#34; 26 TM_GROUPNAME: \u0026#34;timemachine\u0026#34; 27 TM_UID: \u0026#34;1000\u0026#34; 28 TM_GID: \u0026#34;1000\u0026#34; 29 PASSWORD: \u0026#34;DEINPASSWORT\u0026#34; 30 SET_PERMISSIONS: \u0026#34;false\u0026#34; 31 SHARE_NAME: \u0026#34;TimeMachine\u0026#34; 32 SMB_INHERIT_PERMISSIONS: \u0026#34;no\u0026#34; 33 SMB_NFS_ACES: \u0026#34;yes\u0026#34; 34 SMB_PORT: \u0026#34;445\u0026#34; 35 SMB_VFS_OBJECTS: \u0026#34;acl_xattr fruit streams_xattr\u0026#34; 36 VOLUME_SIZE_LIMIT: \u0026#34;1 T\u0026#34; 37 WORKGROUP: \u0026#34;WORKGROUP\u0026#34; 38 volumes: 39 - /mnt/backup/timemachine:/opt/timemachine Speichere die Datei als compose.yml. Dieses Beispiel verwendet den Port 445 fÃ¼r den SMB-Zugriff. Es bindet auch das Verzeichnis /mnt/backup/timemachine auf dem Host mit dem Verzeichnis /opt/timemachine im Container, so dass die Backups auf der externene Festplatte gespeichert werden.\nFÃ¼hre dann den folgenden Befehl aus, um den Container zu starten: docker compose up -d. Dadurch wird der Container im Hintergrund gestartet. Die Option -d stellt sicher, dass der Dienst im Hintergrund (detached mode) lÃ¤uft.\nJetzt lÃ¤uft der Container â€mbentley/timemachine:smbâ€œ und ist Ã¼ber die konfigurierten Ports erreichbar. Du kannst dann eine Time Machine-Verbindung zu deinem Docker-Host mit dem entsprechenden SMB-Protokoll herstellen und das Verzeichnis /mnt/backup/timemachine verwenden, um deine Backups zu speichern.\nBackup auf dem Mac konfigurieren Nun haben wir alle vorbereitenden Schritte auf unserem Server abgeschlossen und widmen uns unserem zu sicherenden Mac. Ich habe die Erfahrung gemacht, dass sich ein Backup am leichtesten einrichten lÃ¤sst, wenn man sich zuvor mit dem Netzlaufwerk auf dem Server verbindet. Um dich mit einem SMB-Laufwerk auf deinem Mac zu verbinden, befolge diese Schritte:\nÃ–ffne den Finder auf deinem Mac. Klicke in der MenÃ¼leiste auf â€Gehe zuâ€œ und wÃ¤hle â€Mit Server verbindenâ€¦â€œ (oder verwende die Tastenkombination â€Cmd + Kâ€œ). Gib die Adresse des SMB-Laufwerks in das Eingabefeld ein. Die Adresse kann entweder die IP-Adresse des Servers oder sein Netzwerkname (falls verfÃ¼gbar) sein. Das Format der Adresse lautet smb://\u0026lt;adresse\u0026gt;. Zum Beispiel: smb://192.168.0.100 oder smb://meinserver. Klicke auf â€Verbindenâ€œ. Es wird ein Dialogfenster angezeigt, in dem du deine Anmeldeinformationen eingeben musst. Gib den Benutzernamen und das Passwort fÃ¼r den Zugriff auf das SMB-Laufwerk, so wie in der Docker Compose Datei konfiguriert, ein. Du hast auch die MÃ¶glichkeit, das KÃ¤stchen â€Anmeldeinformationen im SchlÃ¼sselbund speichernâ€œ anzukreuzen, um die Anmeldeinformationen fÃ¼r zukÃ¼nftige Verbindungen zu speichern. Klicke auf â€Verbindenâ€œ. Wenn die Anmeldeinformationen korrekt sind und der Zugriff gewÃ¤hrt wurde, wird das SMB-Laufwerk im Finder angezeigt. Du kannst nun auf die Dateien und Ordner des Laufwerks zugreifen und diese bearbeiten. Das SMB-Laufwerk wird nach der Verbindung im Abschnitt â€Freigabenâ€œ im Finder angezeigt. Du kannst auch ein Lesezeichen fÃ¼r das SMB-Laufwerk erstellen, indem du es zum Finder-Sidebar ziehst. Dadurch wird der Zugriff in Zukunft vereinfacht.\nWenn du die Verbindung trennen mÃ¶chtest, kannst du das SMB-Laufwerk einfach aus der Seitenleiste des Finders ziehen oder mit der rechten Maustaste auf das Laufwerk klicken und â€Verbindung trennenâ€œ auswÃ¤hlen.\nBitte beachte, dass fÃ¼r eine erfolgreiche Verbindung das SMB-Protokoll auf dem SMB-Laufwerk aktiviert sein muss und die Netzwerkeinstellungen des Macs korrekt konfiguriert sein sollten.\nUm ein regelmÃ¤ÃŸiges Backup auf einen Netzwerkspeicher (Network Attached Storage, NAS) unter macOS einzurichten, kannst du Time Machine verwenden. Folge diesen Schritten:\nStelle sicher, dass dein Netzwerkspeicher ordnungsgemÃ¤ÃŸ mit deinem Netzwerk verbunden ist und zugÃ¤nglich ist. Gehe zu â€Systemeinstellungenâ€œ auf deinem Mac und klicke auf â€Time Machineâ€œ. Klicke auf â€Time Machine aktivierenâ€œ. Klicke auf â€Weitere Optionenâ€œ. WÃ¤hle die Option â€Backup-DatentrÃ¤ger auswÃ¤hlenâ€œ. WÃ¤hle im Finder den Netzwerkspeicher aus, den du fÃ¼r das Backup verwenden mÃ¶chtest, und klicke auf â€AuswÃ¤hlenâ€œ. Time Machine wird das Backup-Volume Ã¼berprÃ¼fen und formatieren, falls erforderlich. Nachdem das Volume ausgewÃ¤hlt wurde, kehre zu den Time Machine-Einstellungen zurÃ¼ck. Aktiviere das HÃ¤kchen bei â€Automatische Backupsâ€œ. Du kannst auch den Zeitplan fÃ¼r die Backups anpassen, indem du das HÃ¤kchen bei â€Automatische Backupsâ€œ deaktivierst und dann bestimmte Zeiten festlegst, zu denen die Backups stattfinden sollen. Du kannst auÃŸerdem die Einstellungen fÃ¼r â€Ausgeschlossene Elementeâ€œ anpassen, um bestimmte Ordner oder Dateien von den Backups auszuschlieÃŸen. Sobald du die oben genannten Schritte abgeschlossen hast, wird Time Machine regelmÃ¤ÃŸig automatische Backups auf deinem Netzwerkspeicher durchfÃ¼hren. Beachte, dass die Geschwindigkeit der Backups von der Netzwerkverbindung abhÃ¤ngt und mÃ¶glicherweise lÃ¤nger dauern kann als bei einem direkt angeschlossenen SpeichergerÃ¤t.\nEs ist auch wichtig sicherzustellen, dass der Netzwerkspeicher ordnungsgemÃ¤ÃŸ konfiguriert ist und mit dem Mac kompatibel ist, um die bestmÃ¶gliche Time Machine-UnterstÃ¼tzung zu gewÃ¤hrleisten. Konsultiere die Dokumentation deines NAS-Herstellers fÃ¼r weitere Informationen zur Einrichtung der Time Machine-UnterstÃ¼tzung.\n","tags":["deutsch","macos"],"section":"blog"},{"date":"1722816000","url":"/blog/matomo/","title":"Web Analytics selbstgemacht mit Matomo","summary":"Jeder, der sich mit Internetauftritten beschÃ¤ftigt, hat sicherlich schon von Google Analytics (https://de.wikipedia.org/wiki/Google_Analytics) gehÃ¶rt. Damit kann man u.a. sehen von wo, unter welchen technischen Bedingungen, wann auf welche Seiten eben dieser zugegriffen wird. Matomo (https://matomo.org/) steht nicht fÃ¼r ein Mozarella-Sandwich mit Mayonaise und Tomaten, sondern ist ein Open Source Tool mit dem sich gleiches und teilweise sogar mehr erreichen lÃ¤sst. In diesem Artikel gehe ich auf die Vorteile bei der Nutzung von Matomo ein und liefere auch gleich ein Beispiel, wie es sich schnell nutzen lÃ¤sst.\n","content":"Jeder, der sich mit Internetauftritten beschÃ¤ftigt, hat sicherlich schon von Google Analytics (https://de.wikipedia.org/wiki/Google_Analytics) gehÃ¶rt. Damit kann man u.a. sehen von wo, unter welchen technischen Bedingungen, wann auf welche Seiten eben dieser zugegriffen wird. Matomo (https://matomo.org/) steht nicht fÃ¼r ein Mozarella-Sandwich mit Mayonaise und Tomaten, sondern ist ein Open Source Tool mit dem sich gleiches und teilweise sogar mehr erreichen lÃ¤sst. In diesem Artikel gehe ich auf die Vorteile bei der Nutzung von Matomo ein und liefere auch gleich ein Beispiel, wie es sich schnell nutzen lÃ¤sst.\nDatenkrake â€“ aber besucherfreundlich Der wesentliche Vorteil bei der Nutzung von Matomo besteht darin, dass man es selbst betreiben (https://hub.docker.com/_/matomo) kann, und man so die volle Kontrolle darÃ¼ber hat welche Daten vom Besucher wo gespeichert werden. Ich selbst nutze das Tool nun schon seit vielen Jahren und habe nur positives darÃ¼ber zu berichten. Es lÃ¤sst sich ganz Datenschutzkonform (https://www.it-recht-kanzlei.de/matomo-richtig-verwenden-dsgvo.html) und trotzdem granular einstellen, welche Informationen gespeichert werden. FÃ¼r aussagekrÃ¤ftige Analysen braucht man z.B. nicht die komplette IP-Addresse des Besuchers zu speichern, um nur eine der vielen Optionen zu nennen. Auch setzen Tools wie Google Analytics stark auf den Einsatz von digitalen FingerabdrÃ¼cken (Cookies), dazu gleich mehr. Aber so viel vorab: mit Matomo lÃ¤sst sich der Einsatz von Cookies auf Wunsch sogar komplett deaktivieren!\nWie installiere und benutze ich den Matomo? Die Matomo Cloud â€“ so verdient der Anbieter von Matomo, neben zusÃ¤tzlichen erwerbsfÃ¤higen Plugins, sein Geld. Analog zu Google Analytics, vertraue ich die Daten meiner Benutzer einem Drittanbieter an. Irgendwie ja das, was wir genau nicht wollen.\nIntegriert als Plugin in ein Web Content Management System (WCMS) â€“ FÃ¼r WordPress und andere gibt es eine komplette Installation von Matomo als Plugin, welches sich Ã¼ber den jeweilig verfÃ¼gbaren Markplatz hinzufÃ¼gen lÃ¤sst. Hier haben wir keine MÃ¶glichkeit der Trennung von Webseiten-Content und Analytics. Das ist deshalb auch nicht besonders performant und nur fÃ¼r kleinere Installationen geeignet. AuÃŸerdem geht das natÃ¼rlich nur, wenn man eines der unterstÃ¼tzten WCMS einsetzt.\nSelbst Hosten â€“ Das ist was ich hier beschreibe und mittels einem von Matomo selbst vorgefertigtem Container (https://hub.docker.com/_/matomo) auch ganz einfach geht.\nEine â€œsaubereâ€ MÃ¶glichkeit Matomo zu installieren ist also Ã¼ber Docker Compose Container. Im Appendix habe ich eine entsprechende Beispieldatei angehangen. Was ich hier abermals unterschlage ist die Konfiguration des Reverse-Proxies samt seiner Konfiguration. Wir gehen einfach davon aus, dass ich fÃ¼r meine Subdomain stats.handtrixxx.com mit dem Nginx Reverse Proxy Manager die benÃ¶tigten Einstellungen vorgenommen habe und auch von dort aus ein entsprechendes Zertifikat von Letâ€™s Encrypt bekommen habe.\nNachdem das alles eingestellt und hochgefahren ist (Dauer ca. 20 Minuten), kÃ¶nnen wir uns and der WeboberflÃ¤che von Matomo fÃ¼r die initiale Konfiguration anmelden. Wie das funktioniert, ist ausfÃ¼hrlich und gut hier https://matomo.org/docs/installation/ beschrieben. Dank Docker Container kÃ¶nnen wir den ersten Teil der Anleitung komplett Ã¼berspringen und setzten ungefÃ¤hr an der Stelle â€œSuper Userâ€ an.\nWie binde ich Matomo nun in meine Web-Seite/App ein? Wie, wenn wir auf https://matomo.org/docs/installation/ ein bisschen gescrollt haben, an gleicher Stelle beschrieben lÃ¤sst sich nun der Einsatz auf einer ersten Website durchfÃ¼hren. Damit wird auch deutlich, dass mit einer einzelnen Matomo Installationen auch mehrere Websites/Apps getrennt voneinander oder zusammen anbindbar sind.\nZur Einbindung in eine Website lÃ¤sst sich, wie im Standard vorgeschlagen, entweder ein JavaScript Snippet verwenden oder aber ein â€œunsichtbaresâ€ Bild Ã¼ber den sogenannten Image Tracker einfÃ¼gen. Auf diese Art lÃ¤sst sich nun auch einstellen, dass KEIN Cookie beim Anwender zum Tracking genutzt werden soll. Prima!\nWartung AbschlieÃŸend ein paar Tipps zur Wartung unserer Installation. Wie ihr, falls ihr selbst eine Installation vorgenommen habt, evtl. schon gesehen habt wird die VerfÃ¼gbarkeit eines Updates von Matomo direkt in der Web-GUI angezeigt und kann auch von dort aus durchgefÃ¼hrt werden.\nUm die Container selbst auch aktuell zu halten, reicht das gelegentliche und auch automatisierbare AusfÃ¼hren von docker-compose pull , mit anschlieÃŸendem docker-compose up -d aus dem Stammverzeichnis eurer Docker Compose Umgebung heraus.\nDas ist einfach.\nFazit Definitiv hat das Schreiben dieses Artikels lÃ¤nger gedauert als eine Matomo Installation. AuÃŸerdem konnte ich hoffentlich verdeutlichen, dass es im Vergleich zu anderen Web-Analytic Tools mit Matomo keine wesentlichen EinschrÃ¤nkungen, aber jede Menge Vorteile gibt. Ein kleiner Wermutstropfen ist vielleicht, dass sich die Art der Reports in der Web-GUI nicht besonders â€œverbiegenâ€ lÃ¤sst. Wahrscheinlich wÃ¤re das aber auch nur fÃ¼r grÃ¶ÃŸere Unternehmen Ã¼berhaupt wÃ¼nschenswert. AuÃŸerdem gibt es dafÃ¼r, vielleicht als abschlieÃŸender Lichtblick, eine groÃŸartige API die es uns erlaubt die Ã¼ber Matomo erfassten Daten in einer Applikation unserer Wahl zu verwenden! Siehe: https://developer.matomo.org/api-reference/reporting-api. Eigentlich war der Artikel nur als â€œkurzeâ€ ErgÃ¤nzung zu einem Vorherigen (Mehr als nur Performance King â€“ Headless CMS, APIs und IoT fÃ¼r die Website) gedacht, bei dem es u.a. um das Aufsetzen einer Website mit einem Headless CMS ging. Es ist aber nun doch â€œetwasâ€ lÃ¤nger geworden. Ich bitte die vielen Fachbegriffe zu entschuldigen. Aufgrund des heterogenen Zielpublikums ist die Wortwahl nicht immer ganz einfach.\nAppendix Hier ein Beispiel einer compose.yml zur einfachen Inbetriebnahme von Matomo.\n1services: 2 stats-db: 3 image: mysql:5.7 4 logging: 5 options: 6 max-size: \u0026#34;10m\u0026#34; 7 max-file: \u0026#34;3\u0026#34; 8 container_name: stats-db 9 command: --max-allowed-packet=64MB 10 restart: always 11 networks: 12 - dmz 13 environment: 14 MYSQL_ROOT_PASSWORD: ADMINPASSWORD 15 MYSQL_DATABASE: DBNAME 16 MYSQL_USER: DBUSER 17 MYSQL_PASSWORD: DBUSERPW 18 volumes: 19 - ./db_data:/var/lib/mysql 20 stats-app: 21 depends_on: 22 - stats-db 23 image: matomo:latest 24 logging: 25 options: 26 max-size: \u0026#34;10m\u0026#34; 27 max-file: \u0026#34;3\u0026#34; 28 restart: always 29 networks: 30 - dmz 31 environment: 32 MATOMO_DATABASE_HOST: stats-db 33 MATOMO_DATABASE_ADAPTER: mysql 34 MATOMO_DATABASE_TABLES_PREFIX: matomo_ 35 MATOMO_DATABASE_USERNAME: DBUSER 36 MATOMO_DATABASE_PASSWORD: DBUSERPW 37 MATOMO_DATABASE_DBNAME: DBNAME 38 volumes: 39 - ./data:/var/www/html 40volumes: 41 db_data: {} 42networks: 43 dmz 44 external: true ","tags":["deutsch"],"section":"blog"},{"date":"1720051200","url":"/blog/mythos-motivation/","title":"Mythos Motivation ***","summary":"Du denkst ein Mitarbeiter muss nur â€richtig motiviertâ€œ werden um dauerhaft HÃ¶chstleistungen zu erbringen? Dann lies dieses Buch um zu verstehen, warum diese Vorstellung weit ab jeglicher RealitÃ¤t liegt. Reinhard Sprenger stellt in vielen Beispielen und FÃ¤llen mit Praxisbezug dar, warum motivierte Mitarbeiter nicht einfach gezÃ¼chtet werden kÃ¶nnen. Als das Buch 1991 erschien, muss es wohl dem ein oder anderen Manager ganz neue Erkenntnisse geliefert haben. Auch heute, oder vielleicht gerade heute, ist das Thema aktuell und der Mythos noch nicht ist der Welt geschafft. Einige der Beispiele passen nur nicht mehr so recht in die heutige Welt und insgesamt kam mir das Buch etwas zu langatmig vor.\n","content":"Du denkst ein Mitarbeiter muss nur â€richtig motiviertâ€œ werden um dauerhaft HÃ¶chstleistungen zu erbringen? Dann lies dieses Buch um zu verstehen, warum diese Vorstellung weit ab jeglicher RealitÃ¤t liegt. Reinhard Sprenger stellt in vielen Beispielen und FÃ¤llen mit Praxisbezug dar, warum motivierte Mitarbeiter nicht einfach gezÃ¼chtet werden kÃ¶nnen. Als das Buch 1991 erschien, muss es wohl dem ein oder anderen Manager ganz neue Erkenntnisse geliefert haben. Auch heute, oder vielleicht gerade heute, ist das Thema aktuell und der Mythos noch nicht ist der Welt geschafft. Einige der Beispiele passen nur nicht mehr so recht in die heutige Welt und insgesamt kam mir das Buch etwas zu langatmig vor.\nMeine Bewertung: 3 von 5 Sternen\nInhalt Das Sachbuch ist in drei Teile gegliedert. Es beginnt mit einem analytischen Teil, in dem die oft falsche Grundeinstellung von FÃ¼hrungskrÃ¤ften gegenÃ¼ber ihren Mitarbeiter und deren Motivation und BeweggrÃ¼nden beschrieben wird. Z.B. der Grundverdacht, dass ein Mitarbeiter von sich aus gar nicht 100% seiner mÃ¶glichen Leistung bringt, sondern da durch Incentives oder sonstige Instrumente immer noch â€was gehtâ€œ. Im zweiten Teil geht es darum, dass Verhalten sowohl von FÃ¼hrungskrÃ¤ften als auch Mitarbeitern zu beleuchten. So wird z.B. der Mitarbeiter der weiss, das es fÃ¼r â€Mehrleistungâ€œ regelmÃ¤ÃŸig Boni gibt, von sich aus dann wirklich immer erstmal nur soviel â€leistenâ€œ wie von ihm erwartet und tunlichst darauf achten jegliches Plus mit dem Boni zu verknÃ¼pfen. So wird aus dem Bonus der Normalzustand und die Motivation des Mitarbeiters in keinster Weise gesteigert. Der dritte Teil beschÃ¤ftigt sich dann intensiv damit, wie man konstruktiv fÃ¼hrt. Ein groÃŸer Abschnitt ist dem Thema Demotivation zu vermeiden gewidmet, da nach Meinung des Autors hier der grÃ¶ÃŸte Hebel eines Vorgesetzten liegt.\nWeitere Infos Genre: Sachbuch Erscheinungsjahr: 1991 Seitenzahl: 311 ISBN: 978-3-593-51485-7 ErhÃ¤ltlich in jeder Buchhandlung oder unter: https://www.medimops.de/sprenger-reinhard-k-mythos-motivation-wege-aus-einer-sackgasse-gebundene-ausgabe-M03593501562.html\n","tags":["deutsch","BÃ¼cher"],"section":"blog"},{"date":"1717372800","url":"/blog/a-scanner-darkly/","title":"Der dunkle Schirm ****","summary":"Wie bei vielen der BÃ¼cher von Philip K. Dick ist den meisten die spÃ¤tere Verfilmung \u0026ldquo;A Scanner Darkly\u0026rdquo; eher ein Begriff. Aber auch Kenner des Films kÃ¶nnen Freude am Buch haben, da es viele Szenen enthÃ¤lt die so im Film nicht vorkommen und den Charakter der einzelnen Akteure tiefer verdeutlichen. Insgesamt handelt es sich um eine sehr dÃ¼stere Geschichte in einer Welt in der der Unterschied zwischen legalen, illegalen Drogen und der Welt dazwischen zu einer bis auf die Wurzeln gespalteten Gesellschaft gefÃ¼hrt hat. Es wird behauptet das Dick wÃ¤hrend des schreibens selbst in grÃ¶ÃŸeren Mengen mit Drogen \u0026ldquo;experimentiert\u0026rdquo; hat, auf jeden Fall kommt beim Lesen nie wirklich gute Laune auf, sondern eher Mitleid fÃ¼r die Protagonisten.\n","content":"Wie bei vielen der BÃ¼cher von Philip K. Dick ist den meisten die spÃ¤tere Verfilmung \u0026ldquo;A Scanner Darkly\u0026rdquo; eher ein Begriff. Aber auch Kenner des Films kÃ¶nnen Freude am Buch haben, da es viele Szenen enthÃ¤lt die so im Film nicht vorkommen und den Charakter der einzelnen Akteure tiefer verdeutlichen. Insgesamt handelt es sich um eine sehr dÃ¼stere Geschichte in einer Welt in der der Unterschied zwischen legalen, illegalen Drogen und der Welt dazwischen zu einer bis auf die Wurzeln gespalteten Gesellschaft gefÃ¼hrt hat. Es wird behauptet das Dick wÃ¤hrend des schreibens selbst in grÃ¶ÃŸeren Mengen mit Drogen \u0026ldquo;experimentiert\u0026rdquo; hat, auf jeden Fall kommt beim Lesen nie wirklich gute Laune auf, sondern eher Mitleid fÃ¼r die Protagonisten.\nMeine Bewertung: 4 von 5 Sternen\nInhalt Bob Arctor ist verdeckter Ermittler auf der Suche nach DrogenhÃ¤ndlern die die neue Droge Substanz T in den Umlauf bringen. Die Geschichte selbst spielt in den U.S.A., grÃ¶ÃŸtenteils in einer durch sÃ¼chtige verkommenen Gegend. Auch Arctor selbst wird Opfer der Droge und kann immer weniger zwischen seiner imaginÃ¤ren und der realen IdentitÃ¤t unterscheiden. Dabei wird er bewusst von hÃ¶heren Instanzen seiner BehÃ¶rde missbraucht, um an die eigentliche Organisation hinter dem Drogenhandel heranzukommen. ZusÃ¤tzlich handelt das Buch vom sozialen Zwischenleben Arctors und seinen \u0026ldquo;Freunden\u0026rdquo;, welches immer mehr, ebenfalls durch die Droge verursacht, in einer verhÃ¤ngnisvollen Interaktion mÃ¼ndet.\nWeitere Infos Originaltitel: A Scanner Darkly Genre: Science-Fiction Erscheinungsjahr: 1977 Seitenzahl: 376 ISBN: 3-453-87368-8 ErhÃ¤ltlich in jeder Buchhandlung oder unter https://www.medimops.de/dick-philip-k-der-dunkle-schirm-taschenbuch-M03453873688.html\n","tags":["deutsch","BÃ¼cher"],"section":"blog"},{"date":"1715472000","url":"/blog/vscode-web/","title":"Visual Studio Code im Webinterface mit Docker und Caddy","summary":"Microsofts Visual Studio Code hat sich Ã¼ber die letzten Jahre als Quasi-Standard, nicht nur fÃ¼r das editieren von Quellcode in allen mÃ¶glichen (Programmier-)Sprachen, sondern auch fÃ¼r eine Vielzahl weiterer Operationen etabliert.\nEiner der Kritikpunkte bleibt die durch den Editor verursachte realtiv hohe Systemlast, die zwar begrÃ¼ndet ist aber fÃ¼r einen â€Texteditorâ€œ doch etwas merkwÃ¼dig erschenint. Ein anderer Kritikpunkt ist, dass fÃ¼r das beliebte â€Remote SSHâ€œ Plugin, welches es ermÃ¶glicht auf einen beliebigen Ã¼ber SSH erreichbaren Server mit VS Code zu arbeiten, ziemlich viele dynamische Ports geÃ¶ffnet werden und auch generell einige Firewalls hier Probleme machen kÃ¶nnen.\n","content":"Microsofts Visual Studio Code hat sich Ã¼ber die letzten Jahre als Quasi-Standard, nicht nur fÃ¼r das editieren von Quellcode in allen mÃ¶glichen (Programmier-)Sprachen, sondern auch fÃ¼r eine Vielzahl weiterer Operationen etabliert.\nEiner der Kritikpunkte bleibt die durch den Editor verursachte realtiv hohe Systemlast, die zwar begrÃ¼ndet ist aber fÃ¼r einen â€Texteditorâ€œ doch etwas merkwÃ¼dig erschenint. Ein anderer Kritikpunkt ist, dass fÃ¼r das beliebte â€Remote SSHâ€œ Plugin, welches es ermÃ¶glicht auf einen beliebigen Ã¼ber SSH erreichbaren Server mit VS Code zu arbeiten, ziemlich viele dynamische Ports geÃ¶ffnet werden und auch generell einige Firewalls hier Probleme machen kÃ¶nnen.\nNun basiert VS Code selbst auf dem Electron Framework, welches wiederum auf JavaScript aufbaut. Warum also nicht die VS Code Installation auf einem Server hosten und per Webinterface zur VerfÃ¼gung stellen? Das haben sich offensichtlich viele gefragt und deshalb ist dies mit Hilfe von Docker und Caddy schnell bewerkstelligt.\nInstallation Zur Installation verlassen wir uns wieder auf Docker Compose fÃ¼r die Verwaltung unserer Containers und Caddy als Reverse Proxy. Ein passendes und regelmÃ¤ÃŸig aktualisiertes VS Code Image finden wir von Linuxserver.io auf Docker Hub.\nUnsere â€compose.ymlâ€œ Datai kann dann so aussehen:\n1--- 2version: \u0026#34;2.1\u0026#34; 3services: 4 code-server: 5 image: lscr.io/linuxserver/code-server:latest 6 container_name: code-server 7 environment: 8 - PUID=1001 9 - PGID=1001 10 - TZ=Etc/UTC 11 - PASSWORD=PASSWORT 12 #- HASHED_PASSWORD=#optional 13 #- SUDO_PASSWORD=#optional 14 #- SUDO_PASSWORD_HASH=#optional 15 #- PROXY_DOMAIN=#optional 16 - DEFAULT_WORKSPACE=ORDNERCONTAINER #optional 17 volumes: 18 - ./config:/config 19 - /ORDNERHOST:/ORDNDERCONTAINER 20 restart: unless-stopped 21 logging: 22 options: 23 max-size: \u0026#34;10m\u0026#34; 24 max-file: \u0026#34;3\u0026#34; 25 networks: 26 - caddy 27volumes: 28 data: 29 db: 30networks: 31 caddy: 32 external: true Die wichtigsten Parameter hier kurz erlÃ¤utert:\nenvironment â€“ PUID: Die Prozess ID die mÃ¶glichst der ID eures Benutzers entsprechen sollte, damit ihr spÃ¤ter Berechtigungsproblemen beim editieren von Dateien aus dem Wege gehen kÃ¶nnt. Eure eigene ID bekommt ihr einfach Ã¼ber die Linux Kommandozeile mit dem Befehl â€idâ€œ heraus. environment â€“ PGID: Das Gleiche wie fÃ¼r die PUID, nur das hier die Gruppen ID gemeint ist. environment â€“ PASSWORD: Das Passwort was abgefragt wird, wenn man die URL des containers aufruft. Da nichtmal ein User gesetzt wird, ist das nicht besonders sicher und es sollte eine zusÃ¤tzliche Sicherheitsstufe z.B. Ã¼ber den Reverse Proxy geschaffen werden. Siehe Kapitel â€HÃ¤rtungâ€œ. environment â€“ SUDO_PASSWORD: Hier lÃ¤sst sich das sudo password fÃ¼r den Container setzen. Nach mÃ¶glichkeit sollte dies vermieden werden, da man in einem Container nach Security Best Practices nie als root arbeiten sollte. environment â€“ HASHED_PASSWORD und SUDO_PASSWORD_HASH: Beide Parameter sollen dabei helfen, dass keine PasswÃ¶rter im Klartext in der â€docker-compose.ymlâ€œ abgelegt werden mÃ¼ssen. Wie man solch einen Hash Wert generiert ist auf der Projektseite erlÃ¤utert, hat aber zumindest fÃ¼r mich so nicht funktioniert. Auch deshalb ist eine zusÃ¤tzliche â€HÃ¤rtungâ€œ wie nachfolgend beschrieben erforderlich. environment â€“ DEFAULT_WORSPACE: Hier geben wir den Pfad auf dem Container an, der nachfolgend unter Volumes spezifiert ist. Diesen sehen wir dann als Workspace/Ordner in VS Code. volumes â€“ /ORDNERHOST:/ORDNDERCONTAINER: Wenn gewÃ¼nscht kÃ¶nnen wir hier einen Pfad auf unserem Host(Server) spezifizieren der in VS Code auftaucht. Die Variable ORDNERCONTAINER ersetzen wir wiederum durch den Wert den wir als â€DEFAULT_WORKSPACEâ€œ angegeben haben. Nun kÃ¶nnen wir den Container mit â€docker compose up -dâ€œ starten.\nHÃ¤rtung und Reverse Proxy Konfiguration In unserem Caddyfile unseres Rerverse Proxy ergÃ¤nzen wir einen Block fÃ¼r die neue VS Code Instanz. Das kann so aussehen:\n1code.handtrixxx.com { 2 basicauth { 3 USER HASHWERT 4 } 5 reverse_proxy code-server:8443 6} Man beachte die Werte in den geschweiften Klammern hinter â€basicauthâ€œ. Diese bringen zusÃ¤tzlichem Schutz vor mÃ¶glichen Angreifern, in dem sie ein weitere Anmeldung bereitstellen, sobald jemand die URL Ã¶ffnet. Als USER kÃ¶nnten ihr einen Namen euerer Wahl festlegen wÃ¤hrend man fÃ¼r die Generierung des Hashwerts auf der Kommandoziele in den Docker Compose Ordener von Caddy wechselt und dort folgendes Kommando ausfÃ¼hrt: docker compose exec -w /etc/caddy caddy caddy hash-password\nDen aus der Abfrage resultierenden Wert kopiert ihr einfach in das Caddyfile als HASHWERT.\nNachdem wir die Konfiguration von Caddy neu geladen haben, ist VS Code unter der angegebenen URL erreichbar und die eine zusÃ¤tzliche Sicherheitsschicht aktiviert.\n","tags":["deutsch"],"section":"blog"},{"date":"1714262400","url":"/blog/chatgpt-clone/","title":"Setup your own ChatGPT clone in 5 minutes","summary":"And another post about A.Iâ€¦ And why this headline? OpenAIâ€™s ChatGPT, Microsoftâ€™s Co-Pilot and Salesforceâ€™s Einstein are just running well! But, are they? And even if you are happy about them, are you willing to pay regular (not cheap) license fees for all of your employees, even if they just use it from time to time? Also, do you really trust Big Techâ€™s promises about confidentiallity, when itâ€™s about your intellectual property? If you answer all of this by yes, you can stop reading, now. But, in case you would like to know how you easily can run your own A.I. chatbot or you are just curious like me, about how that is done, my article gives you an overview how to do that by utilizing the two amazing tools Ollama and Open WebUI.\n","content":"And another post about A.Iâ€¦ And why this headline? OpenAIâ€™s ChatGPT, Microsoftâ€™s Co-Pilot and Salesforceâ€™s Einstein are just running well! But, are they? And even if you are happy about them, are you willing to pay regular (not cheap) license fees for all of your employees, even if they just use it from time to time? Also, do you really trust Big Techâ€™s promises about confidentiallity, when itâ€™s about your intellectual property? If you answer all of this by yes, you can stop reading, now. But, in case you would like to know how you easily can run your own A.I. chatbot or you are just curious like me, about how that is done, my article gives you an overview how to do that by utilizing the two amazing tools Ollama and Open WebUI.\nPreparation and Requirements Theoretically you could even run everything on a laptop, but for sure you would face several issues at least after a while. Better is to have a server with installed docker up and running. Also a reverse proxy, and an URL would make things more smooth, but are not mandatory. As we see later in chapter performance, it would be beneficial if your server has a dedicated graphics card, but also that is not a must.\nInstallation and Configuration The guys from the â€Open WebUIâ€œ project made it extremely easy to get your chatbot running. Basically you just create a new docker-compose.yml file like the one in the example below and start the thing as usual by command â€docker compose up -dâ€œ. Thatâ€™s it, no joke!\n1services: 2 chat: 3 container_name: chat 4 image: ghcr.io/open-webui/open-webui:ollama 5 volumes: 6 - ./ollama:/root/.ollama 7 - ./open-webui:/app/backend/data 8 restart: unless-stopped 9 #ports: 10 # - 8080:8080 11 networks: 12 caddy: 13networks: 14 caddy: 15 external: true As you can see in my example file I customized the network configuration, and also configured my reverse proxy caddy to point access to chat.handtrixxx.com to my new container. As you can see in the following screenshot you can click on â€Sign upâ€œ to create a new user account for yourself as Administrator.\nNow, after you logged in, there are just two steps more to do to start your A.I. chats. At first you should go to the admin panel and then at the â€Admin settingsâ€œ to disable registration for other users to avoid other users just create an account on your instance. Then in the settings at the models tab you will have to download one ore more language models. There are plenty to choose from. An overview is available at: https://ollama.com/library . You are done and as you see it does not have to take more than 5 minutes in case you are a bit experienced in docker and setting up tools in general.\nCosts Since everything I introduced and described is based on Open Source software, there are no costs or licensing fees at all. Great, isnâ€™t it? But to say it is completly free is also not completly true, since you have to cover the charges for the server if you do not â€haveâ€œ one anyway ğŸ™‚.\nPerformance As mentioned before, a dedicated graphics card would speed up the response times of the chatbot trendemously. By running it only on CPU, like i did in my example, every generation of a response took all the CPU power i have (and I have a lot) for some seconds. So the whole thing feels a bit like the early versions of ChatGPT. Thatâ€™s no drama, but definitly noticeable.\nConclusion As conclusion i let the openchat language model answer itself to my prompt:\n","tags":["english"],"section":"blog"},{"date":"1712361600","url":"/blog/ai-on-website/","title":"My website charged with Aritificial Intelligence","summary":"After years of tinkering with various CMS systems and even creating one of my own, one thing remains constant: the struggle to craft captivating content! â³ But fear not, because Iâ€™m tapping into the newest technology â€“ AI! ? Check out my latest post on how artificial intelligence maybe can revolutionize content creation, translations, and even programming. Dive into the cutting-edge with me at niklas-stephan.de ! #AI #ContentCreation #Innovationâ€œ\nUser Experience ","content":"After years of tinkering with various CMS systems and even creating one of my own, one thing remains constant: the struggle to craft captivating content! â³ But fear not, because Iâ€™m tapping into the newest technology â€“ AI! ? Check out my latest post on how artificial intelligence maybe can revolutionize content creation, translations, and even programming. Dive into the cutting-edge with me at niklas-stephan.de ! #AI #ContentCreation #Innovationâ€œ\nUser Experience Curious about the look and feel of your website? Wondering what to showcase? Well, hereâ€™s a peek behind the scenes! I delved into the world of â€šclassicâ€˜ ChatGPT and other freely available assistants to uncover some best practices. But letâ€™s be real: relying solely on standard templates can stifle creativity. Thatâ€™s why I leaned on my professional experience and years of accumulated expertise. Sure, thereâ€™s the Microsoft Brand Kit Generator, but letâ€™s face it: resorting to it might signal a creative rut, as evidenced by the rather lackluster brand card it churns out. Plus, Microsoftâ€™s Terms of Use put the brakes on any commercial use â€“ talk about a close call! ?\nWhat works much better is to analyze the output of Google Chromes Lighthouse report to figure out weakness of your website and then ask an A.I. assistant like ChatGPT or Github CoPilot how to solve them. More about this at chapter â€Development and Programmingâ€œ.\nContent Creation for Media Assets Attractive and professionally crafted photos, images, and graphics can often come with a hefty price tag, yet they are what truly bring texts to life. To create media such as teasers, profile, and post images, I frequently turn to AI. For instance, using apps like PicsArt, I was able to generate a set of profile pictures for myself for just around â‚¬6. ? After feeding the system with roughly 20 of my images, I received a collage of 50 AI-generated unique images within 30 minutes. Among these 50 images, I find about a third to be exceptionally well-done, definitely worth the minimal investment. Additionally, I utilize Adobe Firefly and Microsoftâ€™s Image Creator, especially for creating cover images for individual posts. While the quality varies, with Adobe generally surpassing Microsoft, the time saved is always substantial, and there are no additional costs. Impressive!?\nContent Creation for Text Imagine you have a brilliant idea in mind but struggle to articulate it into compelling written content for you website. This is where AI-powered tools like ChatGPT and Copilot step in to revolutionize your writing process. Simply provide them with a prompt or topic, and watch as they seamlessly generate coherent and engaging text tailored to your needs.\nChatGPT and Microsoftâ€™s CoPilot for instance, leverages its vast knowledge base to understand your prompt and craft responses that mimic human conversation.\nTranslations Translating content can be just as time-consuming as creating it from scratch. Thatâ€™s why Iâ€™ve incorporated the powerful Libre Translate API, which leverages AI language models to offer translations for words, sentences, or entire paragraphs across a wide range of languages. By integrating this API, Iâ€™ve streamlined the translation process, eliminating the need for manual copying and pasting, and significantly accelerating productivity. However, itâ€™s important to note that while the API facilitates efficiency, the quality of translations may vary and may not always meet optimal standards.\nDevelopment and Programming In programming, AI has become an indispensable tool for me. Gone are the days of searching for solutions to each problem individually through Google, then sifting through search results to find the right approach. Now, even just using the free version of ChatGPT significantly boosts efficiency. Additionally, Iâ€™ve subscribed to Github CoPilot, seamlessly integrated into VS Code. CoPilot not only answers my code-related questions in a chat but also provides code suggestions directly in the editor when it anticipates my intent. Itâ€™s truly remarkable! Moreover, error analysis and correction are now much faster and more effective. However, itâ€™s important to note that while Co-Pilot enhances productivity, it cannot replace human expertise, and occasionally, it may suggest solutions that are less than ideal.\nConclusion In conclusion, our exploration into the integration of AI in content creation and development unveils a landscape ripe with innovation and efficiency. From redefining the process of crafting captivating content to optimizing user experiences and streamlining development workflows, AI emerges as a transformative force.\nBy leveraging tools such as ChatGPT, Copilot, and Libre Translate API, weâ€™ve unlocked new dimensions of creativity and productivity. Whether itâ€™s generating visually stunning media assets, crafting engaging text, or accelerating programming tasks, AI proves to be an indispensable ally.\nHowever, amidst the strides in efficiency, itâ€™s crucial to remain vigilant about maintaining quality standards, particularly in translations and code accuracy. While AI enhances processes, human expertise remains irreplaceable.\nIn essence, our journey through the realms of AI-driven content creation and development underscores the potential for innovation and advancement.\n","tags":["english"],"section":"blog"},{"date":"1710201600","url":"/blog/caddy-docker/","title":"Caddy als Reverse Proxy fÃ¼r Docker Compose Container","summary":"Heute beschreibe ich was ein Reverse Proxy ist und warum er in deiner Docker Landschaft nicht fehlen darf. Nachdem ich mehrere Jahre lang sowohl Traefik, als auch den Nginx Proxy Manager als Reverse Proxy fÃ¼r meine Docker Container genutzt habe, bin ich nun bei Caddy gelandet. Am Nginx Proxy Manager stÃ¶rte mich, dass zur Konfiguration ausschlieÃŸlich die Web UI zur VerfÃ¼gung steht. Das hat mich in Backup/Restore Szenarien Ã¶fter an die Grenzen gebracht, was zum Schluss dazu fÃ¼hrte jedes Mal aufs neue eine Klickorgie zu veranstalten. An Traefik stÃ¶rte mich wiederum die aufgeblÃ¤hte Konfiguration, sowie die vielen Labels die an jedem zu berÃ¼cksichtigenden Container ergÃ¤nzt werden mÃ¼ssen. Caddy bietet mit einer CLI und einer API, die gewÃ¼nschte FlexibilitÃ¤t und lÃ¤sst sich auch einfach sichern/wiederherstellen.\n","content":"Heute beschreibe ich was ein Reverse Proxy ist und warum er in deiner Docker Landschaft nicht fehlen darf. Nachdem ich mehrere Jahre lang sowohl Traefik, als auch den Nginx Proxy Manager als Reverse Proxy fÃ¼r meine Docker Container genutzt habe, bin ich nun bei Caddy gelandet. Am Nginx Proxy Manager stÃ¶rte mich, dass zur Konfiguration ausschlieÃŸlich die Web UI zur VerfÃ¼gung steht. Das hat mich in Backup/Restore Szenarien Ã¶fter an die Grenzen gebracht, was zum Schluss dazu fÃ¼hrte jedes Mal aufs neue eine Klickorgie zu veranstalten. An Traefik stÃ¶rte mich wiederum die aufgeblÃ¤hte Konfiguration, sowie die vielen Labels die an jedem zu berÃ¼cksichtigenden Container ergÃ¤nzt werden mÃ¼ssen. Caddy bietet mit einer CLI und einer API, die gewÃ¼nschte FlexibilitÃ¤t und lÃ¤sst sich auch einfach sichern/wiederherstellen.\nWarum ein Reverse Proxy fÃ¼r Docker Container? Die Nutzung eines sogenannten Reverse Proxies fÃ¼r die eigene (Docker) Container Landschaft soll zum einen FlexibilitÃ¤t als zum anderen auch erhÃ¶hte Sicherheit bieten. Das (virtuelle) Netzwerk der Container wird in verschiedene Zonen aufgeteilt, so dass z.B. Datenbankinstanzen nicht direkt aus dem Internet erreichbar sind. Auch kÃ¶nnen so z.B. mehrere WordPress Installationen parallel betrieben werden, ohne dass sie sich irgendwie in die â€Quereâ€œ kommen. Beides wird in folgender Darstellung illustriert:\nAuÃŸerdem soll der Zugriff auf die selbst gehosteten Webseiten auschlieÃŸlich Ã¼ber das verschlÃ¼sselte HTTPS Protokoll funktionieren. Die dafÃ¼r benÃ¶tigten Zertifikate verwaltet und aktualisiert eine moderne Rerverse Proxy LÃ¶sung fÃ¼r uns voll automatisch.\nWie installiere und betreibe ich Caddy als Reverse Proxy? ZunÃ¤chst richten wir und ein dediziertes Netzwerk fÃ¼r Caddy und die Container die Ã¶ffentlich erreichbar sein sollen unter einem gewÃ¼nschten Namen (hier â€Caddyâ€œ) mit folgendem Befehl ein: docker network create caddy . Diesen Namen siehst nur du in deinen Konfigurationsdateien und in Docker.\nCaddy selbst stellen wir ebenfalls Ã¼ber einen Docker Container zur VerfÃ¼gung, auf dem wir die Ports 80 und 443 exponieren. Eine mÃ¶gliche Docker Compose Datei â€compose.ymlâ€œ dazu kann so aussehen:\n1services: 2 caddy: 3 container_name: caddy 4 image: caddy:latest 5 restart: unless-stopped 6 ports: 7 - \u0026#34;80:80\u0026#34; 8 - \u0026#34;443:443\u0026#34; 9 - \u0026#34;443:443/udp\u0026#34; 10 volumes: 11 - ./Caddyfile:/etc/caddy/Caddyfile 12 - ./site:/srv 13 - ./caddy_data:/data 14 - ./caddy_config:/config 15 logging: 16 options: 17 max-size: \u0026#34;10m\u0026#34; 18 max-file: \u0026#34;3\u0026#34; 19 networks: 20 - caddy 21volumes: 22 caddy_data: 23 caddy_config: 24networks: 25 caddy: 26 external: true Bevor wir den Container starten, legen wir im gleichen Verzeichnis eine Datei â€Caddyfileâ€œ an. Diese musst du auf deine BedÃ¼rfnisse anpassen und orientiert sich hier am oben gezeigten Beispiel:\n1{ 2 # TLS Options 3 email deine.email@provider.com 4} 5 6(wordpress) { 7 header { 8 Cache-Control \u0026#34;public, max-age=3600, must-revalidate\u0026#34; 9 } 10} 11 12quickscot.de, www.quickscot.de { 13 import wordpress 14 reverse_proxy wordpresscontainer1:80 15} 16 17niklas-stephan.de, www.niklas-stephan.de { 18 import wordpress 19 reverse_proxy wordpresscontainer3:80 20} Nun kÃ¶nnen wir den Container mit folgendem Befehl starten: docker compose up -d. Falls irgendetwas nicht funktioniert hilft eine ÃœberprÃ¼fung der Logdatei mit: docker compose logs .\nBei Problemen hilft auch ein Blick in die Caddy Community, die viele Problembehandlungen und Konfigurationsbeispiele bereitstellt: https://caddy.community/.\nFalls ihr Ã„nderungen/ErgÃ¤nzungen an dem Caddyfile vornehmt mÃ¼sst ihr nicht jedes Mal den kompletten Container durchstarten und kÃ¶nnt so Dowtimes durch folgenden Befehl, der die Konfiguration live neu lÃ¤d, aktualisieren:\n1docker compose exec -w /etc/caddy caddy caddy reload Nun gilt es noch unsere Applikationen im Caddy Netzwerk sichtbar zu machen. Hier ein Beispiel einer WordPress Installation in der der Applikationsserver von Caddy aus erreicht werden kann, aber nicht der Datenbankserver:\n1services: 2 wordpresscontainer1: 3 container_name: wordpresscontainer1 4 depends_on: 5 - wordpresscontainer1-db 6 image: wordpress:latest 7 logging: 8 options: 9 max-size: \u0026#34;10m\u0026#34; 10 max-file: \u0026#34;3\u0026#34; 11 volumes: 12 - ./data:/var/www/html 13 - /etc/localtime:/etc/localtime:ro 14 restart: unless-stopped 15 environment: 16 WORDPRESS_DB_HOST: wordpresscontainer1-db:3306 17 WORDPRESS_DB_USER: EUERUSER 18 WORDPRESS_DB_PASSWORD: EUERSUPERPASSWORT 19 networks: 20 - default 21 - caddy 22 wordpresscontainer1-db: 23 container_name: wordpresscontainer1-db 24 image: mysql:5.7 25 logging: 26 options: 27 max-size: \u0026#34;10m\u0026#34; 28 max-file: \u0026#34;3\u0026#34; 29 volumes: 30 - ./db:/var/lib/mysql 31 restart: unless-stopped 32 environment: 33 MYSQL_ROOT_PASSWORD: ROOTSUPERPASSWORT 34 MYSQL_DATABASE: wordpress 35 MYSQL_USER: EUERUSER 36 MYSQL_PASSWORD: EUERSUPERPASSWORT 37 networks: 38 - default 39volumes: 40 data: 41 db: 42networks: 43 caddy: 44 external: true Fazit Bis jetzt ist Caddy genau der Kompromiss nach dem ich suchte. Die Konfiguration Ã¼ber eine einzelne Datei lÃ¤sst sich super einfach sichern und bei Bedarf wiederherstellen. Bei Wunsch nach mehr, stellt Caddy alternativ auch die Konfiguration Ã¼ber eine API als Option bereit. Ein Blick in die technische Dokumentation unter https://caddyserver.com/docs/ offenbart, dass Caddy auch noch viel mehr kann als das gezeigte. Noch nicht herausgefunden habe ich, ob ich Ã¤hnlich wie bei Traefik auch zusÃ¤tzliche Module/Plugins wie Crowdsec, fÃ¼r eine erweiterte Sicherheit aktivieren kann. Generell wÃ¼rde ich nie wieder zurÃ¼ck zum Nginx Proxy Manager wechseln, halte aber Traefik bei komplizierteren Szenarien, evtl. fÃ¼r die bessere Wahl. FÃ¼r den Hobby Fullstack Entwickler wie mich, ist Caddy aber erstmal eine beinah rundum glÃ¼cklich LÃ¶sung.\nUpdate 12.03.2025 Ich nutze immernoch Caddy und bin nach wie vor absolut damit zufrieden. Ein wechsel z.B. zu Traefik ist auch nach vielen Monaten nicht nÃ¶tig.\n","tags":["deutsch"],"section":"blog"},{"date":"1707350400","url":"/blog/ssh-copy-id/","title":"Passwordless SSH Login to a Remote Server with Visual Studio Code","summary":"Following article describes how to enable You to login to a Remote Server with the industry standard SSH network protocol by using a key and Visual Studio Code, so you do not have to enter your user/password every time you want to connect. It was written for/on MacOS, but the procedure should be the same on Linux and similar for Windows.\nPreparation At first download, install and open VS Code.\n","content":"Following article describes how to enable You to login to a Remote Server with the industry standard SSH network protocol by using a key and Visual Studio Code, so you do not have to enter your user/password every time you want to connect. It was written for/on MacOS, but the procedure should be the same on Linux and similar for Windows.\nPreparation At first download, install and open VS Code.\nThen install the â€Remote â€“ SSHâ€œ Extension by changing to the Extensions tab, searching for it and click on â€Installâ€œ.\nFun with Keys At next open a terminal session by option â€New Terminalâ€œ.\nAt the terminal prompt (normally shown at the bottom of your window) connect to your server by entering following line and pressing enter key.\n1ssh -p PORT USERNAME@SERVER So for example your line could be like â€ssh -p 22 heinz@mydomain.comâ€œ. Then accept/confirm any dialogues you are prompted to for confirmation as well es entering the password.\nNow, just leave the ssh session by entering following command to the terminal:\n1exit At next you create a key for ssh (in case you do not already have one) on your local PC. Be sure you really exited/disconnected from the server before:\n1ssh-keygen Once done, you can copy the public key part to the server by utilizing following command:\n1ssh-copy-id -p PORT USERNAME@SERVER Next time if you login to the server with ssh you will not be asked for the password anymore but the key will be automatically verified in the background.\nSetting up the environment So letâ€™s try that by login at the server (ssh -p PORT USERNAME@SERVER) in the terminal again and by creating a workspace directory for the files you want to edit/control with VS code later on.\n1mkdir DIRECTORYNAME The directory name can be anything you prefer, so e.g. â€mkdir dataâ€œ could be suitable. Jump into this directory by sending command:\n1cd DIRECTORYNAME e.g. â€cd dataâ€œ. Then print the full directory path to the terminal screen by:\n1pwd The result will be something like â€/home/heinz/dataâ€œ. Copy this path to your clipboard or note/remember it. You are more then 50% done, now.\nConfigure VS Code Open the Remote SSH tab in VS Code and click on â€+â€œ at SSH in the Remote Explorer.\nEnter the same command you used before to connect to your server at the new prompt shown on top of the window:\n1ssh -p PORT USERNAME@SERVER Choose the local ssh config file to update, normally the first proposal is fine.\nNow, in the Remote Explorer you should see the new option for connection, like:\nIf itâ€™s not directly shown, just click the refresh button as shown on top right of the screenshot. Then click on the â€-\u0026gt;â€œ arrow to connect.\nFinally, switch to the File Explorer Tab and click on â€Open Folderâ€œ. At the prompt you enter or select the directory you remembered or copied before and click on â€OKâ€œ:\nIn case you are asked if you trust the authors of the directory it makes sense to choose yes, since you are the author ğŸ™‚\nWe are done. You most likely see an empty folder structure since we just created the folder and you can create new files of any kind, like the â€README.MDâ€œ on the next screenshot, and folders as you wish.\nItâ€™s also possible to copy files from your PC to the server by drag-and-drop as well as downloading files/folders by a right-click on it at the VS Code File Explorer.\nResult Next time you open VS Code the connection will be established automatically. In case you switch between different environments you can always go back to the Remote Explorer tab of VS Code and connect to the server. Be sure to select the folder for connection and not the server itself, otherwise you will be prompted to select a folder at the File Explorer Tab again.\nI hope that short tutorial helped you out a bit ğŸ™‚\n","tags":["english"],"section":"blog"},{"date":"1704931200","url":"/blog/next-js-18/","title":"Learn Next.js - Chapter 18: Docker Environment","summary":"If you went successfully through Vercel\u0026rsquo;s free and amazing Learn Next.js online course, you have end up in a nice and \u0026ldquo;fast\u0026rdquo; (the database access is actually slow, but that was on purpose for better understanding of the tutorial) Demo of a modern web application. You have also learned about plenty considerations when developing an web app. So now, you maybe feel like \u0026ldquo;How can I use all of this for a real project?\u0026rdquo;. Also there are still some constraints left like; \u0026ldquo;How can I get independent from Vercel\u0026rsquo;s nice but (in their free plan) slow hosting offer?\u0026rdquo;.\n","content":"If you went successfully through Vercel\u0026rsquo;s free and amazing Learn Next.js online course, you have end up in a nice and \u0026ldquo;fast\u0026rdquo; (the database access is actually slow, but that was on purpose for better understanding of the tutorial) Demo of a modern web application. You have also learned about plenty considerations when developing an web app. So now, you maybe feel like \u0026ldquo;How can I use all of this for a real project?\u0026rdquo;. Also there are still some constraints left like; \u0026ldquo;How can I get independent from Vercel\u0026rsquo;s nice but (in their free plan) slow hosting offer?\u0026rdquo;.\nMy tutorial here is supposed to be an extension to the course but you can also use it, since I share my code on Github, without going through Vercel\u0026rsquo;s course. At first we will \u0026ldquo;dockerize\u0026rdquo; our development area, then host our own database to finally be able to push all of that to a webserver of our choice.\nStarting Point At first let\u0026rsquo;s have a look how our setup looks like after we finished chapter 17 of the tutorial:\nYou have a local directory \u0026ldquo;nextjs-dashboard\u0026rdquo; somewhere on your local machine, where you can edit your code with your favorite IDE (I would recommend VS Code) and look at the result localy on your browser, because you also installed node.js on your client. Additionally, just like a professional, you commit your code to an GIT instance which is Microsoft\u0026rsquo;s popular GitHub in our case. Beside that you learned how to setup and configure Vercel\u0026rsquo;s offer of a database instance as well as a public webserver.\nThat already reveals some flaws and pain (at least in my stomach):\nYou had to install node.js on your client and working with files directly part of your local file system node.js compiles everything depending on your clients architecture, but your production server most likely (will) run(s) on Linux. You have to rely on Vercel\u0026rsquo;s deployment and recompiling for production, which is not 100% transparent (at least to me). The database is reachable from the public internet (even if secured a bit) You literally use the same database for development and production purposes, which totally is not how it should be in a real-life example. Summarized: The setup is not like it would be, if you are a (semi-)professional developer.\nThat was totally on purpose to keep the \u0026ldquo;Learn Next.js\u0026rdquo; tutorial as simple as possible, but will be changed by us, now.\nDocker Containers?! While in former articles I already wrote about the benefits of the Docker container infrastructure from the view of an administrator, it also brings big advantages for software developers. Even or especially large software companies rely on container architecture for software development, they just do not use Docker but Kubernetes which was designed by Google for running in big datacenters. Nevertheless, Docker provides most of the functionalities similar to Kubernetes and is absolutly fine for private developers as well as small(er) companies.\nInstall Docker on your PC To install docker you can follow the well described tutorials on their website: https://www.docker.com/get-started/ . I recommend you to install the Docker Desktop App in case you are using Windows or Mac OS. In case you are using Linux i find it easier and much more leightweigth to install Docker via commandline, the same as you would do on a server. Once you have successfully installed Docker and the engine is running, it is time for the next steps.\nDockerize your Deveopment Area In case you want to skip the Learn Next.js online course or you do not have the resources of the result available, you can clone my Repository by git clone https://github.com/handtrixx/nextjs-dashboard.git or by the GitHub Desktop App.\nThen in your project\u0026rsquo;s root directory you create/enter folder \u0026ldquo;docker\u0026rdquo;. In folder docker we create/enter a subfolder called \u0026ldquo;dev\u0026rdquo;.\nInside the folder \u0026ldquo;dev\u0026rdquo; you create file \u0026ldquo;.env\u0026rdquo;:\n1POSTGRES_URL=\u0026#34;xxx\u0026#34; 2POSTGRES_PRISMA_URL=\u0026#34;xxx\u0026#34; 3POSTGRES_URL_NON_POOLING=\u0026#34;xxx\u0026#34; 4POSTGRES_USER=\u0026#34;xxx\u0026#34; 5POSTGRES_HOST=\u0026#34;xxx\u0026#34; 6POSTGRES_PASSWORD=\u0026#34;xxx\u0026#34; 7POSTGRES_DATABASE=\u0026#34;xxx\u0026#34; 8 9AUTH_SECRET=xxx Please replace xxx by the values of your postgres database from Vercel. In case you don\u0026rsquo;t have one or do not want one leave the content like described here, just remember when you startup the app it will not be completly functional then, since there is no database connection.\nThen inside the folder \u0026ldquo;dev\u0026rdquo; you create or open the file \u0026ldquo;Dockerfile\u0026rdquo; with following content:\n1# Use an official Node.js runtime as a base image 2FROM node:latest 3 4# Update OS mostly to enjoy latest security updates 5RUN apt-get update \u0026amp;\u0026amp; apt-get upgrade -y 6 7# Set the working directory inside the container 8WORKDIR /usr/src/app 9 10# Copy the rest of the application code to the working directory 11COPY --chown=node:node ../../. . 12 13# update npm package manager if required 14RUN npm install -g npm 15 16# Copy the startup script to the container 17COPY --chown=node:node app-startup.sh /usr/src/app/app-startup.sh 18 19# Give execute permissions to the script 20RUN chmod +x /usr/src/app/app-startup.sh 21 22# set user to node 23USER node 24 25# Expose the port your app runs on 26EXPOSE 3000 27 28# Define the startup command 29CMD [\u0026#34;sh\u0026#34;,\u0026#34;/usr/src/app/app-startup.sh\u0026#34;] What did we just do? At first the \u0026ldquo;Dockerfile\u0026rdquo; pulls the official node.js repository from the Docker Hub repository for further use. Then it installs the latest security and functional updates into our container. Then it copies the content of our projects directory into directory /usr/src/app inside the container. By \u0026ldquo;RUN npm install -g npm\u0026rdquo; the newest version of npm is installed. Then our startup script is copied and the user used inside the container for all of that is changed to \u0026ldquo;node\u0026rdquo; to reduce security flaws. At the end the file exposes our development port 3000 and instructs the system to start our startup script everytime the container will be launched.\nThe next step is to open/create the file \u0026ldquo;docker-compose.yml\u0026rdquo; in the same directory with following content:\n1services: 2 nextjs-dashboard: 3 build: . 4 ports: 5 - 3000:3000 6 restart: unless-stopped 7 volumes: 8 - ../../:/usr/src/app/ 9 - ./app-startup.sh:/usr/src/app/app-startup.sh 10 networks: 11 - default 12 env_file: 13 - .env Docker compose helps us to build up a real dev environment and later to additionally add a database server. For the time being the file contains the info to use the Dockerfile in the same directory to build our \u0026ldquo;nextjs-dashboard\u0026rdquo; service, exposes port 3000 to the host and would restart the service in case it crashed without manual intervention. On top of that, by the \u0026ldquo;volumes\u0026rdquo; are we specify that our complete project directory on the host is mapped inside the container at /user/src/app and does the same for our startup script. By defining network \u0026ldquo;default\u0026rdquo; it will create a encapsuled virtual network for the service as another layer of security. The last step by providing \u0026ldquo;env_file\u0026rdquo; is to forward the content ouf our \u0026ldquo;.env\u0026rdquo; file in the same directory as environment variables into the container.\nWe are nearly done, the last step is to open/create file app-startup.sh again in the same folder with following content:\n1echo Installing node modules from package.json 2npm install --no-progress 3npm install next@latest --no-progress 4echo Starting your Node.js application 5npm run dev 6 7# Keep the script running in case of errors to be able to inspect the container 8tail -f /dev/null The script runs every time you will startup the dev container and installs any missing node modules before starting the webserver. That\u0026rsquo;s it your file structure should now look like this:\nContainer Operations While you can still use git like before to push your code to Github you have several other options to control your container. If you are inside your /dev folder, with\n1docker compose up -d your container will be build and strated for the first time. Any time you make changes of the docker-compose.yml file the same command will refresh and restart the configuration.\nThe command\n1docker compose down will shutdown your whole environment. While with\n1docker compose logs the log output including the console.log will be shown. And if you are not sure if you container is running or not you can test this with command:\n1docker compose ps In case you want to change/update your Dockerfile, after saving you have to run:\n1docker compose build 2docker compose up -d to make these changes active.\nSummary In this tutorial you successfully \u0026ldquo;dockerized\u0026rdquo; your development area, but maybe can\u0026rsquo;t already see big advantages beside that you do not have to install node.js and keep it up to date. That will change during the next tutorials when wie first add a database service to become independent from Vercels database instance, to divide dev and prd data and to increase security.\n","tags":["english"],"section":"blog"},{"date":"1704672000","url":"/blog/mindset/","title":"Mindset *****","summary":"Du denkst du bist immer offen gegenÃ¼ber allem und jedem und auch so total â€open-mindedâ€œ? Das dachte ich auch und dann las ich dieses Buch. Dr. Carol Dweck gelingt es auf weniger als 300 Seiten, uns selbst soweit zu entlarven dass man erkennt, dass man vielleicht in einigen oder sogar vielen Disziplinen ein wirklich offener Mensch, aber in anderen Bereichen genau in das Gegenteil verfallen ist.\nSchÃ¶ne Beispiele aus Dr. Dwecks privaten Leben und beruflichen Laufbahn veranschaulichen wie uns die Macht der gewohnheit, Erziehung und das Leben selbst in vielen Lagen zu Gewohnheitstieren gemacht hat. Erfreulicherweise liefert sie auch gleich Methoden und MÃ¶glichkeiten aus diesem Hamsterrad in unserem Verstand zu entkommen.\n","content":"Du denkst du bist immer offen gegenÃ¼ber allem und jedem und auch so total â€open-mindedâ€œ? Das dachte ich auch und dann las ich dieses Buch. Dr. Carol Dweck gelingt es auf weniger als 300 Seiten, uns selbst soweit zu entlarven dass man erkennt, dass man vielleicht in einigen oder sogar vielen Disziplinen ein wirklich offener Mensch, aber in anderen Bereichen genau in das Gegenteil verfallen ist.\nSchÃ¶ne Beispiele aus Dr. Dwecks privaten Leben und beruflichen Laufbahn veranschaulichen wie uns die Macht der gewohnheit, Erziehung und das Leben selbst in vielen Lagen zu Gewohnheitstieren gemacht hat. Erfreulicherweise liefert sie auch gleich Methoden und MÃ¶glichkeiten aus diesem Hamsterrad in unserem Verstand zu entkommen.\nMeine Bewertung: 5 von 5 Sternen.\nInhalt Die U.S. amerikanische Professorin hat es an der Standford University zur Weltbekanntheit gebracht. Um so erstaunlicher war es fÃ¼r mich, dass ich von diesem Buch zuvor noch nichts gehÃ¶rt hatte. In insgesamt 9 Kapiteln wird zunÃ¤chst versucht einem klar zu machen, dass unser Mindset in vielen lebenslagen gar nicht so offen ist, wie wir es vielleicht denken. Es folgen viele Beispiele aus Sport, Beruf, Liebesleben und weiteren, die verdeutlichen wie eine Ã„nderungen der Betrachtungsweise zu grÃ¶ÃŸerem Erfolg und Wohlebefinden helfen kann. Im letzten Kapitel folgen dann konkrete Empfehlungen wie man sich selbst eine wirklich offenes Mindset zulegen und dieses auch bewahren kann.\nWeitere Infos Originaltitel: Mindset â€“ Changing the way you think to fulfil your potential Genre: Sachbuch Erscheinungsjahr: 2006 Seitenzahl: 288 ISBN: 978-1-4721-3995-5 ErhÃ¤ltlich in jeder Buchhandlung oder unter https://www.medimops.de/dweck-dr-carol-mindset-updated-edition-changing-the-way-you-think-to-fulfil-your-potential-taschenbuch-M0147213995X.html ","tags":["deutsch","BÃ¼cher"],"section":"blog"},{"date":"1697068800","url":"/blog/install-docker-engine/","title":"Docker Engine auf einem Linux Server installieren","summary":"Docker hilft uns durch sogenannte Container dabei die technische Serverinfrastruktur, die fÃ¼r verschiedenste Projekte benÃ¶tigt wird, schnell und vom Basissystem isoliert aufzusetzen. AuÃŸerdem veringern wir durch den Einsatz von Docker den Wartungsaufwand z.B. fÃ¼r Systemaktualsierungen und Backups. Tausende von Projekten stehen vorgefertigt zur VerfÃ¼gung, die mit einem einfachen Kommando nur gestartet werden kÃ¶nnen. In diesem Artikel beschreibe ich wie Docker und das zusÃ¤tzlich hilfreiche Docker Compose installiert werden. Docker Compose hilft uns dabei ganze Containerlandschaften zur orchestrieren. Ein Beispiel: Bei der Installation von WordPress Ã¼ber Docker werden mehrere Container benÃ¶tigt, einer fÃ¼r die App und einer fÃ¼r die Datenbank. Wenn wir diese nun alle einzeln starten und evtl. dabei noch bestimmte Parameter Ã¼bergeben wollen, wird es schnell uÃ¼nbersichtlich. Mit Compose kÃ¶nnen wir alle benÃ¶tigten Parameter und Container in einer strukturierten Datei zusammenfassen und diese Umgebung mit einfachen Befehlen steuern.\n","content":"Docker hilft uns durch sogenannte Container dabei die technische Serverinfrastruktur, die fÃ¼r verschiedenste Projekte benÃ¶tigt wird, schnell und vom Basissystem isoliert aufzusetzen. AuÃŸerdem veringern wir durch den Einsatz von Docker den Wartungsaufwand z.B. fÃ¼r Systemaktualsierungen und Backups. Tausende von Projekten stehen vorgefertigt zur VerfÃ¼gung, die mit einem einfachen Kommando nur gestartet werden kÃ¶nnen. In diesem Artikel beschreibe ich wie Docker und das zusÃ¤tzlich hilfreiche Docker Compose installiert werden. Docker Compose hilft uns dabei ganze Containerlandschaften zur orchestrieren. Ein Beispiel: Bei der Installation von WordPress Ã¼ber Docker werden mehrere Container benÃ¶tigt, einer fÃ¼r die App und einer fÃ¼r die Datenbank. Wenn wir diese nun alle einzeln starten und evtl. dabei noch bestimmte Parameter Ã¼bergeben wollen, wird es schnell uÃ¼nbersichtlich. Mit Compose kÃ¶nnen wir alle benÃ¶tigten Parameter und Container in einer strukturierten Datei zusammenfassen und diese Umgebung mit einfachen Befehlen steuern.\nVorraussetzungen Wir gehen davon aus, dass wir ein Linuxsystem auf Basis von Debian als Betriebssystem nutzen. Das kÃ¶nnen z.B. das Raspberry Pi OS, Ubuntu oder eine der vielen anderen Varianten davon sein. GrunsÃ¤tzlich funktioniert Docker aber auch unter allen anderen Linux Systemen und auch unter Windows oder MacOS.\nInstallation der Docker Engine Wir wollen immer die aktuellsten Versionen nutzen und orientieren uns deshalb an den Anleitungen auf den offiziellen Docker Seiten dazu.\nDocker installieren: https://docs.docker.com/engine/install/debian/ Schritte zur komfortableren Nutzung von Docker: https://docs.docker.com/engine/install/linux-postinstall/ Wenn man sich nicht von der augeinscheinlich groÃŸen Masse der Schritte erschlagen lÃ¤sst, sehen wir dass wir im Endeffekt nur 3-4 Kommandos per Copy \u0026amp; Pase in unsere Kommandozeile kopieren mussten.\nAm besten fÃ¼hren wir abschlieÃŸend noch einmalig einen Reboot unseres System durch.\nVerwendung von Docker Compose Der Aufbau einer Docker-Composse Konfiguration ist im endeffeckt immer gleich und basiert auf einer einzelnen Datei, der compose.yml. Diese legt man am besten nicht einfach irgendwo hin, sondern Ã¼berlegt sich eine Ordnerstruktur, da man ja evtl. viele Umgebungen parallel betreiben und den Ãœberblick behalten mÃ¶chten. Ein Beispiel:\nwir legen im Verzeichnis /opt/ ein Unterverzeichnis /opt/docker/an. Und hier dann wiederum Unterverzeichnisse z.B. nach Projekten oder URLs gegliedert. Also zum Beispiel /opt/docker/wordpress/. In dieses Untervzeichnis gehÃ¶rt dann unsere Datei zur Definition der Umgebung, also /opt/docker/wordpress/compose.yml Wenn wir beim Beispiel WordPress bleiben bekommen wir auf https://hub.docker.com/_/wordpress ein Beispiel fÃ¼r den Inhalt der compose.yml Dieses Beispiel wollen wir noch etwas fÃ¼r uns anpassen und fÃ¼r den dauerhaften Betrieb optimieren. Das kann dann so aussehen (Achtung die Formatierung mit den freien Zeichen als Tabs ist bindend):\n1services: 2 wordpress-app: 3 container_name: wordpress-app 4 hostname: wordpress-app 5 image: wordpress 6 restart: always 7 ports: 8 - 8080:80 9 environment: 10 WORDPRESS_DB_HOST: wordpress-db 11 WORDPRESS_DB_USER: exampleuser 12 WORDPRESS_DB_PASSWORD: examplepass 13 WORDPRESS_DB_NAME: exampledb 14 TZ: Europe/Berlin 15 volumes: 16 - ./wordpress:/var/www/html 17 logging: 18 options: 19 max-size: \u0026#34;10m\u0026#34; 20 max-file: \u0026#34;3\u0026#34; 21 22 wordpress-db: 23 container_name: wordpress-db 24 hostname: wordpress-db 25 image: mysql:5.7 26 restart: always 27 environment: 28 MYSQL_DATABASE: exampledb 29 MYSQL_USER: exampleuser 30 MYSQL_PASSWORD: examplepass 31 MYSQL_RANDOM_ROOT_PASSWORD: \u0026#39;1\u0026#39; 32 TZ: Europe/Berlin 33 volumes: 34 - ./db:/var/lib/mysql 35 logging: 36 options: 37 max-size: \u0026#34;10m\u0026#34; 38 max-file: \u0026#34;3\u0026#34; Zur ErklÃ¤rung:\nservice, container_name, hostname: Nur den Service selbst zu benennen ist Pflicht. Die anderen beiden Variablen setzen wir um unseren Container je nach Zugriffsart schnell finden zu kÃ¶nnen, wenn wir ihn mal suchen. image: Die Anwendung welche wir verwenden mÃ¶chten, im Standard ein Image von Docker Hub https://hub.docker.com/search?q= restart: always bedeutet, dass der Container neu gestartet wird, egal warum auch immer er zuvor abgebrochen/abgestÃ¼rzt ist. environment: Verschiedene Umgebungsvariablen die an den Container weitergegben werden. Das kÃ¶nnen je nach Image ganz unterschiedliche sein. volumes: Persistente Daten, die wir z.B. auch fÃ¼r ein Backup berÃ¼cksichtig bzw. grundsÃ¤tzlich unter unserer Kontrolle haben und nicht im Container â€œverstecktâ€ haben wollen. durch ./bestimmen wir, dass es sich dabei um einen Uneterverzeichnis des aktuellen Ordners handelt. Falls nicht vorhanden wÃ¼rde dieser automatisch angelegt werden, besser aber vorher schon selber anlegen (Berechtigungen, etcâ€¦). logging: Manche Container/Applikationen sind recht freudig im Schreiben von Protokolldateien. Hiermit schrÃ¤nken wir dies auf 3 10MB groÃŸe Dateien ein, wobei der Ã¤lteste Eintrag beim Ã¼berschreiten der Grenze Ã¼berschrieben wird. ports: Falls kein Reverse Proxy zum Einsatz kommt, gibt z.B. 8080:80 an, dass der im Container laufende Port 80 and das Host System unter Port 8080 weitergleitet wird. D.h. wenn wir in unserem Beispiel WordPress aufrufen mÃ¶chten, dann mit: http://HOST:8080 . Zur Kommunikation der Container Untereinander also z.B. von App zur DB wird keine Portfreigabe benÃ¶tigt! Das erhÃ¶ht die gleichzeitig die Sicherheit unserer Umgebung. NÃ¼tzliche Befehle Aus dem Verzeichnis heraus, in dem wir unsere compose.yml angelegt haben kÃ¶nnen wir unsere Umgebung nun mit docker compose up -d starten. Wenn wir alle Container aus der Compose Datei herunterfahren mÃ¶chten, geht das mit docker compose down. Mit dem Befehl docker compose pull ziehen wir die aktuellste Version unserer Images von Docker Hub. Nach dem Pull ist ein erneutes docker compose up -d erforderlich um die Container in der neuen Version zu starten. Die persistenten Daten aus den definierten Volumes bleiben dabei natÃ¼rlich erhalten. GefÃ¤hrlich wird es nur bei grÃ¶ÃŸeren VersionssprÃ¼ngen innerhalb der Anwendung. Dann gibt es entweder die MÃ¶glichkeit die Umgebung herunterzufahren, das letzte geladene Image zu lÃ¶schen und dann wieder zu starten. Alternativ lÃ¤sst sich in der compose.yml auch eine version des Images fest einstellen. Damit verzichtet man in der Regel aber auch Sicherheitsupdates.\n","tags":["deutsch"],"section":"blog"},{"date":"1694390400","url":"/blog/windows10-password-reset/","title":"Passwort vergessen in Windows 10? (Fast) kein Problem!","summary":"In einer besonders unglÃ¼cklichen Situation bin ich kÃ¼rzlich von einem Freund kontaktiert worden: Er hat sein Windows 10 Anmeldepasswort unwiederbringlich vergessen. Also auf zu Google und siehe da; es gibt unzÃ¤hlige LÃ¶sungsvorschlÃ¤ge um ein Win10 Passwort zurÃ¼ckzusetzen. Dann aber die ErnÃ¼chterung: Microsoft hat schon nachgebessert und alle Tricks, die ich gefunden habe, funktionieren leider nicht mehr. Was nun?\nZeit fÃ¼r Eigeninitative ğŸ™‚ In dieser Anleitung beschreibe ich in KÃ¼rze, wie man ein Administratorkonto, auch in der aktuellsten Windows 10 Version, zurÃ¼cksetzen kann.\n","content":"In einer besonders unglÃ¼cklichen Situation bin ich kÃ¼rzlich von einem Freund kontaktiert worden: Er hat sein Windows 10 Anmeldepasswort unwiederbringlich vergessen. Also auf zu Google und siehe da; es gibt unzÃ¤hlige LÃ¶sungsvorschlÃ¤ge um ein Win10 Passwort zurÃ¼ckzusetzen. Dann aber die ErnÃ¼chterung: Microsoft hat schon nachgebessert und alle Tricks, die ich gefunden habe, funktionieren leider nicht mehr. Was nun?\nZeit fÃ¼r Eigeninitative ğŸ™‚ In dieser Anleitung beschreibe ich in KÃ¼rze, wie man ein Administratorkonto, auch in der aktuellsten Windows 10 Version, zurÃ¼cksetzen kann.\nSo viel im Voraus: Microsoft muss hier weiter nachbessern, denn die beschriebene Methode kann dazu genutzt werden administrativen Zugriff auf jegliche Win10 PCs zu erlangen. Einzige Vorraussetzung ist der physische Zugriff auf den Rechner.\nVorbereitung Mein Freund hat mir den betroffenen Computer vorbeigebracht, also hatte ich direkten Zugriff auf den Rechner. Das Benutzerkonto auf das er nicht mehr zugreifen konnte, war ein lokales Konto.\nHinweis: FÃ¼r ein Benutzerkonto, dass mit der Microsoft Cloud verknÃ¼pft ist, funktioniert diese Anleitung nur teilweise â€“ Dort ist das zurÃ¼cksetzen des Passworts aber auch kein Problem und kann von einem anderen EndgerÃ¤t aus, Online erledigt werden.\nAuÃŸerdem habe ich mir ein Windows 10 Installationsmedium auf einem USB-Stick erzeugt, weil ich wÃ¤hrend der ersten Recherechen darauf gestoÃŸen bin, dass das erforderlich ist. Den offiziellen Windows 10 Download findet man unter in den Quellen am Ende des Artikels. Zum erstellen des Sticks brauchen wir natÃ¼rlich irgendwo einen zweiten Rechner auf den wir Zugriff haben. Die heruntergeladene ISO Datei lÃ¤sst sich je nach Betriebssystem unterschiedlich auf den Stick bringen. Google hilft.\nVorgehensweise Folgende Schritt-fÃ¼r-Schritt Anleitung umreiÃŸt die als funktionierend gestestete Vorgehensweise, um das Passwort eines lokalen Windows 10 Benutzers zurÃ¼ckzusetzen.\nZunÃ¤chst booten wir von unserem zuvor erzeugten Bootstick. Im ersten Dialogfenster der Sprachauswahl etc. drÃ¼cken wir gleichzeitig die SHIFT + F10 Tasten. In der daraufhin erscheinenden Kommandozeile (oha; dass dÃ¼rfte doch eigentlich gar nicht funktionieren), wechseln wir auf die Partition unserer lokalen Windows 10 Installation, also z.B. mit den Befehl â€œC:â€.\nNun â€œhangelnâ€ wir uns ins lokale Systemverzeichnis. Das geht z.B. mit den Befehlen: â€œcd Windowsâ€ Enter Taste, dann â€œcd system32â€ Enter Taste.\nJetzt erstellen wir unser â€œmagisches Torâ€ um spÃ¤ter, beim normalen Hochfahren, ebenfalls in die Kommandozeile wechseln zu kÃ¶nnen. Mit â€œcp utilman.exe utilman_old.exeâ€ sichern wir zunÃ¤chst den Eingabehilfeassistenten um ihn spÃ¤ter wiederherstellen zu kÃ¶nnen.\nMit â€œcopy cmd.exe utilman.exeâ€ erzeugen wir eine Kopie des Kommandzeilenprogramms unter dem Namen des Eingabehilfeassistenten.\nDas war der erste Streich. Wir kÃ¶nnen nun die Windows Installationsroutine schlieÃŸen und den Rechner wieder normal hochfahren.\nWir mÃ¼ssen am Anmeldebildschirm einen erneuten Reboot im abgesicherten Modus herbeifÃ¼hren, da zumindest mir, sonst Aufrufe der Kommandozeile blockiert wurden (vermutlich vom im Hintergrund laufenden Systemschutz-Programâ€¦). Also die â€œSHIFTâ€ Taste gedrÃ¼ckt halten, unten Rechts auf das Powersymbol drÃ¼cken und Neustart auswÃ¤hlen. Im nach kurzer Zeit erscheinenden Zwischendialog wÃ¤hlen wir â€œAdvanced Optionsâ€ bzw. â€œErweiterte Optionenâ€, und dann â€œStartup Settingsâ€ bzw. â€œStarteinstellungenâ€. Nun auf â€œRestartâ€ bzw. â€œNeustartâ€ klicken. (Sorry, habe gerade nur Englische Screenshots) Sobald der Rechner wieder oben ist, drÃ¼cken wir die Taste 6 um den abgesicherten Modus mit Kommandozeile zu starten. Leider startet sich die Kommandozeile nicht direkt, da wir zunÃ¤chst unser Passwort eingeben sollen (haha), aber unser Trick den wir durch Schritt 6 ermÃ¶glicht haben, kann nun ausgefÃ¼hrt werden: Wir klicken auf das Symbol fÃ¼r die Eingabehilfe (das zweite Symbol rechts unten) und VoilÃ : Die Kommandozeile Ã¶ffnet sich, und dieses mal mit vollen Berechtigungen! Durch Eingabe des Befehls â€œnet userâ€, bekommen wir eine Liste aller lokalen Benutzerkonten angezeigt. Da fehlt mir anscheinend das Know-How, aber das Benutzerkonto um das es mir eigentlich geht, wird gar nicht angezeigt?! Macht nichts: Wir reaktivieren einfach das vorhandene â€œAdministratorâ€ Benutzerkonto und arbeiten mit diesem weiter. Wie? nÃ¤chster Schritt: Der Befehl â€œnet user Administrator /active:yesâ€ aktiviert das Konto und der Befehl â€œnet user Administrator PASSWORDâ€ setzt das Passwort PASSWORD fÃ¼r diesen. (mit angepassten Befehlen lÃ¤sst sich so natÃ¼rlich auch ein komplett neuer User als Admin einrichten, falls der User â€œAdministratorâ€ nicht verfÃ¼gbar ist.) Weil es so schÃ¶n ist: Zeit fÃ¼r einen Reboot\nm Anmeldebildschirm unten links sehen wir nun mehrere Benutzer zur Auswahl, und eben auch den â€œAdministratorâ€ den wir soeben aktiviert haben. Auf diesen klicken wir jetzt und melden uns dann als dieser User am System, mit dem zuvor gestetzten Passwort, an.\nNachdem wir die Windows â€œErsteinrichtungshÃ¶lleâ€ hinter uns gelassen haben, sehen wir jetzt einen mehr oder weniger augerÃ¤umten Desktop und haben uns erfolgreich mit Adminstratorberechtigungen angemeldet!\nJetzt kÃ¶nnen wir Ã¼ber die Systemsteuerung -\u0026gt; Benutzer -\u0026gt; Andere Konten das Passwort unseres eigentlichen Benutzer komfortabel Ã¤ndern und ja; den PC neu starten.\nFertig: Jetzt kÃ¶nnen wir im Anmeldebildschirm von Windows wieder unseren eigentlichen Benutzer auswÃ¤hlen und uns mit dem neu gesetzten Passwort anmelden.\nAufrÃ¤umen Unsere gesetzten Spuren machen wir Ã¼ber folgende Befehle in der Windows Kommandozeile wieder rÃ¼ckgÃ¤ngig:\nnet user administrator /active:no â€“ Wir deaktivieren den â€œAdministratorâ€ Benutzer wieder, brauchen wir nun ja nicht mehr. C:â€, â€œcd Windowsâ€, â€œcd system32â€, â€œdel utilman.exeâ€, â€œcopy utilman_old.exe utilman.exeâ€ â€“ stell die Systedateien wieder auf ihren ursprÃ¼nglichen Zustand. Fazit Um dem gestellten Problem von vornherein aus dem Wege zu gehen, kÃ¶nnte man z.B. ein Microsoft Account (wenn man dem Unternehmen traut) erstellen und mit dem Konto verknÃ¼pfen. Dann ist das zurÃ¼cksetzen evtl. Online mÃ¶glich, insofern der betroffene PC eine Internetverbindung hat.\nAuch kann man mit kÃ¼rzeren PINs arbeiten, die Microsoft seit geraumer Zeit fÃ¼r das Anmelden, neben der weiteren Option eines Erinnerungssatzes, bereitstellt.\nNichtsdestotrotz (ja, das schreibt sich wohl so): Mit der beschriebenen Vorgehensweise lÃ¤sst sich nicht nur ein vergessenes Windows Passwort zurÃ¼cksetzen, sondern auch Zugriff auf jeden Windows 10 PC, auf den physischer Zugriff besteht, beschaffen. Eine eklatante SicherheitslÃ¼cke die in anderen Betriebssystemen wie Linux und MacOS besser â€œgestopftâ€ ist.\nNaja, meinem Freund konnte ich so jedenfalls helfen ğŸ™‚\nEnde gut, alles gut? Wie kann ich mich vor so einem â€œAngriffâ€ schÃ¼tzen? Eigentlich dÃ¼rfte das beschriebene Szenario Ã¼berhaupt nicht funktionieren, weil sich so jeder administrativen Zugang auf einen Windows PC verschaffen kann.\nUrsache ist eine Verkettung von UmstÃ¤nden, die uns diese SicherheitslÃ¼cke Ã¶ffnet. Um solch einen Angriff zu verhindern reicht es nicht aus, einfach auf ein Micorosoft Online Konto zu wechseln. Es kann sich ja trotzdem jeder Ã¼ber ein neues Benutzerkonto administrativen Systemzugriff verschaffen, sobald sie/er physischen Zugriff auf den Rechner hat.\nEinen solchen Angriff zu verhindern ist nicht trivial, denn folgendes mÃ¼sste in Kombination gemacht werden:\nSetzen eines Passworts auf BIOS Ebene: Verhindert den Start eines anderen Bootmediums und Ã¤hnliches. Das werden viele aus Bequemlichkeit nicht machen wollen. Aktivieren der FestplattenverschlÃ¼ssung, z.B. mit Bitlocker: Sonst kÃ¶nnte man die Festplatte einfach aus dem Rechner herausschrauben/lÃ¶ten und den Inhalt woanders auslesen. Kontra: Das macht den Windows PC (noch) langsamer. Microsoft selbst kÃ¶nnte wiederum nachbessern indem Sie in Windows:\nden Zugriff auf die Kommandozeile wÃ¤hrend der Installationroutine unterbinden. Vermutlich ist das nicht so einfach umzusetzen, da die Kommandozeile hier und da ebenfalls fÃ¼r SpezialfÃ¤lle benÃ¶tigt werden kÃ¶nnte. die Signaturen von Systemdateien, die vor dem erfolgreichen Login ausgefÃ¼hrt werden, prÃ¼fen. Das sollte sich auf jeden Fall umsetzen lassen und es ist verwunderlich das Microsoft das nicht schon lange tut. Unter Linux gibt es dafÃ¼r schon seit Jahren Implementierungen wie z.B. SELinux. Quellen https://adamtheautomator.com/reset-windows-10-password-command-prompt/ â€“ funktionierte nicht, aber nÃ¼tzliche Tipps zum ProzessversÃ¤ndnis https://www.avast.com/c-recover-windows-password#gref â€“ funktionierte nicht, aber ebenso nÃ¼tzliche Tipps zum ProzessversÃ¤ndnis https://www.makeuseof.com/tag/3-ways-to-reset-the-forgotten-windows-administrator-password/ â€“ funktionierte auch nicht, noch mehr nÃ¼tzliche Tipps zum ProzessversÃ¤ndnis https://www.sony.com/electronics/support/articles/00123047 â€“ wie man einen deaktivierten Windows Account aus der Kommandzeile heraus aktiviert https://www.microsoft.com/en-us/software-download/windows10ISO â€“ Windows 10 ISO Download ","tags":["deutsch"],"section":"blog"},{"date":"1691625600","url":"/blog/docker-fixed-ips/","title":"Feste IP-Adressen mit Docker Compose","summary":"Warum um Himmels willen sollte man Containern feste IP-Adresse zuweisen? Eine absolut berechtigte Frage auf die die Antwort lautet: Am besten nie! Denn: Docker kommt mit einer Art eingebauten DNS- und DHCP-Server und verwaltet die IP-Adresse wunderbar selbst. Leider gibt es trotzdem SpezialfÃ¤lle in denen man sich das Leben vereinfachen kann, wenn man IPs hÃ¤ndisch setzt. So ein Spezialfall wÃ¤re z.B. gegeben, wenn die Applikation die in einem Container betrieben wird nicht mit Hostnamen sondern nur mit IP-Adressen umgehen kann, um auf einen weiteren Container zuzugreifen.\n","content":"Warum um Himmels willen sollte man Containern feste IP-Adresse zuweisen? Eine absolut berechtigte Frage auf die die Antwort lautet: Am besten nie! Denn: Docker kommt mit einer Art eingebauten DNS- und DHCP-Server und verwaltet die IP-Adresse wunderbar selbst. Leider gibt es trotzdem SpezialfÃ¤lle in denen man sich das Leben vereinfachen kann, wenn man IPs hÃ¤ndisch setzt. So ein Spezialfall wÃ¤re z.B. gegeben, wenn die Applikation die in einem Container betrieben wird nicht mit Hostnamen sondern nur mit IP-Adressen umgehen kann, um auf einen weiteren Container zuzugreifen.\nFrÃ¼her war das ein relativ kompliziertes unterfangen, mit den aktuellen Docker (Compose) Versionen ist aber auch das kinderleicht.\nIP-Adressen laufender Container herausfinden Falls du bereits diverse Container innerhalb eines Netzes am laufen hast, wÃ¤re es gefÃ¤hrlich nur einigen davon eine feste IP-Adresse zuzuordnen. Warum? Weil, es dir dann z.B. nach einem Reboot deines Servers passieren kÃ¶nnte, dass sich die automatisch zugewiesene IP eines Containers mit einer von dir fest zugeordneten eines anderen Containers Ã¼berschneidet. Das Resultat wÃ¤re, dass der zweite Container nicht starten kann, da die IP ja bereits vergeben ist.\nUm zu erfahren welche IP-Adresse einem Container aktuell zugewiesen ist, hilft uns das Kommando\n1docker inspect -f \u0026#39;{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; CONTAINERNAMEoderID Falls man die IP-Adressen aller laufenden Container herausfinden mÃ¶chte, ist dieses Kommando sehr nÃ¼tzlich:\n1docker inspect $(docker ps -q ) --format=\u0026#39;{{ printf \u0026#34;%-50s\u0026#34; .Name}} {{range .NetworkSettings.Networks}}{{.IPAddress}} {{end}}\u0026#39; Ein Netz einrichten In unserem Beispiel wollen wir allen Containern die in einem fÃ¼r unseren Caddy Reverse Proxy erstellen Netz liegen feste IP-Adressen vergeben. Falls ihr noch kein solches Netz habt, kÃ¶nnt ihr dieses Ã¼ber den Befehl\n1docker network create --subnet=172.28.0.0/16 proxy anlegen. Nachdem dies erfolgt ist, kÃ¶nnen wir nun in den Docker Compose Definitionen der einzelnen Container die IP-Adressen 172.20.0.X verwenden. Wobei X fÃ¼r die Zahlen 1-255 steht.\nIP-Zuweisung mit Docker Compose Beginnen wir mit der Zuweisung einer festen IP fÃ¼r unseren Caddy Reverse Proxy:\n1services: 2 caddy: 3 container_name: caddy 4 image: caddy:latest 5 restart: unless-stopped 6 ports: 7 - \u0026#34;80:80\u0026#34; 8 - \u0026#34;443:443\u0026#34; 9 - \u0026#34;443:443/udp\u0026#34; 10 volumes: 11 - ./Caddyfile:/etc/caddy/Caddyfile 12 - ./site:/srv 13 - ./caddy_data:/data 14 - ./caddy_config:/config 15 logging: 16 options: 17 max-size: \u0026#34;10m\u0026#34; 18 max-file: \u0026#34;3\u0026#34; 19 networks: 20 proxy: 21 ipv4_address: 172.28.0.1 22volumes: 23 caddy_data: 24 caddy_config: 25networks: 26 proxy: 27 external: true Nach dem speichern und anschlieÃŸendem Neustart der Umgebung, ist die IP fest zugeordnet. Entscheidend sind die BlÃ¶cke:\n1 networks: 2 proxy: 3 ipv4_address: 172.28.0.1 sowie\n1networks: 2 proxy: 3 external: true Nun verfahren wir genauso mit allen anderen Containern mit einer jeweils individuellen IP und haben unser vorhaben erfolgreich beendet.\nFazit Auch wenn die Zuweisung fester IP-Adressen relativ einfach ist, erscheint das Ganze als etwas â€unsauberâ€œ. So widerspricht die Vorgehensweise doch dem Prinzip, dass ich die Definition eines Docker Containers nehmen und jederzeit auf einem anderen Server/Host starten kann. Das ist so nun nicht mehr mÃ¶glich, weil zunÃ¤chst manuell ein neues Netz angelegt werden muss. Anderseits ist die manuelle Erstellung des Netzes sowieso erforderlich, wenn man eine Art DMZ (eigenes Netz) fÃ¼r seinen Reverse Proxy definiert. Nur der manuell vergebene Subnetzbereich ist hier der zusÃ¤tzliche Aufwand. So bleibt die Verwendung fester IP-Adressen als ein Spezialfall der nur in bestimmten Szenarien Sinn macht.\n","tags":["deutsch"],"section":"blog"},{"date":"1689638400","url":"/blog/wordpress-caches/","title":"Redis Objekt- und Caddy Seitencache fÃ¼r WordPress unter Docker einrichten","summary":"WordPress ist nicht gerade berÃ¼hmt dafÃ¼r, besonders schnell zu sein. Es bemÃ¼hen sich daraus reultierend unzÃ¤hlige Plugins auf dem WordPress Marktplatz, mit dem versprechen daran etwas zu Ã¤ndern, darum die Gunst des Administrators zu gewinnen. Oft, vielleicht sogar meistens, kÃ¶nnen diese Plugins ihr versprechen nicht halten oder reiÃŸen sogar neue SicherheitslÃ¼cken im System auf. Vom nichts tun wird die Leistung aber auch nicht besser, weshalb dieser Artikel beschreibt wie man die Performance durch einen Seitencache und einen Objektcache steigern kann, ohne die Sicherheit des Systems aufs Spiel zu setzen.\n","content":"WordPress ist nicht gerade berÃ¼hmt dafÃ¼r, besonders schnell zu sein. Es bemÃ¼hen sich daraus reultierend unzÃ¤hlige Plugins auf dem WordPress Marktplatz, mit dem versprechen daran etwas zu Ã¤ndern, darum die Gunst des Administrators zu gewinnen. Oft, vielleicht sogar meistens, kÃ¶nnen diese Plugins ihr versprechen nicht halten oder reiÃŸen sogar neue SicherheitslÃ¼cken im System auf. Vom nichts tun wird die Leistung aber auch nicht besser, weshalb dieser Artikel beschreibt wie man die Performance durch einen Seitencache und einen Objektcache steigern kann, ohne die Sicherheit des Systems aufs Spiel zu setzen.\nWofÃ¼r sind die Caches da? Ein Seitencache sagt dem Client (Browser) des Besuchers einer Seite, dass er beim erneuten Besuch oder mehrmaligen Aufruf gleicher Dateien, diese nicht jedes Mal wieder neu vom Server laden zu braucht, sondern die Versionen im seinem lokalen Cache (Speicher) nutzen kann. Dadurch wird die Ladezeit der Seite nicht nur stark beschleunigt, sondern auch der Server auf dem die Website liegt entlastet. Der Seitencache sollte andereseits aber auch nur eine bestimmte Zeit lokal vorgehalten werden, damit Ã„nderungen an der Seite selbst bzw. deren Inhalten auch beim Besucher ankommen. Dies lÃ¤sst sich Ã¼ber eine direktive in allen gÃ¤ngigen Webserver Systemen einstellen.\nDer Objektcahce wiederum agiert auf dem Server selbst und lÃ¤d hÃ¤ufig benutzte Elemente einer Website in seinen Arbeitsspeicher. In modernen Varianten passiert das Ã¼ber eine sogenannte In-Memory Datenbank, also einer Datenbank die eben ihre Inhalte im Arbeitspeicher ablegt. Das hat neben der gesteigerten Performance den positiven Nebeneffekt, dass weniger von der Festplatte/SSD geladen werden muss und die Hardware so geschont wird.\nSeiten Cache in Caddy Reverse Proxy aktivieren Um den Seiten Cache fÃ¼r unsere WordPress Installationen zu aktivieren fÃ¼gen wir oben unter den globalen Definitionen in unserem Caddyfile folgende Regel hinzu:\n1(wordpress) { 2 header { 3 Cache-Control \u0026#34;public, max-age=36000, must-revalidate\u0026#34; 4 } 5} Nun importieren wir diese Regel in allen unseren WordPress Defintionen darunter, z.B. so:\n1domain.de { 2 import wordpress 3 reverse_proxy container:80 4} Das war es schon. Nun mÃ¼ssen wir die Caddy Konfiguration nach dem speichern nur einmal neu laden, schon ist der Seitencache Ã¼berall aktiviert.\nRedis Objektcache fÃ¼r WordPress konfigurieren Um einen Redis Objektcache fÃ¼r WordPress zu aktivieren gibt es mehrere MÃ¶glichkeiten. Ich habe mich dafÃ¼r entschieden einen eigenen Redis Docker Container zu starten der dann von WordPress Ã¼ber ein kleines Plugin angesprochen wird. Dazu mÃ¼ssen wir drei Dinge tun.\ncompose.yml anpassen ZunÃ¤chst ergÃ¤nzen wir unsere Compose Datei im einen weiteren Block fÃ¼r den Redis Service. Das kann ungefÃ¤hr so aussehen:\n1 wordpress-redis: 2 image: redis:alpine 3 hostname: wordpress-redis 4 container_name: wordpress-redis 5 restart: unless-stopped 6 networks: 7 - default 8 logging: 9 options: 10 max-size: \u0026#34;10m\u0026#34; 11 max-file: \u0026#34;3\u0026#34; Bitte daran denken das Ganze vorher Ã¼ber docker compose down herunterzufahren und nach dem speichern wieder mit docker compose up -d zu starten. Nun steht uns der Redis Server zur VerfÃ¼gung und es geht mit dem nÃ¤chsten Schritt weiter.\nwp-config.php editieren In der WordPress Konfigurationsdatei â€wp-config.phpâ€œ mÃ¼ssen wir noch zwei Zeilen ergÃ¤nzen. Diese Datei liegt im Stammordner euerer WordPress Installation und ist (soll) in der Regel sowohl lese- als auch schreibgeschÃ¼tzt sein. D.h. zum editieren mÃ¼sst ihr kurz die Berechtigungen auf die Datei so Ã¤ndern, dass ihr sie beschreiben kÃ¶nnt. AnschlieÃŸend nicht vergessen die Berechtigung wieder auf den Ausgangswert einzustellen.\n1define( \u0026#39;WP_REDIS_HOST\u0026#39;, \u0026#39;wordpress-redis\u0026#39; ); 2define( \u0026#39;WP_REDIS_PORT\u0026#39;, 6379 ); Das funktionierte bei mir nur ordnungsgemÃ¤ÃŸ, wenn ich diese Zeilen relativ weit oben in der Datei eingefÃ¼gt habe.\nPlugin installieren AbschlieÃŸend kommen wir um die Installation eines kleinen Plugins, dass WordPress sagt den Redis Cache zu nutzen, nicht ganz herum. Installiert auch dafÃ¼r aus dem Marktplatz das Plugin Redis Object Cache und aktivert dieses.\nIn der Konfiguration des Plugins (unter Einstellungen -\u0026gt; Redis) klickt ihr nun auf â€Object-Zwischenspeicher aktivierenâ€œ.\nFazit Wenn bis hierhin alles funktioniert hat, seid ihr auch schon fertig und sowohl Seiten- als auch Object-Cache sind aktiv. Das kÃ¶nnt ihr euch auch beim betrachten des Website Zuststands in WordPress bestÃ¤tigen lassen.\n","tags":["deutsch"],"section":"blog"},{"date":"1685750400","url":"/blog/buddhismus-fuer-praktiker/","title":"Buddhismus fÃ¼r Praktiker von Volker Zotz *****","summary":"Das Wort Buddha kommt im Buch nur genau 3mal vor, das Cover eingeschlossen. Auch sonst hÃ¤lt der Ã–sterreicher Volker Zotz im 1999 zum ersten Mal erschienen Buch â€Mit Buddha das Leben meisternâ€œ Abstand von Esotherik und Mystifizierung, gibt aber eine gelungene kurze und prÃ¤gnante Zusammenfassung der Leitmotive des klassischen Buddhismus.\nMeine Bewertung: 5 von 5 Sternen\nInhalt Nach dem ersten viertel des Buches, welches den Lebensweg von Gautamas und seiner Begleiter skiziert geht es im wesentlichen um Ãœbungen die in die Tat umgesetzt werden wollen, sowie um philosphische GrundsÃ¤tze an denen sich kein Ãœbel identifizieren lÃ¤sst.\n","content":"Das Wort Buddha kommt im Buch nur genau 3mal vor, das Cover eingeschlossen. Auch sonst hÃ¤lt der Ã–sterreicher Volker Zotz im 1999 zum ersten Mal erschienen Buch â€Mit Buddha das Leben meisternâ€œ Abstand von Esotherik und Mystifizierung, gibt aber eine gelungene kurze und prÃ¤gnante Zusammenfassung der Leitmotive des klassischen Buddhismus.\nMeine Bewertung: 5 von 5 Sternen\nInhalt Nach dem ersten viertel des Buches, welches den Lebensweg von Gautamas und seiner Begleiter skiziert geht es im wesentlichen um Ãœbungen die in die Tat umgesetzt werden wollen, sowie um philosphische GrundsÃ¤tze an denen sich kein Ãœbel identifizieren lÃ¤sst.\nErstaunlicherweise sind viele, wenn nicht die Meisten, AnsÃ¤tze in modernen psychologischen Therapieformen wieder zu finden. Dies ist erstaunlich wenn man berÃ¼cksichtig das diese bereits um die 2.500 Jahre alt sind.\nDie gerade mal knapp Ã¼ber 200 Seiten sind verstÃ¤ndlich geschrieben, regen zum Denken an und kÃ¶nnen dem ein oderen Anderen sicherlich dabei helfen die persÃ¶nliche Mitte (wieder) zu finden. Ob nun aus allgemeiner Neugier, auf der Suche nach Stressreduktion oder aus anderen Motiven: Das Buch kann ich guten Gewissens jedem empfehlen, der seinen Horizont erweitern mÃ¶chte.\nIm folgendenen ein paar AuszÃ¼ge der Ãœbungen und Prinzipien.\nÃœbungen Die Ãœbungen werden hier nur Auszugsweise als Notiz fÃ¼r die DurchfÃ¼hrung umschrieben. Sinn und Zweck entnimmst du am besten direkt aus dem Buch.\nÃœbung 1 (Seite 43): Der TagesrÃ¼ckblick Lass keinen Tag erinnerungslos verstreichen. Vor dem Schlafen das was am Tag geschah im Schnelldurchlauf (nicht an einzelnen Momenten hÃ¤ngen bleiben) Revue passieren lassen. In umgekehrter Reihenfolge, also von abends nach morgens.\nÃœbung 2 (Seite 45): Das gesammelte Aufstehen Sich schon beim Aufstehen, des Moments bewusst sein. Dazu 2-3 Minuten lang mit den Bewusstsein von FÃ¼ÃŸen bis Kopf durch den KÃ¶rper â€wandernâ€œ und dabei das entsprechende KÃ¶rperteil z.B. durch Bewegung bewusst waahrnehmen. In modernen Therapieformen ist dies auch in einer langsameren Version als â€Body Scanâ€œ bekannt. Wer dazu eine gefÃ¼hrte Anleitung mÃ¶chte kann sich diese bei der Techniker Krankenkasse als Audiodatei herunterladen.\nÃœbung 3 (Seite 46): Das Feststellen des Zeitauwandes FÃ¼hre 3 bis 5 Tage genau Buch, womit du Deine Zeit verbringst. Danach das Ergebnis betrachten um eine genaue Ãœbersicht zu erhalten mit welchen Dingen man wieviel Zeit am Tag verbringt.\nÃœbung 4 (Seite 73): Die Zeit zur Meditation Durch Meditation soll Entspannung und wahre Freiheit entstehen. 15 Minuten sind ok. Wichtiger als die LÃ¤nge ist die RegelmÃ¤ÃŸigkeit, am besten immer zu(m) gleichen Zeitpunkt(en). Mehr zur Vorgehensweise unter â€Anleitung zur Meditationâ€œ.\nÃœbung 5 (Seite 76): Glauben und Wissen Man sollte wissen dass man glaubt und nicht glauben, dass man weiss. Damit ist gemeint deine festen Ãœberzeugungen, sowie die eigene ObjektivitÃ¤t und Einstellung anderen gegenÃ¼ber zu hinterfragen. Diese Einsichten ermÃ¶glichen erst Ã¼berhaupt, den eigenen Horizont zu erweitern.\nÃœbung 6 (Seite 77): Was macht mich zu dem, was ich bin? Analysiere die Ergebnisse aus Ãœbung 3 dahingehend, inwiefern regelmÃ¤ÃŸig wiederholte TÃ¤tigkeiten im privaten und/oder Beruf deine PersÃ¶nlichkeit Formen. Frage Vertraute danach wie Sie dein Verhaltene in bestimmten Situationen wahrnehmen. Vergleiche Fremdwahrnehmung mit deinem Selbstbild.\nÃœbung 7 (Seite 102): Das Denken schulen Beginne dein Denken zu beobachten. Gibt es etwas das dich fÃ¼hlen lÃ¤sst ungenÃ¼gend zu sein oder dich selbst an dir stÃ¶rt? Wenn ja, was ist es? Ist es eine Sache oder vieles? WÃ¤hle zunÃ¤chst ein spezifisches Thema und handle nach der Beschreibung im Bereich â€Schulung des Denkensâ€œ.\nÃœbung 8 (Seite 104): Beobachtung der Rede Achte einen Tag lang auf:\nwie du andere grÃ¼ÃŸt wie du Fragen beantwortest was du bestimmten Menschen sagst oder nicht sagst wann und zu wem du die Unwahrheit sagst ob du zu jemanden grob/unfreundlich sprichst ob du dich manchmal oder oft nicht traust zu reden ob du den Impuls zu reden unterdrÃ¼ckst und wie du dich dabei fÃ¼hlst Nimm diese Punkte wahr ohne sie direkt zu bewerten. In der Meditationszeit denkst du dann darÃ¼ber nach was dich wie reden lÃ¤sst.\nÃœbung 9 (Seite 104): Selbstehrlichkeit im Wirken Gestehe dir die wahren Motiven deines Handelns ein. Wenn du an dir arbeiten mÃ¶chtest, solltest du wissen was du bewusst und auch unterbewusst beabsichtigst. Denke auch an Momente aus der Vergangenheit zu denen du anders gehandelt hast als du wolltest und warum dies so war.\nÃœbung 10 (Seite 129): Ã„nderung der Blickrichtung HÃ¶re auf deinen eigenen Schmerz zu wichtig zu nehmen. Wie geht es anderen? Wenn es anderen schlechter geht als dir: Der Grund warum es dir so schlecht geht ist, dass dich andere nicht interessieren. Das nÃ¤chste Mal wenn dich jemand verletzt oder dir etwas schlechtes passiert, dann Ã¤rgerst du dich nicht darÃ¼ber und wirst auch nicht wÃ¼tend, sondern hilfst jemanden oder bist zu einer Person besonders nett. So ist aus etwas schlechtem etwas gutes entstanden.\nÃœbung 11 (Seite 132): Arbeit an der Angst Gautamas Ansicht nach, sollst du dich deinen Ã„ngsten direkt stellen um sie nicht dein Handeln bestimmen zu lassen. Dieser Ansatz wird auch in der modernen Verhaltenstherapie bei Agoraphobien angewandt, siehe https://de.wikipedia.org/wiki/Agoraphobie .\nSchritt 1: Die Angst akzeptieren Auch das bewusstwerden der Angst und die Identifikation der AuslÃ¶ser, wÃ¤hrend der Meditation Ã¤hnelt anderen modernen AnsÃ¤tzen der Psychotherapie.\nSchritt 2: Einteilen der Ã„ngste Einteilung der Ã„ngste nach Dingen die du gar nicht oder nur unwesentlich beeinflussen kannst, wie z.B. Erdbeben und nach Dingen die du direkt beeinflussen kannst, wie z.B. eine PrÃ¼fungsangst. Wenn Ã„ngste aber dein Leben bestimmen empfiehlt der Autor zusÃ¤tzlich professionelle psychologische Konsultation.\nSchritt 3: Gedankenarbeit und Handeln Versuche bei den Dingen an denen du so gut wie nichts Ã¤ndern kannst deine Denkweise darÃ¼ber zu Ã¤ndern.\nÃœbung 12 (Seite 135): Betrachtung Ã¼ber den Tod Zusammengefasst sollst du dich nicht erst mit deinem Tod beschÃ¤ftigen wenn er â€vor der TÃ¼r stehtâ€œ, sondern Ã¼berlegen was du tun wÃ¼rdest wenn du wÃ¼sstest, dass dir nur noch ein begrenzter Zeitraum bleibt. Wenn du das grÃ¼ndlich zusammengefasst hast, Ã¼berlege was du davon nicht vielleicht jetzt schon angehen kannst.\nÃœbung 13 (Seite 162): Wo sitzen meine Fesseln? Gehe wÃ¤hrend der Meditation die ersten 9 der 10 Fesseln (siehe Kapitel â€Zehn Fesselnâ€œ) durch. Am besten beschÃ¤ftigst du dich an jedem Tag mit jeweils einer.\nÃœbung 14 (Seite 162): Erweiterung des Horizonts Nimm dir vor sobald irgend mÃ¶glich etwas neues zu lernen um deinen Horizont zu erweitern. Es ist egal um was es sich dabei handelt, es soll nur nicht aus einem zwanghaften Grund (z.B. Aufgabe von der Arbeitsstelle) geschehen und â€machbarâ€œ sein.\nÃœbung 15 (Seite 163): Zeitweiliger Verzicht Identifiziere deine SÃ¼chte und verzichte auf diese zumindest zeitweise. Also z.B. kein Fernseher fÃ¼r 3 Tage o.Ã¤. . Die eventuell daraus entstehende Leere sollst du positiv und bereichern nutzen. D.h. zum Beispiel die durch das nicht Fernsehen gewonnene Zeit fÃ¼r ein neues Hobby, Sport oder Meditation nutzen.\nAnleitung in die Meditation (Seite 165-186). Meditation = die Mitte finden und sich daran ausrichten um die grÃ¶ÃŸeren ZusammenhÃ¤nge zu erkennen. Auch soll die Meditation dabei helfen das Leben nicht an sich, durch die vielen unbewusst ausgefÃ¼hrten AktivitÃ¤ten, vorbei ziehen zu lassen. Um deine Mitte zu finden wird eine herangensweise aufgeteilt in verschiedene Ãœbungen im Buch empfohlen. Die erste MeditationsÃ¼bung ist das bewusste Beobachten des eigenen Atems. Du setzt dich aufrecht hin, achtest darauf dass du ungestÃ¶rt bist und fÃ¤ngst an auf deine Ein- und Ausatmung zu achten, mÃ¶glichst ohne diese dabei zu beeinflussen. Sobald du bemerkst, dass du gedanklich â€abgedriftetâ€œ bist, Ã¤rgerst du dich nicht darÃ¼ber, sondern setzt die Beobachtung des Atems weiter fort. Verzage nicht, falls dir dies am Anfang sehr schwer fÃ¤llt, das ist vÃ¶llig normal da wir alle gewohnt sind nur auf Ã¤uÃŸere Reize und nicht unser innerstes zu achten. Umso hÃ¤ufiger du die Ãœbung wiederholst, desto leichter wird sie dir fallen. Ebenso das Stille sitze kann schwer fallen, weshalb es auch so wichtig ist an einem ruhigen Ort ungestÃ¶rt zu meditieren. Die Sitzhaltung ist dabei sekundÃ¤r, die Hauptsache ist, dass du so bequem, aufrecht, frei und ungezwungen sitzt, dass du dies fÃ¼r einen lÃ¤ngeren Zeitraum ohne Schmerz und ohne einzuschlafen halten kannst. Falls du dich zu Beginn Ã¼berhaupt nicht auf deinen Atem konzentrieren kannst, beobachte die Dinge in deiner NÃ¤he und nehme sie bewusst wahr. Dem Juckreiz und Positionskorrekturen gibst du ebenso bewusst und langsam nach. Auch kÃ¶nnen die AtemzÃ¼ge gezÃ¤hlt werden, solang bis man sich vollstÃ¤ndig auf das Atemen selbst konzentrieren kann.\nDas Kapitel behandelt weiterhin die Meditation mit dem Fokus auf die eigenen GefÃ¼hle, AblÃ¤ufe in uns selbst sowie die Meditation im tÃ¤glichen Leben.\nFÃ¼nf Regeln Anders als z.B. die 10 Gebote der Bibel, handelt es sich bei den 5 Regeln nicht um Gesetze sondern Ziele die man anstreben soll. Auch lÃ¤sst deren Formulierung bewusst Spielraum zur eigenen Interpretation.\n(Seite 191) Kein Lebewesen bewuÃŸt tÃ¶ten oder verletzen (Seite 196) Nicht-Gegebenes nicht nehmen (Seite 200) Ein sittlich reine Leben fÃ¼hren (Seite 205) LÃ¼gen und grobe Worte vermeiden (Seite 209) Die BewuÃŸtheit nicht durch Drogen trÃ¼ben Zehn Fesseln (Seite 138) Das falsche Selbstbild (Seite 141) Zweifelsucht (Seite 144) Riten und Regeln (Seite 148) Gier nach sinnlicher Wahrnehmung (Seite 150) Groll oder Ãœbelwollen (Seite 152) Verlangen nach Gestalt (Seite 153) Verlangen nach Gestaltlosigkeit (Seite 156) Vergleichender DÃ¼nkel (Seite 159) Aufgeregtheit (Seite 161) Nichtwissen Vier edle Wahrheiten Die Beschreibungen habe ich Wikipedia entnommen und werden im Buch detailierter und verstÃ¤ndlicher mit Beispielen formuliert.\n(Seite 60) Unser Leid â€Das Leben im Daseinskreislauf ist leidvoll: Geburt ist Leiden, Altern ist Leiden, Krankheit ist Leiden, Tod ist Leiden; Kummer, Lamentieren, Schmerz und Verzweiflung sind Leiden. Gesellschaft mit dem Ungeliebten ist Leiden, das GewÃ¼nschte nicht zu bekommen ist Leiden.\n(Seite 66) Wie Leiden entsteht Die Ursachen des Leidens sind Gier, Hass und Verblendung.\n(Seite 69) Leid ist vermeidbar ErlÃ¶schen die Ursachen, erlischt das Leiden.\n(Seite 70) Der Weg Siehe â€der edle achtfache Pfadâ€œ\nDer edle achtfache Pfad Aus der 4. edlen Wahrheit, dem Weg, ergibt sich wiederum der edle achtfache Pfad.\n1. rechte Einsicht/Anschauung â†’ Erkenntnis Rechte Erkenntnis ist die Einsicht in die Vier edlen Wahrheiten vom Leiden, der Leidensentstehung, der LeidenserlÃ¶schung und des zur LeidenserlÃ¶schung fÃ¼hrenden Achtfachen edlen Pfades.\n2. rechte(s/r) Gesinnung/Absicht â†’ Denken â†’ Entschluss Rechte Gesinnung ist der Entschluss zur Entsagung, zum NichtschÃ¤digen, zur Enthaltung von Groll. Rechtes Denken ist ohne Habgier, hasslos in der Gesinnung und groÃŸzÃ¼gig.\n3. rechte Rede Rechte Rede meidet LÃ¼ge, Verleugnung, Beleidigung und GeschwÃ¤tz. Wie die Gedanken ist die Rede heilsam oder unheilsam, nÃ¼tzlich oder unnÃ¼tzlich, wahr oder falsch.\n4. rechte(s) Handeln/Tat Rechtes Handeln vermeidet das TÃ¶ten, Stehlen und sinnliche Ausschweifungen. Im weiteren Sinne bedeutet es ein Leben gemÃ¤ÃŸ den FÃ¼nf Silas, den Tugendregeln des Buddhismus.\n5. rechter Lebenserwerb/-unterhalt Rechter (Lebens)wandel bedeutet, auf unrechten Lebenswandel zu verzichten. Namentlich werden fÃ¼nf Arten von TÃ¤tigkeiten genannt, die ein buddhistischer LaienanhÃ¤nger nicht ausÃ¼ben sollte und zu denen er Andere nicht veranlassen sollte: Handel mit Waffen, Handel mit Lebewesen, Tierzucht und Handel mit Fleisch, Handel mit Rauschmitteln, Handel mit Giften. Im weiteren Sinn bedeutet rechter Lebenserwerb, einen Beruf auszuÃ¼ben, der anderen Lebewesen nicht schadet und der mit dem Edlen achtfachen Pfad vereinbar ist.\n6. rechte(s) Streben/Ãœben/Anstrengung Rechtes Streben oder rechte Einstellung bezeichnet den Willen, Affekte wie Begierde, Hass, Zorn, Ablehnung usw. bei Wahrnehmungen und Widerfahrnissen zu kontrollieren und zu zÃ¼geln. Wie beim â€rechten Denkenâ€œ geht es hier um das PrÃ¼fen seiner Gedanken, und das Austauschen unheilsamer Gedanken durch heilsame Gedanken.\n7. rechte Achtsamkeit/Bewusstheit Rechte Achtsamkeit betrifft zunÃ¤chst den KÃ¶rper: Bewusstwerdung aller kÃ¶rperlichen Funktionen, dem Atmen, Gehen, Stehen usw.; Bewusstwerdung gegenÃ¼ber allen Sinnesreizen, Affekten und Denkinhalten. Sie sollen umfassend bewusst gemacht sein, ohne sie kontrollieren zu wollen. Die Achtsamkeit auf das â€Innereâ€œ prÃ¼ft die Geistesregungen und benennt sie. Es geht um ein Bewusstwerden des stÃ¤ndigen Flusses der GefÃ¼hle und der BewusstheitszustÃ¤nde. Die Achtsamkeit auf â€das Ã„uÃŸereâ€œ bewirkt, ganz im Hier-und-Jetzt zu sein, nicht der Vergangenheit nachzugrÃ¼beln und nicht in der Zukunft zu schwelgen.\n8. rechte Sammlung/Konzentration â†’ Versenkung Rechte Sammlung bezeichnet die Fertigkeit, den unruhigen und abschweifenden Geist zu kontrollieren. HÃ¤ufig auch als einspitziger Geist oder als hÃ¶chste Konzentration bezeichnet, ist sie ein zentraler Teil der buddhistischen SpiritualitÃ¤t. Es geht hier im Wesentlichen um eine buddhistische Meditation, die vor allem die Konzentration auf ein einziges PhÃ¤nomen (hÃ¤ufig den Atem) verwendet, wodurch der Geist von Gedanken befreit wird und zur Ruhe kommt.\nFÃ¼nf Aspekte des Werdens unbelebte Materie belebte Materie menschliches UnbewuÃŸte Wirken (Karma) Dharma (ab Seite 81)\nWeitere Infos Mehr zum Buch bei Wikipedia oder Ã¼ber ISBN 3-499-60586-4\nZu kaufen entweder in jeder lokalen Buchhandlung oder z.B. bei https://www.medimops.de/volker-zotz-mit-buddha-das-leben-meistern-buddhismus-fuer-praktiker-taschenbuch-M03499605864.html\n","tags":["deutsch","BÃ¼cher"],"section":"blog"},{"date":"1682985600","url":"/blog/colors-of-magic/","title":"Die Farben der Magie ***","summary":"Im ersten Band der Scheibenwelt-Reihe wird auf humorvolle Art liebevoll die Geschichte von Rincewind, einem Zauberer ohne abgeschlossenes Zauberstudium, Zweiblum, dem ersten Touristen, und anderen WeggefÃ¤hrten erzÃ¤hlt. Alle ernsthaftig aus anderen Vertretern des Genres wie dem Herr der Ringe oder dem Lied von Eis und Feuer fehlt dem Buch, was grundsÃ¤tzlich nicht slecht sein soll. Die Art des Humors erinnert mich ein wenig an \u0026ldquo;Per Anhalter durch die Galaxie\u0026rdquo;, leider bin ich aber schon zu alt geworden oder die Witze sind nicht ganz so gelungen wie in eben diesem. Wer eine leichte LektÃ¼re mit viel Phantasie und Witz sucht ist mit dem Titel trotzdem gut beraten.\n","content":"Im ersten Band der Scheibenwelt-Reihe wird auf humorvolle Art liebevoll die Geschichte von Rincewind, einem Zauberer ohne abgeschlossenes Zauberstudium, Zweiblum, dem ersten Touristen, und anderen WeggefÃ¤hrten erzÃ¤hlt. Alle ernsthaftig aus anderen Vertretern des Genres wie dem Herr der Ringe oder dem Lied von Eis und Feuer fehlt dem Buch, was grundsÃ¤tzlich nicht slecht sein soll. Die Art des Humors erinnert mich ein wenig an \u0026ldquo;Per Anhalter durch die Galaxie\u0026rdquo;, leider bin ich aber schon zu alt geworden oder die Witze sind nicht ganz so gelungen wie in eben diesem. Wer eine leichte LektÃ¼re mit viel Phantasie und Witz sucht ist mit dem Titel trotzdem gut beraten.\nInhalt Durch unglÃ¼ckliche UmstÃ¤nde wird Rincewind, ein Zauberer der nur einen einzigen Zauerspruch verinnerlicht hat, dazu verdonnert auf den Touristen Zweiblum acht zu geben, damit diesem nichts geschiet. Das erweist sich alles andere als einfach, da Zweiblum die gepflogenheiten der gefÃ¤hrlichen Scheibenwelt vÃ¶llig unklar sind, er alles als ein Abenteuer betrachtet und dazu noch furchtbar naiv ist. Rincewind muss Zweiblum allerdings nicht allein beschÃ¼tzen, denn Zweiblums magische Truhe aus intelligentem Birnenbaumholz und versehen mit hunderten von FÃ¼ÃŸen ist ein groÃŸer Schutzengel der Beide vor viel Unheil bewahrt. Ob gewollt oder nicht bewegen sich die drei von Abenteur zu Abenteuer, an welchen auch die GÃ¶tter nicht unbeteiligt sind. Die Reise endet im ersten Band am Rand der Welt und hÃ¤lt alles fÃ¼r weitere Fortsetzungen offen.\nWeitere Infos Originaltitel: The Colour of Magic Genre: Fantasy Erscheinungsjahr: 1983 Seitenzahl: 256 ISBN: 3-89897-529-0 ErhÃ¤ltlich in jeder Buchhandlung oder unter https://www.medimops.de/terry-pratchett-die-farben-der-magie-ein-roman-von-der-bizarren-scheibenwelt-terry-pratchetts-scheibenwelt-taschenbuch-M03492280625.html ","tags":["deutsch","BÃ¼cher"],"section":"blog"},{"date":"1681516800","url":"/blog/searxng/","title":"Deine eigene Suchmaschine mit SearXNG","summary":"Wir alle wissen um die Macht des Konzerns Alphabet, der hinter der Google Suchmaschine steht: Wir suchen eine bestimmte Information im Internet und ein Algorithmus entscheidet intransparent fÃ¼r uns, welche Ergebnisse uns prÃ¤sentiert werden. NatÃ¼rlich kÃ¶nnte man auf eine andere Suchmaschine ausweichen, aber Google ist doch so schÃ¶n bequemâ€¦\nIn diesem Artikel beschreibe ich, wie man sich mit SearXNG selbst eine (Meta-)Suchmaschine aufsetzen kann um dem Dilemma zumindest ein StÃ¼ck weit zu entkommen.\n","content":"Wir alle wissen um die Macht des Konzerns Alphabet, der hinter der Google Suchmaschine steht: Wir suchen eine bestimmte Information im Internet und ein Algorithmus entscheidet intransparent fÃ¼r uns, welche Ergebnisse uns prÃ¤sentiert werden. NatÃ¼rlich kÃ¶nnte man auf eine andere Suchmaschine ausweichen, aber Google ist doch so schÃ¶n bequemâ€¦\nIn diesem Artikel beschreibe ich, wie man sich mit SearXNG selbst eine (Meta-)Suchmaschine aufsetzen kann um dem Dilemma zumindest ein StÃ¼ck weit zu entkommen.\nSuchmaschine, Suchindex, Meta-Suchmaschine Das HerzstÃ¼ck einer jeden Suchmaschine ist ein Index, hier ein Verzeichnis aller bekannten Webseiten im Internet. Wie sich jeder vorstellen kann, ist der Aufbau eines Index fÃ¼r das Internet kein Leichtes unterfangen und die resultierende Datenbank gigantisch groÃŸ. Ebenso muss so ein Index stÃ¤ndig aktualisiert werden, damit es nicht zu toten Links in den Suchergebnissen kommt.\nDer Betreiber einer echte Suchmaschine muss also einen eigenen Index aufbauen und diese dazu gehÃ¶rigen AufwÃ¤nde stemmen. Deshalb gibt es weit weniger echte Suchmaschinen mit eigenen Index, als man vielleicht denkt. Ein Blick auf den entsprechenden Beitrag in Wikipedia offenbart uns, dass weltweit nur um die 10 Betreiber eines Ã¶ffentlich Index gibt. Reduziert auf die fÃ¼r uns relevanten bleiben Alphabets Google, Microsofts Bing und 1-2 Exoten.\nAlleine einen eigenen Suchindex aufzubauen ist allerdings auch utopisch, weshalb jetzt Meta-Suchmaschinen ins Spiel kommen. Eine Meta-Suchmaschine nimmt unseren Suchbegriff, holt sich die Ergebnisse bei verschiedenen Suchmaschinenbetreibern ab und stellt uns die Ergebnisse Ã¼bersichtlich dar.\nSearXNG \u0026lt;href=\u0026ldquo;https://docs.searxng.org/\u0026quot; target=\u0026quot;_blank\u0026rdquo;\u0026gt;SearXNG ist eine Open Source Projekt das uns eine Meta-Suchmaschine fÃ¼r den eigenen Betrieb oder in Ã¶ffentlich gehosteten Implementierungen zur VerfÃ¼gung stellt. Dadurch entsteht eben nicht nur der Vorteil der weniger limitierten Suchergebnisse, sondern zusÃ¤tzlich auch ein zusÃ¤tzlich grad an Anonymisierung. D.h. Google oder Microsoft kÃ¶nnen die Suchabfragen weder direkt eurem Benutzer als auch nicht eurer IP-Adresse zuordnen. Dieser Effekt wird umso grÃ¶ÃŸer, desto mehr Menschen die jeweilige SearXNG Instanz nutzen. Bei SearXNG handelt es sich um einen sogenannten Fork (Abspaltung/Vergabelung) von SearX, der aktiv gewartet und in vielen Punkten verbessert wurde.\nSearXNG in einer Docker Compose Umgebung betreiben Jetzt wollen wir nicht lÃ¤nger warten und SearXNG auf unserem eigenen Server starten. Wir gehen davon aus, dass wir Docker und einen Reverse Proxy bereits installiert haben. Falls das bei euch nicht der Fall ist schaut euch zunÃ¤chst die entsprechenden Anleitung an. Eine einfache Docker Compose Defintion in der \u0026ldquo;docker-compose.yml\u0026rdquo; kann dann wie folgt aussehen:\n1services: 2 search-redis: 3 container_name: search-redis 4 image: \u0026#34;redis:alpine\u0026#34; 5 command: redis-server --save \u0026#34;\u0026#34; --appendonly \u0026#34;no\u0026#34; 6 networks: 7 - default 8 tmpfs: 9 - /var/lib/redis 10 cap_drop: 11 - ALL 12 cap_add: 13 - SETGID 14 - SETUID 15 - DAC_OVERRIDE 16 search-searxng: 17 container_name: search-searxng 18 image: searxng/searxng:latest 19 networks: 20 - default 21 - proxy 22 volumes: 23 - ./searxng:/etc/searxng:rw 24 environment: 25 - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-search.handtrixxx.com}/ 26 restart: unless-stopped 27 cap_drop: 28 - ALL 29 cap_add: 30 - CHOWN 31 - SETGID 32 - SETUID 33 - DAC_OVERRIDE 34 logging: 35 options: 36 max-size: \u0026#34;1m\u0026#34; 37 max-file: \u0026#34;1\u0026#34; 38networks: 39 proxy: 40 external: true Nach dem hochfahren erhalten wir also 2 Contianer, einen Redis Cache und den Applikationsserver von SearXNG. Entgegen der offiziellen Beispieldokumentation haben wir hier auf einen zusÃ¤tzlichen Caddy Reverse Proxy verzichtet und auch keine Ports exponiert. DafÃ¼r liegt der Applikationscontainer zusÃ¤tzlich im gleichen Netz (Proxy) wie unser generischer Reverse Proxy.\nFazit Wie aus der Konfiguration hervorgehen haben wir jetzt eine laufende Installation, welche Ã¼ber https://search.handtrixxx.com aus dem Internet erreichbar ist. AnschlieÃŸend habe ich noch die Standardsuchengine in meinen lokalen Browsern so umgestellt, dass immer meine eigene Engine verwendet wird. Leider erlaubt Apple dies auf dem iPhone/iPad nicht. Gerne kÃ¶nnt ihr aber ebenfalls meine Instanz verwenden und so zur weiteren Anonymisierung beitragen oder euch eben selbst an einer Installation probieren.\n","tags":["deutsch"],"section":"blog"},{"date":"1677974400","url":"/blog/ncms/","title":"Noch ein Web Content Management System?! Ich stelle vor: nCMS","summary":"Warum sollte man sein eigenes Web Content Management System (WCMS) erstellen wollen, es gibt doch schon hunderte?! VÃ¶llig korrekt und es gibt auch keinen wirklichen Grund so etwas zeitintensives zu tun, auÃŸer natÃ¼rlich man ist mit dem was es gibt nicht so richtig zufrieden und/oder mÃ¶chte einfach selber wissen und ausprobieren wie man so etwas angehen kann. Genau mit dieser Motivation, und der ursprÃ¼nglichen Intention fÃ¼r meine persÃ¶nliche Website ein neues Layout zu entwickeln, habe ich mir innerhalb eines Monats ein eigenes WCMS erstellt: nCMS â€“ â€œniklas stephanâ€™s Content Management Systemâ€ oder auch â€œnode-red Content Management Systemâ€ oder â€œnot another Content Management Systemâ€!\n","content":"Warum sollte man sein eigenes Web Content Management System (WCMS) erstellen wollen, es gibt doch schon hunderte?! VÃ¶llig korrekt und es gibt auch keinen wirklichen Grund so etwas zeitintensives zu tun, auÃŸer natÃ¼rlich man ist mit dem was es gibt nicht so richtig zufrieden und/oder mÃ¶chte einfach selber wissen und ausprobieren wie man so etwas angehen kann. Genau mit dieser Motivation, und der ursprÃ¼nglichen Intention fÃ¼r meine persÃ¶nliche Website ein neues Layout zu entwickeln, habe ich mir innerhalb eines Monats ein eigenes WCMS erstellt: nCMS â€“ â€œniklas stephanâ€™s Content Management Systemâ€ oder auch â€œnode-red Content Management Systemâ€ oder â€œnot another Content Management Systemâ€!\nWas ist nCMS und was kann es? nCMS ist ein Flat-File und Headless Web Content Management System. D.h. im Gegensatz zu z.B. zu WordPress, kommt es ohne eine Datenbank aus und besteht aus Dateien. NCMS basiert im Backend auf Node-Red und darin entwickelten Node.js JavaScript Funktionen.\nÃ„nderungen am Code von NCMS werden durch Deployments in Node-Red geschrieben und die Ã„nderungen an Dateien Ã¼ber GIT in Github synchronisiert und versioniert. Ã„nderungen an Inhalten wie neue Posts werden ebenfalls Ã¼ber Deployments, manuell oder als Webhook gestartet. Alle Dateien werden darauf hin generiert und einem Webserver als statische Dateien zur VerfÃ¼gung gestellt. Das heisst dass der Abruf der Inhate Ã¼ber eine mit nCMS erstellte Website rasend schnell ist. Das erstellen einzelner Blog BeitrÃ¤ge erfolgt in Markup Syntax in einem beliebigem Texteditor.\nWeitere bis jetzt integrierte Features sind:\nMulti-Language Support fÃ¼r Blog BeitrÃ¤ge und alle Seiten Kommentarfunktion in den BlogbeitrÃ¤gen Media Management Ã¼ber einfachen Datei-Upload Template- und Snippet-basierte Erstellung des HTML GerÃ¼sts Explizit kein Einsatz von Frontend Frameworks wie Vue oder Angular, sondern reines â€œVanillaâ€ Javascript. Automatische Generierung von Meta-Daten fÃ¼r Social Media Integration und SEO Volltextsuche auf Basis eines automatisch generierten lokalen Index Freigabefunktion von Posts Ã¼ber ein â€œpublishedâ€ Attribut Und viele mehr, dokumentiert im GitHub Repository des Projekts: https://github.com/handtrixx/ncms\nSchneller, aufgerÃ¤umter und einfacher als WordPress und andere? Ja, ja, ja. Wie genau, erlÃ¤utere ich hier.\nSchnell Im Backend â€“ Ein Deployment (erzeugen der statischen Files fÃ¼r den Webserver) dauert zwischen 70 und 320 Millisekunden.\nIm Frontend â€“ z.B. die Startseite ist trotz aller Animationen, Effekte und Bilder nur 670KB groÃŸ und kann in zwischen 100 und 300 Millisekunden vom Client vollstÃ¤ndig geladen werden.\nAufgerÃ¤umt Im Backend: Der Einsatz von Node-Red gibt eine grafische Ãœbersicht und gewÃ¤hrleistet so, dass wir den Ãœberblick nicht verlieren. Dazu spÃ¤ter noch mehr.\nIm Frontend: Durch den Verzicht auf JavaScript Frameworks und der Einfachheit des Systems an sich, werden wir mit sauberen HTML Code bei der Ausspielung belohnt. FÃ¼r https://niklas-stephan.de habe ich zwar fÃ¼r das Frontend UI auf das HTML5 GrundgerÃ¼st von Bootstrap 5 zurÃ¼ck gegriffen, aber das fÃ¤llt nicht mehr schwer ins Gewicht, und kommt mittlerweile im Standard ebenfalls ohne das aufgeblÃ¤hte jQuery Framework zurecht.\nEinfach Im Backend: Assets, Medien, Templates und Snippets werden Ã¼ber Visual Studio Code oder einen anderen Texteditor + Dateimanager bereitgestellt. In Node-Red wird das alles zusammengefÃ¼gt, durch JavaScript Code und node.js Module um FunktionalitÃ¤ten erweitert und das Endergebnis schlieÃŸlich in Form von statischen Dateien in ein Verzeichnis auf einem nginx webserver zur VerfÃ¼gung gestellt. Das ist der Kern von nCMS.\nIm Frontend: Frontend: Die Wahl zwischen Deutsch und Englisch erfolgt entweder automatisch oder wird manuell festegelegt. Es gibt eine Startseite, eine Seite zu Suche, eine Seite zu Datenschutz und Impressum, eine Seite zum abfangen ungÃ¼ltiger Aufrufe und eine Seite mit der Ãœbersicht Ã¼ber alle Posts. Selsbstredend ist das komplette UI auf alle EndgerÃ¤tearten zur Darstellung optimiert.\nHinter den Kulissen Der komplette Aufbau von nCMS erfolgt in einer Docker-Compose Umgebung. Alle Dateien aus dem Volume src liegen ebenfalls auf https://github.com/handtrixx/ncms.\nHier meine Beispielkonfiguration auf Basis des offiziellen Node-Red Images von Docker Hub (https://hub.docker.com/r/nodered/node-red):\n1services: 2 ncms: 3 container_name: ncms 4 hostname: ncms 5 image: nodered/node-red:latest 6 restart: always 7 environment: 8 - TZ=Europe/Berlin 9 networks: 10 - dmz 11 logging: 12 options: 13 max-size: \u0026#34;10m\u0026#34; 14 max-file: \u0026#34;3\u0026#34; 15 volumes: 16 - ./data:/data 17 - ./dist:/dist:rw 18 - ./src:/src:rw 19 - /etc/localtime:/etc/localtime:ro 20networks: 21 dmz: 22 external: true ErkÃ¤rungsbedÃ¼rftig sind hier eigentlich nur die Netzwerkonfiguration und die verschiedenen Volumes. Wie auch in meinen anderen BeitrÃ¤gen zu Matomo, Boinc und anderen setze ich auf meinem Cloud Server einen Reverse Proxy auf Basis des Nginx Proxy Managers (https://hub.docker.com/r/jc21/nginx-proxy-manager) ein. Das Volume/Verzeichnis ./distist in meinem Reverse Proxy ebenfalls Ã¼ber ein ln -s verlinkt, so dass man keine doppelten Deployments machen muss und ein weiterer Webserver obsolet bleibt. Node-Red selbst, also das Backend ist Ã¼ber eine eigene Subdomain erreichbar.\nVerzeichnisstruktur Im Verzeichnis ./src befinden sich folgende Unterordner:\nassets css fonts img js json md posts media x snippets templates In das Verzeichnis assets und dessen Unterordner gehÃ¶ren alle im Frontend wiederholt benÃ¶tigten Dateien, wie z.B. css und javascript von Bootstrap 5 (https://getbootstrap.com/), aber natÃ¼rlich vor allem auch eigene Stylesheets und Javascript Funktionen. Im Unterordner json legen wir unsere statischen ÃœbersetzungschlÃ¼ssel ab.\nDer Ordner md enthÃ¤lt in seinem Unterordner posts offensichtlich alle unsere in Markup geschriebenen Posts, welche auch an ihrer Dateiendung .md erkennbar sind.\nIn das Verzechnis media und maximal eine Unterodnerebene tiefer kÃ¶nnen wir alle Bilder â€œwerfenâ€ die wir in unseren Posts verwenden wollen. Diese werden dann wÃ¤hrend des Deployments automatisch ins platzsparende .webp Format konvertiert und ein Thumbnail fÃ¼r jedes Bild generiert.\nIm Ordner snippteshaben wir alle HTML Elemente die wir auf allen Seiten immer wieder benÃ¶tigen gelegt.\nAnalog dazu liegen im Verzeichnis templates die Ausgangsdateien unseres Frontends, die wÃ¤hrend des Deployments mit Inhalt angereichtert werden.\nAll das lÃ¤sst sich Transparent auch im Github Projekt unter: https://github.com/handtrixx/ncms nachvollziehen.\nTemplates Im Verzeichnis Templates befinden sich folgende Dateien: 404.html â€“ Die Fehlerseite die immer dann angezeigt wird, wenn eine ungÃ¼ltige Abfrage auf die Website erfolgt. blog.html â€“ Die Seite die die Ãœbersicht Ã¼ber alle Posts bereit stellt. index.html â€“ Die Startseite mit ihren Inhalten. post.html â€“ Die Vorlage aus der die einzelen Beitragsseiten gerendert werden. privacy-policy.html â€“ Die Seite zu Datenschutz und im Impressum auf die im Footer verllinkt ist und die somit von Ã¼berall aus erreichbar ist und sein muss. robots.txt â€“ Infos fÃ¼r die Crawler von Suchmaschinen. search.html â€“ Meine Seite auf der man Suchen kann und die Ã¼ber einen json Index sÃ¤mtliche Suchergebnisse ohne Abfrage am Server bereitstellt.\nSnippets Die Snippets die spÃ¤ter im Deployment in alle HTML Templates eingeschlÃ¤ust werden sind footer.html, head.html, navbar.html, script.html . Durch die Aufteilung in diese Snippets haben wir den massiven Vorteil, dass wir im Falle einer gewÃ¼nschten Anpassung z.B. im NavigationsmenÃ¼, diese nur genau einmal durchfÃ¼hren mÃ¼ssen um sie auf allen Seiten zu Ã¤ndern.\nProgrammlogik mit node.js und Node-Red Node-Red basiert auf node.js und erlaubt uns in einer Art Ablaufdiagramm verschiedene Elemente und Funktionen miteinander zu verbinden. Das nennt sich in Node-Red â€œFlowâ€. In NCMS nutze ich auch nur diese BasisfunktionalitÃ¤t von Node-Red und keine weitere Plug-Ins aus der Palette. Stattdessen lade ich in den verschiedenen Nodes node.js Pakete nach um den Funktionsumfang des Systems zu erweitern.\nIm folgenden eine ErlÃ¤uterung zu den einzelen Nodes.\n/deploy Hierbei handelt sich um einen lauschenden http in Node, um den Start eines Deplyoments durch einen Webhook zu starten. D.h. z.B. durch curl -X POST -d \u0026lsquo;key=\u0026mdash;-\u0026rsquo; https://ncms.niklas-stephan.de/deploy startet man das Deployment.\ncatch key NatÃ¼rlich soll nicht einfach jeder ein Deployment starten kÃ¶nnen, deshalb noch eine kleine Sicherheitsabfrage im catch key Node.\nIm Node unter â€œSetupâ€ wird das npm Modul fs-extra geladen und als fse bereit gestellt, damit wir Zugriff auf das Dateisystem haben und den im Ordner /data hinterlegten key mit dem vergleichen kÃ¶nnen, der uns fÃ¼r das Deployment im Webhook zur VerfÃ¼gung gestellt wurde.\nDie Funktion selbst sieht dann so aus:\n1const transferedKey = msg.payload.key; 2const systemKey = fse.readFileSync(\u0026#39;/data/deploy.key\u0026#39;, \u0026#39;utf8\u0026#39;) 3 4if (transferedKey == systemKey) { 5 msg.payload = \u0026#34;Deployment Started\u0026#34;; 6 msg.statusCode = 200; 7 msg.type = \u0026#34;webhook\u0026#34;; 8 msg.starttime = Date.now(); 9 return [null,msg]; 10} else { 11 msg.payload = \u0026#34;Wrong authentication!\u0026#34; 12 msg.statusCode = 400; 13 return [msg,null]; 14} Auch hat dieser Node 2 AusgÃ¤nge. Falls die beiden keys Ã¼bereinstimmen wird mit dem Deployment fortgefahren â€“ Ausgang 2. Falls aber nicht, dann wird eine Fehlernachricht an Ausgang 1 Ã¼bergeben und das Deployment damit abgebrochen.\ndeploy Startet das Deployment ebenfalls, aber eben manuell Ã¼ber die Node-Red OberflÃ¤che und nicht als Webhook.\nget posts In diesem Node laden wir den Inhalt aller Posts aus den *.md Dateien und speichern diesen als Objekte in einem Array zu spÃ¤teren Verwendung ab. AuÃŸerdem machen wir neben dem fs-extra Modul noch intensiven Gebrauch des Moduls markdown-it und Plugins fÃ¼r diesen. markdown-it (https://github.com/markdown-it/markdown-it) hilft uns dabei den Inhalt von Markup nach HTML zu konvertieren.\n1msg.baseurl = \u0026#34;https://niklas-stephan.de\u0026#34; 2msg.dist = {}; 3msg.posts = []; 4const path = \u0026#39;/src/md/posts/\u0026#39;; 5const postfiles = fse.readdirSync(path) 6const alength = postfiles.length; 7 8for (var i=0; i\u0026lt;alength; i++) { 9 var srcFile = path+postfiles[i]; 10 var distFilename = postfiles[i].split(\u0026#39;.\u0026#39;)[0]+\u0026#34;.html\u0026#34;; 11 var srcContent = fse.readFileSync(srcFile, \u0026#39;utf8\u0026#39;) 12 13 var md = new markdownIt({ 14 html: true,linkify: true,typographer: true,breaks: true}) 15 .use(markdownItFrontMatter, function(metainfo) {meta = JSON.parse(metainfo);}) 16 .use(markdownItLinkifyImages, {target: \u0026#39;_blank\u0026#39;,linkClass: \u0026#39;custom-link-class\u0026#39;,imgClass: \u0026#39;custom-img-class\u0026#39;}) 17 .use(markdownItLinkAttributes, { attrs: {target: \u0026#34;_blank\u0026#34;,rel: \u0026#34;noopener\u0026#34;,} 18 }); 19 20 distContent = md.render(srcContent); 21 22 let data = {\u0026#34;srcFile\u0026#34;:\u0026#34;\u0026#34;+srcFile+\u0026#34;\u0026#34;,\u0026#34;srcContent\u0026#34;:\u0026#34;\u0026#34;+srcContent+\u0026#34;\u0026#34;,\u0026#34;distContent\u0026#34;:\u0026#34;\u0026#34;+distContent+\u0026#34;\u0026#34;,\u0026#34;distFilename\u0026#34;:\u0026#34;\u0026#34;+distFilename+\u0026#34;\u0026#34;,...meta}; 23 24 msg.posts.push(data) 25} 26 27return msg; get snippets Ãœber das npm Modul fs-extra laden wir den Inhalt unserer snippets und speichern diese als Array msg.snippets, damit wir spÃ¤ter im Flow darauf zugreifen kÃ¶nnen.\n1msg.snippets = {}; 2const path = \u0026#39;/src/snippets/\u0026#39;; 3const files = fse.readdirSync(path) 4const alength = files.length; 5 6for (var i=0; i\u0026lt;alength; i++) { 7 var srcFile = path+files[i]; 8 var srcContent = fse.readFileSync(srcFile, \u0026#39;utf8\u0026#39;) 9 10 msg.snippets[files[i]] = srcContent; 11} 12 13return msg; get templates Genauso wie die Snippets speichern wir auch den Inhalt der Templates in unserem Flow zur spÃ¤teren Verwendung.\n1msg.templates = {}; 2const path = \u0026#39;/src/templates/\u0026#39;; 3const files = fse.readdirSync(path) 4const alength = files.length; 5 6for (var i=0; i\u0026lt;alength; i++) { 7 var srcFile = path+files[i]; 8 var srcContent = fse.readFileSync(srcFile, \u0026#39;utf8\u0026#39;) 9 10 msg.templates[files[i]] = srcContent; 11} 12 13return msg; create index.html Nun fangen wir an die Inhalte der einzelnen Dateien zu generieren, den Start macht unsere index.html Datei. Die obere HÃ¤lfte des Codes betrifft das generieren der Meta Tags fÃ¼r Social Media und Suchmaschinen. Im zweiten Block fÃ¼gen wir in die Platzhalter des Templates die Werte Snippets, den Seitentitel, sowie die Metadaten ein. Letztlich steht unsere fertige index.html als Objekt msg.dist.index in unserem Flow bereit um spÃ¤ter als Datei geschrieben zu werden.\n1msg.dist.index = \u0026#34;\u0026#34;; 2var ogmetalang = \u0026#34;de_DE\u0026#34;; 3var ogmeta = ` 4\u0026lt;meta property=\u0026#34;og:type\u0026#34; content=\u0026#34;website\u0026#34;\u0026gt; 5\u0026lt;meta property=\u0026#34;og:locale\u0026#34; content=\u0026#34;`+ogmetalang+`\u0026#34;\u0026gt; 6\u0026lt;meta property=\u0026#34;og:site_name\u0026#34; content=\u0026#34;niklas-stephan.de\u0026#34;\u0026gt; 7\u0026lt;link rel=\u0026#34;canonical\u0026#34; href=\u0026#34;`+msg.baseurl+`/index.html\u0026#34;\u0026gt; 8\u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 9\u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;Projects \u0026amp; Blog - niklas-stephan.de\u0026#34;\u0026gt; 10\u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 11\u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;`+msg.baseurl+`/index.html\u0026#34;\u0026gt; 12\u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt; 13\u0026lt;meta property=\u0026#34;og:image:secure_url\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt; 14\u0026lt;meta name=\u0026#34;twitter:card\u0026#34; content=\u0026#34;summary\u0026#34;\u0026gt; 15\u0026lt;meta name=\u0026#34;twitter:description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 16\u0026lt;meta name=\u0026#34;twitter:title\u0026#34; content=\u0026#34;Projects \u0026amp; Blog - niklas-stephan.de\u0026#34;\u0026gt; 17\u0026lt;meta name=\u0026#34;twitter:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt;` 18 19msg.dist.index = msg.templates[\u0026#34;index.html\u0026#34;].replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 20msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 21msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 22msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 23msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,\u0026#34;Home\u0026#34;); 24msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- meta tags --\u0026gt;\u0026#34;,ogmeta); 25 26return msg; create 404.html Die 404.html ist schnell zusammen gebaut. wir fÃ¼gen alle Snippets in unser Template ein und geben der Seite einen Namen. AbschlieÃŸend steht unser Objekt als msg.dist.errorpage zur VerfÃ¼gung.\ncreate privacy-policy.html Genauso ein â€œLow-Brainerâ€ ist die Seite mit Datenschutz und Impressum und schnell als msg.dist.privacy aufbereitet.\n1msg.dist.privacy = \u0026#34;\u0026#34;; 2 3msg.dist.privacy = msg.templates[\u0026#34;privacy-policy.html\u0026#34;].replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 4msg.dist.privacy = msg.dist.privacy.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 5msg.dist.privacy = msg.dist.privacy.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 6msg.dist.privacy = msg.dist.privacy.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 7msg.dist.privacy = msg.dist.privacy.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,\u0026#34;Datenschutz \u0026amp; Impressum\u0026#34;); 8 9return msg; create search.html Bevor es wieder ein wenig komplizierter wird, zunÃ¤chst noch die einfach Erzeugung des Objekts msg.dist.searchindex zur spÃ¤teren Verwendung.\n1msg.dist.search = \u0026#34;\u0026#34;; 2 3msg.dist.search = msg.templates[\u0026#34;search.html\u0026#34;].replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 4msg.dist.search = msg.dist.search.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 5msg.dist.search = msg.dist.search.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 6msg.dist.search = msg.dist.search.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 7msg.dist.search = msg.dist.search.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,\u0026#34;Suche\u0026#34;); 8 9return msg; create search index.json Um den Suchindex aufzubauen, der spÃ¤ter eine Suche ermÃ¶glicht ohne eine Abfrage an den Server zu stellen, verwenden ich eine for Schleife die durch alle Elemente im Array msg.posts lÃ¤uft und fÃ¼r jeden Beitrag einen Eintrag im Index als JSON Objekt erzeugt. Letztlich wird der Index als Objekt msg.dist.searchindex bereit gestellt.\n1var alength = msg.posts.length; 2var index = \u0026#34;[\u0026#34;; 3 4for (var i=0; i\u0026lt;alength; i++) { 5 index = index+`{\u0026#34;lang\u0026#34;:\u0026#34;`+msg.posts[i].language+`\u0026#34;,\u0026#34;link\u0026#34;:\u0026#34;/posts/`+msg.posts[i].distFilename+`\u0026#34;,\u0026#34;headline\u0026#34;:\u0026#34;`+msg.posts[i].title+`\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;`+msg.posts[i].distContent.replace(/[^a-zA-Z0-9]/g, \u0026#39; \u0026#39;)+`\u0026#34;},`; 6} 7index = index.slice(0, -1); 8index = index+\u0026#34;]\u0026#34;; 9 10msg.dist.searchindex = index; 11 12return msg; create sitemap.xml Bei der Erzeugung der Sitemap gehen wir Ã¤hnlich vor wie beim Suchindex. Anstelle einer klassischen for Schleife verwende ich die Javascript forEach() Funktion, die im Endeffekt das gleiche bewirkt, nur etwas moderner ist.\n1var xml = `\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; 2\u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\u0026#34;\u0026gt; 3 \u0026lt;url\u0026gt; 4 \u0026lt;loc\u0026gt;`+msg.baseurl+`/\u0026lt;/loc\u0026gt; 5 \u0026lt;priority\u0026gt;1.00\u0026lt;/priority\u0026gt; 6 \u0026lt;/url\u0026gt; 7 \u0026lt;url\u0026gt; 8 \u0026lt;loc\u0026gt;`+msg.baseurl+`/index.html\u0026lt;/loc\u0026gt; 9 \u0026lt;priority\u0026gt;0.80\u0026lt;/priority\u0026gt; 10 \u0026lt;/url\u0026gt; 11 \u0026lt;url\u0026gt; 12 \u0026lt;loc\u0026gt;`+msg.baseurl+`/blog.html\u0026lt;/loc\u0026gt; 13 \u0026lt;priority\u0026gt;0.80\u0026lt;/priority\u0026gt; 14 \u0026lt;/url\u0026gt;` 15 16msg.posts.forEach(postxml); 17 18xml = xml + ` 19\u0026lt;/urlset\u0026gt;` 20msg.dist.sitemap = xml; 21return msg; 22 23 24function postxml(item) { 25 if (item.published == true ) { 26 xml = xml + ` 27 \u0026lt;url\u0026gt; 28 \u0026lt;loc\u0026gt;`+msg.baseurl+`/posts/`+item.distFilename+`.html\u0026lt;/loc\u0026gt; 29 \u0026lt;priority\u0026gt;0.64\u0026lt;/priority\u0026gt; 30 \u0026lt;/url\u0026gt;` 31 } 32} create posts Wir haben zur im get posts Node bereits den Inhalt der einzelnen Posts im Array msg.posts konvertiert und bereit gestellt. Jetzt wollen wir dies noch dahingehend finalisieren, dass wir analog zu den zuvor erzeugten Dateien auch fÃ¼r jeden Post eine einzelne Datei erzeugen kÃ¶nnen und lege diese wiederum als Objekt im Objekt msg.dist.posts ab.\n1msg.dist.posts = {}; 2var alength = msg.posts.length; 3var data = \u0026#34;\u0026#34;; 4var ogmetalang = \u0026#34;\u0026#34;; 5var ogmeta = \u0026#34;\u0026#34;; 6var postdate = \u0026#34;\u0026#34;; 7 8for (var i=0; i\u0026lt;alength; i++) { 9 10 if (msg.posts[i].language == \u0026#34;de\u0026#34;) { 11 ogmetalang = \u0026#34;de_DE\u0026#34; 12 } else { 13 ogmetalang = \u0026#34;en_US\u0026#34; 14 } 15 16 img = msg.posts[i].imgurl.split(\u0026#39;.\u0026#39;)[0]+\u0026#34;.webp\u0026#34;; 17 18 19 ogmeta = ` 20 \u0026lt;meta property=\u0026#34;og:type\u0026#34; content=\u0026#34;website\u0026#34;\u0026gt; 21 \u0026lt;meta property=\u0026#34;og:locale\u0026#34; content=\u0026#34;`+ogmetalang+`\u0026#34;\u0026gt; 22 \u0026lt;meta property=\u0026#34;og:site_name\u0026#34; content=\u0026#34;niklas-stephan.de\u0026#34;\u0026gt; 23 \u0026lt;link rel=\u0026#34;canonical\u0026#34; href=\u0026#34;`+msg.baseurl+`/posts/`+msg.posts[i].language+`/`+msg.posts[i].distFilename+`\u0026#34;\u0026gt; 24 \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;`+msg.posts[i].excerpt+`\u0026#34;\u0026gt; 25 \u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;`+msg.posts[i].title+`\u0026#34;\u0026gt; 26 \u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;`+msg.posts[i].excerpt+`\u0026#34;\u0026gt; 27 \u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;`+msg.baseurl+`/posts/`+msg.posts[i].language+`/`+msg.posts[i].distFilename+`\u0026#34;\u0026gt; 28 \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/media/full/`+img+`\u0026#34;\u0026gt; 29 \u0026lt;meta property=\u0026#34;og:image:secure_url\u0026#34; content=\u0026#34;`+msg.baseurl+`/media/full/`+img+`\u0026#34;\u0026gt; 30 \u0026lt;meta name=\u0026#34;twitter:card\u0026#34; content=\u0026#34;summary\u0026#34;\u0026gt; 31 \u0026lt;meta name=\u0026#34;twitter:description\u0026#34; content=\u0026#34;`+msg.posts[i].excerpt+`\u0026#34;\u0026gt; 32 \u0026lt;meta name=\u0026#34;twitter:title\u0026#34; content=\u0026#34;`+msg.posts[i].title+`\u0026#34;\u0026gt; 33 \u0026lt;meta name=\u0026#34;twitter:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/media/full/`+img+`\u0026#34;\u0026gt;` 34 35 postdate = \u0026#39;\u0026lt;small class=\u0026#34;c-gray pb-3\u0026#34; id=\u0026#34;postdate\u0026#34;\u0026gt;\u0026#39;+msg.posts[i].date+\u0026#39;\u0026lt;/small\u0026gt;\u0026#39;; 36 37 data = \u0026#34;\u0026#34;; 38 data = msg.templates[\u0026#34;post.html\u0026#34;]; 39 40 41 data = data.replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 42 data = data.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 43 data = data.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 44 data = data.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 45 46 data = data.replace(\u0026#34;\u0026lt;!-- mardown content from posts --\u0026gt;\u0026#34;,msg.posts[i].distContent); 47 data = data.replace(\u0026#34;\u0026lt;!-- Post Headline --\u0026gt;\u0026#34;, msg.posts[i].title); 48 data = data.replace(\u0026#34;\u0026lt;!-- postdate --\u0026gt;\u0026#34;, postdate); 49 data = data.replace(\u0026#34;\u0026lt;!-- Post Image --\u0026gt;\u0026#34;, \u0026#39;\u0026lt;img src=\u0026#34;/media/thumb/\u0026#39;+img+\u0026#39;\u0026#34; class=\u0026#34;img-fluid mb-2\u0026#34; alt=\u0026#34;postImage\u0026#34;\u0026gt;\u0026#39;); 50 data = data.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,msg.posts[i].title); 51 data = data.replace(\u0026#34;\u0026lt;!-- meta tags --\u0026gt;\u0026#34;,ogmeta); 52 53 msg.dist.posts[msg.posts[i].distFilename] = data; 54} 55 56return msg; create blog.html Nun unser letzter vorbereitender Streich, die Erzeungung der Ãœbersicht aller Posts im Objekt msg.dist.blog. Es ist am Umfang der Funktion ersichtlich, dass hier etwas mehr als bei den anderen Dateien passiert.\nZuerst sammeln wir alle definierten Kategorien aus den einzelnen Posts Ã¼ber eine for Schleife ein um dann doppelt vorhandene Werte wieder aus dem erzeugtem Array zu lÃ¶schen. Das brauchen wir damit die Besucher unserer Website spÃ¤ter Ã¼ber Kategorien filtern kÃ¶nnen. Jede Kategorie bekommt auÃŸerdem eine eindeutige Farbe, die einer CSS Klasse in unserem Stylesheet entspricht, zugeordnet.\nAls nÃ¤chstes extrahieren wir aus den Metadaten der Posts noch Titel, Kurzbeschreibung, Datum und Bild. WÃ¤hrend dieser for-Schleife verhindert eine if-Bedingung, dass wir unverÃ¶ffentliche Posts zur Auswahl aufbereiten.\nAbschlieÃŸend erzeugen wir aus den ermittelten Werten den entsprechenden HTML Code, fÃ¼gen die Daten der Snippets ein, ergÃ¤nzen die Meta Informationen zur Seite und schreiben das Ganze in das Objekt msg.dist.blog.\n1//get categories from all posts and extract unique ones 2var categories = []; 3for (var i=0 ; i\u0026lt;msg.posts.length;i++) { 4 if (msg.posts[i].published == true) { 5 for (var j = 0; j \u0026lt; msg.posts[i].keywords.length; j++) { 6 categories.push(msg.posts[i].keywords[j]); 7 } 8 } 9} 10var uniqueCategories = [...new Set(categories)]; 11 12// define a color to stick for each category 13const colorcat = {}; 14var catcolors = [\u0026#34;green\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;yellow\u0026#34;, \u0026#34;pink\u0026#34;, \u0026#34;purple\u0026#34;,\u0026#34;indigo\u0026#34;]; 15var c=0; 16 for (const key of uniqueCategories) { 17 colorcat[key] = catcolors[c]; 18 c = c+1; 19 } 20 21 22//generate and set html for categorie selection 23var cathtml = \u0026#34;\u0026#34;; 24for (var k = 0; k \u0026lt; uniqueCategories.length; k++) { 25 26 cathtml = cathtml + `\u0026lt;button data-filter=\u0026#34;.cat-`+uniqueCategories[k]+`\u0026#34; type=\u0026#34;button\u0026#34; 27 onclick=\u0026#34;sort()\u0026#34; class=\u0026#34;btn bg-` + catcolors[k] + ` c-white me-2\u0026#34;\u0026gt;` + uniqueCategories[k]+`\u0026lt;/button\u0026gt;`; 28} 29msg.dist.blog = msg.templates[\u0026#34;blog.html\u0026#34;].replace(\u0026#34;\u0026lt;!-- CATEGORIES --\u0026gt;\u0026#34;, cathtml); 30 31 32// get card content from all posts and generate html 33var posthtml = \u0026#34;\u0026#34;; 34for (var l = 0; l \u0026lt; msg.posts.length; l++) { 35 if (msg.posts[l].published == true) { 36 37 var link = msg.posts[l].filename.slice(0, -3)+\u0026#34;.html\u0026#34;; 38 39 //get color for current post 40 var postcolor = \u0026#34;\u0026#34;; 41 for (const key in colorcat) { 42 if (key == msg.posts[l].keywords[0]) { 43 postcolor = colorcat[key]; 44 } 45 } 46 47 imgurl = msg.posts[l].imgurl.split(\u0026#39;.\u0026#39;)[0]+\u0026#34;.webp\u0026#34;; 48 49 50 posthtml = posthtml + ` 51 \u0026lt;div class=\u0026#34;col-sm-6 col-lg-4 my-4 filterDiv cat-`+msg.posts[l].keywords[0]+` lang-`+ msg.posts[l].language + `\u0026#34;\u0026gt; 52 \u0026lt;span class=\u0026#34;date hidden d-none\u0026#34;\u0026gt;`+ msg.posts[l].date + `\u0026lt;/span\u0026gt; 53 \u0026lt;span class=\u0026#34;name hidden d-none\u0026#34;\u0026gt;`+ msg.posts[l].title + `\u0026lt;/span\u0026gt; 54 \u0026lt;div onclick=\u0026#34;goto(\u0026#39;`+ link+ `\u0026#39;,\u0026#39;blog\u0026#39;)\u0026#34; class=\u0026#34;card h-100 d-flex align-items-center bo-`+postcolor+`\u0026#34;\u0026gt; 55 \u0026lt;div class=\u0026#34;card-header bg-`+ postcolor + `\u0026#34;\u0026gt;` + msg.posts[l].keywords[0] + `\u0026lt;/div\u0026gt; 56 \u0026lt;div class=\u0026#34;card-img-wrapper d-flex align-items-center\u0026#34;\u0026gt; 57 \u0026lt;img src=\u0026#34;media/thumb/`+ imgurl + `\u0026#34; 58 class=\u0026#34;card-img-top\u0026#34; alt=\u0026#34;iot\u0026#34;\u0026gt; 59 \u0026lt;/div\u0026gt; 60 \u0026lt;div class=\u0026#34;card-body\u0026#34;\u0026gt; 61 \u0026lt;h5 class=\u0026#34;card-title\u0026#34;\u0026gt;`+ msg.posts[l].title + `\u0026lt;/h5\u0026gt; 62 \u0026lt;p class=\u0026#34;card-text\u0026#34;\u0026gt; 63 `+ msg.posts[l].excerpt + ` 64 \u0026lt;/p\u0026gt; 65 \u0026lt;/div\u0026gt; 66 \u0026lt;div class=\u0026#34;card-footer small text-center c-gray pdate\u0026#34;\u0026gt; 67 `+msg.posts[l].date+` 68 \u0026lt;/div\u0026gt; 69 \u0026lt;/div\u0026gt; 70 \u0026lt;/div\u0026gt; 71 `; 72 } 73} 74 75msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- POSTS --\u0026gt;\u0026#34;, posthtml); 76 77var ogmetalang = \u0026#34;de_DE\u0026#34;; 78var ogmeta = ` 79\u0026lt;meta property=\u0026#34;og:type\u0026#34; content=\u0026#34;website\u0026#34;\u0026gt; 80\u0026lt;meta property=\u0026#34;og:locale\u0026#34; content=\u0026#34;`+ogmetalang+`\u0026#34;\u0026gt; 81\u0026lt;meta property=\u0026#34;og:site_name\u0026#34; content=\u0026#34;niklas-stephan.de\u0026#34;\u0026gt; 82\u0026lt;link rel=\u0026#34;canonical\u0026#34; href=\u0026#34;`+msg.baseurl+`/blog.html\u0026#34;\u0026gt; 83\u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 84\u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;Projects \u0026amp; Blog - niklas-stephan.de\u0026#34;\u0026gt; 85\u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 86\u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;`+msg.baseurl+`/blog.html\u0026#34;\u0026gt; 87\u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt; 88\u0026lt;meta property=\u0026#34;og:image:secure_url\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt; 89\u0026lt;meta name=\u0026#34;twitter:card\u0026#34; content=\u0026#34;summary\u0026#34;\u0026gt; 90\u0026lt;meta name=\u0026#34;twitter:description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 91\u0026lt;meta name=\u0026#34;twitter:title\u0026#34; content=\u0026#34;Projects \u0026amp; Blog - niklas-stephan.de\u0026#34;\u0026gt; 92\u0026lt;meta name=\u0026#34;twitter:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt;` 93 94msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- meta tags --\u0026gt;\u0026#34;,ogmeta); 95msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 96msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 97msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 98msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 99msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,\u0026#34;Projekte \u0026amp; Blog\u0026#34;); 100 101return msg; write files Jetzt wollen wir endlich unsere mÃ¼hevoll erzeugten Objekte in msg.dist als reale Dateien festschreiben. In unserer Funktion â€œwrite filesâ€ binden wir dafÃ¼r wieder das npm modul fs-extra als fse ein.\nAlle erzeugten Dateien sollem im Verzeichnis /dist/ landen. Gleichzeitig wollen wir auch alle spuren vorheriger Deployments lÃ¶schen, damit uns â€œDateileichenâ€ und Ã¤hnliche nicht zu Inkonsitenzen fÃ¼hren.\nDas den fse Funktionen vorangestellte await stellt eine sequentielle AusfÃ¼hrung der einzelnen Schritte sicher, in dem es wartet bis der jeweilige Aufruf auch komplett abgeschlossen ist. Also wird in der Funktion im ersten Schritt das Verzeichnes /dist komplett geleert und dann alle benÃ¶tigten Unterordner wieder leer erstellt.\nAls nÃ¤chstes kopieren wir unsere Assets von /src/assets/ nach /dist/assets/ und machen das Gleiche mit der robots.txt Datei.\nAnschlieÃŸend schreiben wir die Inhalte der Objekte in msg.dist in die jeweilige Datei fest, um dann in einer Schleife durch alle Posts zu gehen und diese ebenfalls in das Dateisystem zu schreiben.\n1await fse.emptyDir(\u0026#39;/dist\u0026#39;).then(() =\u0026gt; { 2 fse.mkdirSync(\u0026#39;/dist/media\u0026#39;); 3 fse.mkdirSync(\u0026#39;/dist/media/full\u0026#39;); 4 fse.mkdirSync(\u0026#39;/dist/media/thumb\u0026#39;); 5 fse.mkdirSync(\u0026#39;/dist/posts\u0026#39;); 6}); 7 8await fse.copySync(\u0026#39;/src/assets/\u0026#39;, \u0026#39;/dist/assets/\u0026#39;); 9await fse.copySync(\u0026#39;/src/templates/robots.txt\u0026#39;, \u0026#39;/dist/robots.txt\u0026#39;); 10 11await fse.writeFileSync(\u0026#39;/dist/index.html\u0026#39;, msg.dist.index); 12await fse.writeFileSync(\u0026#39;/dist/blog.html\u0026#39;, msg.dist.blog); 13await fse.writeFileSync(\u0026#39;/dist/404.html\u0026#39;, msg.dist.errorpage); 14await fse.writeFileSync(\u0026#39;/dist/privacy-policy.html\u0026#39;, msg.dist.privacy); 15await fse.writeFileSync(\u0026#39;/dist/search.html\u0026#39;, msg.dist.search); 16await fse.writeFileSync(\u0026#39;/dist/sitemap.xml\u0026#39;, msg.dist.sitemap); 17await fse.writeFileSync(\u0026#39;/dist/searchindex.json\u0026#39;, msg.dist.searchindex); 18 19const postfilefolder = \u0026#34;/dist/posts/\u0026#34; 20 21await Object.entries(msg.dist.posts).forEach(item =\u0026gt; { 22 var [key, value] = item; 23 fse.writeFileSync(postfilefolder+key, value) 24}); 25 26return msg; convert media files Noch etwas komplexer ist die Erzeugung der Mediendatein. Die Dateien aus dem Verzeichnis /src/media/ und einer Unterordnerebene tiefer, wollen wir ins Speicher sparende .webp Format konvertieren und zusÃ¤tzlichen jeweils einen Thumbnail in geringerer AuflÃ¶sung generieren. AuÃŸerdem wollen wir das nur Medien mit den Quellformaten .jpg, .jpeg, .png oder .gif konvertiert werden. Alle anderen Dateien werden ohne Ã„nderung direkt nach /dist/media/full/ kopiert. Um uns die Arbeit zu erleichten greifen wir auf die npm Module fs-extra als fse und sharp zurÃ¼ck.\nMan beachte auch, das ich das await Kommando bewusst bei der Erzeugung der Dateien ausspare, so dass die Schleifen bereits mit der nÃ¤chsten Datei aus ihren Arrays starten, bevor die Schreiboperation abgeschlossen ist. Das beschleunigt das Deployment um einen sehr groÃŸen Faktor, bei dem geringen Risiko bzw. dem akzeptieren Umstand, dass eine Mediendatei noch nicht geschrieben/verfÃ¼gbar ist, wenn das Deplyoment abgeschlossen ist.\n1var srcpath = \u0026#39;/src/media/\u0026#39;; 2var fullpath = \u0026#39;/dist/media/full/\u0026#39;; 3var thumbpath = \u0026#39;/dist/media/thumb/\u0026#39;; 4const filetypes = [\u0026#34;jpg\u0026#34;, \u0026#34;jpeg\u0026#34;, \u0026#34;png\u0026#34;, \u0026#34;gif\u0026#34;, \u0026#34;webp\u0026#34;]; 5const mediafiles = fse.readdirSync(srcpath); 6const alength = mediafiles.length; 7var mediafile = \u0026#34;\u0026#34;; 8var targetfile = \u0026#34;\u0026#34;; 9 10for (var i=0; i\u0026lt;alength; i++) { 11 mediafile = mediafiles[i]; 12 13 if (mediafile.includes(\u0026#34;.\u0026#34;)) { 14 targetfile = mediafile.split(\u0026#39;.\u0026#39;)[0]; 15 if (filetypes.includes(mediafile.split(\u0026#39;.\u0026#39;)[1]) ) { 16 sharp(srcpath+mediafile) 17 .toFile(fullpath+targetfile+\u0026#39;.webp\u0026#39;); 18 sharp(srcpath+mediafile).resize({ width: 440 }) 19 .toFile(thumbpath+targetfile+\u0026#39;.webp\u0026#39;); 20 } else { 21 fse.copySync(srcpath+mediafile, fullpath+mediafile) 22 } 23 24 } else { 25 var subsrcpath = srcpath+mediafile+\u0026#34;/\u0026#34;; 26 var subfullpath = fullpath+mediafile+\u0026#34;/\u0026#34;; 27 var subthumbpath = thumbpath+mediafile+\u0026#34;/\u0026#34;; 28 var submediafiles = fse.readdirSync(subsrcpath); 29 var blength = submediafiles.length; 30 await fse.mkdirSync(subfullpath); 31 await fse.mkdirSync(subthumbpath); 32 33 for (var j=0; j\u0026lt;blength; j++) { 34 submediafile = submediafiles[j]; 35 subtargetfile = submediafile.split(\u0026#39;.\u0026#39;)[0]; 36 37 if (filetypes.includes(submediafile.split(\u0026#39;.\u0026#39;)[1]) ) { 38 sharp(subsrcpath+submediafile) 39 .toFile(subfullpath+subtargetfile+\u0026#39;.webp\u0026#39;); 40 41 sharp(subsrcpath+submediafile).resize({ width: 440 }) 42 .toFile(subthumbpath+subtargetfile+\u0026#39;.webp\u0026#39;); 43 } else { 44 fse.copySync(subsrcpath+submediafile, subfullpath+submediafile) 45 } 46 } 47 } 48} 49 50return msg; finish Unser Deployment nÃ¤hert sich dem Ende. Wir errechnen noch die Dauer des Deployments und setzen einen Zeitstempel um diese Informationen in der Datei /dist/deploy.log festzuhalten. Die Datei wird dann wieder mit dem npm Modul fs-extra als fse geschrieben. Je nach AuslÃ¶ser des Depyloyments, also manuell vs. webhook, wird dann abschlieÃŸnd an ein Debug Node oder an den Debug Node und einen http-out Node weitergeleitet.\n1var endtime = Date.now(); 2var duration = endtime-msg.starttime; 3duration = duration; 4let yourDate = new Date() 5var depdate = yourDate.toISOString(); 6 7var log = \u0026#34;Deployment duration: \u0026#34;+duration+\u0026#34; ms \\n\u0026#34;; 8log = log+\u0026#34;Deployment timestamp: \u0026#34;+depdate; 9msg.statusCode = 200; 10 11fse.writeFileSync(\u0026#39;/dist/deploy.log\u0026#39;, log); 12msg.payload = log; 13 14if (msg.type != \u0026#34;manual\u0026#34;) { 15 return [msg,msg]; 16} else { 17 return [msg,null]; 18} msg \u0026amp; http Fertig! Die Beiden Nodes msg und http dienen zum sauberen Abschluss unseres Deployments. Der â€œhttp outâ€ Node liefert den zuvor definierten Body als Nachricht zurÃ¼ck an den aufrufenden Webhook.\nDer â€œmsgâ€ Debug Node zeigt uns den kompletten Inhalt des Deployments im Debugger von Node-Red an, wenn aktiviert.\nDen kompletten Node-Red Flow kÃ¶nnt ihr euch auch hier herunterladen https://niklas-stephan.de/media/orig/ncms/flow.json (Version 0.60).\nFazit Zugegeben, hÃ¤tte ich eine Stoppuhr genutzt um aufzuzeichnen wie lange die Entwicklung von nCMS gedauert hat, vielleicht hÃ¤tte ich irgendwann abgebrochen. Aber: von Start bis Ende des Projekts, ganz wie bei einem rundenbasierten Strategiespiel, war regelmÃ¤ÃŸig der â€œnur diese eine Sache nochâ€ Moment da. Eine Menge SpaÃŸ hat es auÃŸerdem gemacht, mit Hilfe von Node-Red immer ausgefeiltere JavaScript Funktionen zu entwickeln. Der fest integrierte Debugger war dabei eine fast genauso groÃŸe Hilfe, wie die MÃ¶glichkeit in Node-Red javascript Funktionen maximal einfach auf weitere node.js Module zuzugreifen. Falls ihr mal eine Ã¤hnliches Vorhaben umsetzen mÃ¶chtet, kÃ¶nnt ihr gerne meine Quellen Auf Github forken.\nEinige verbesserungswÃ¼rdige Schwachstellen gibt es natÃ¼rlich auch noch.\nZum Beispiel an den Stellen, bei denen ich im Node-Red Backend auf das Frontend referenziere. Das macht das Ganze etwas weniger flexibel, denn wenn wirklich mal jemand meine Quellen nutzen mÃ¶chte, mÃ¼sste sie/er sich entsprechend noch in das Frontend einarbeiten.\nUnd Ã¼berhaupt bin ich hier im Artikel nicht auf das HTML5 Frontend mit javascript Funktionen, CSS und HTML weiter eingegangen.\nVielleicht folgt das ein andernmal.\nQuellen / WeiterfÃ¼hrende Links Hier nochmal alle Quellen, Links und Dateien aus dem Artikel zusammen aufgefÃ¼hrt:\nDie Quellen von NCMS bei Github: https://github.com/handtrixx/ncms Der Node-Red Flow von NCMS: https://niklas-stephan.de/media/full/ncms/ncms_flow.json Das fantastastische Node-Red: https://nodered.org/ Node-Red als Container bei Docker Hub: https://hub.docker.com/r/nodered/node-red Ngnix Proxy Manager: https://nginxproxymanager.com/ Das famose Bootstrap HTML5 Template von Twitter: https://getbootstrap.com/ markdown-it â€“ von Markup nach HTML in einfach: https://markdown-it.github.io/ Dateisystemoperation in node.js mit fs-extra: https://github.com/jprichardson/node-fs-extra Rasend schnelle Konvertierung von Bildern in node.js mit sharp: https://sharp.pixelplumbing.com/ ","tags":["deutsch"],"section":"blog"},{"date":"1670025600","url":"/blog/ir-remote/","title":"IR-Remote Control 4.0 â€“ Alles ins IoT!","summary":"Universalfernbedienung? Mehr 90er geht es ja nicht! Was aber wenn die alte Infrarot Fernbedienung alle Lichter im Raum ein- und ausschalten, die Heizung oder Rollos bedienen und das Heimkino gleichzeitig steuern kann? Dieser Beitrag hat es in sich, denn auf diese Art lÃ¤sst sich jedes irgendwie auslesbare/anbindbare GerÃ¤t in euer Alexa, HomeKit oder nahezu jede andere Smart Home LÃ¶sung integrieren! Das alles ohne Abos, versteckte Kosten und auf Wunsch auch ganz ohne die Cloud irgendeines groÃŸen Konzerns.\n","content":"Universalfernbedienung? Mehr 90er geht es ja nicht! Was aber wenn die alte Infrarot Fernbedienung alle Lichter im Raum ein- und ausschalten, die Heizung oder Rollos bedienen und das Heimkino gleichzeitig steuern kann? Dieser Beitrag hat es in sich, denn auf diese Art lÃ¤sst sich jedes irgendwie auslesbare/anbindbare GerÃ¤t in euer Alexa, HomeKit oder nahezu jede andere Smart Home LÃ¶sung integrieren! Das alles ohne Abos, versteckte Kosten und auf Wunsch auch ganz ohne die Cloud irgendeines groÃŸen Konzerns.\nZielsetzung In dieser kleinen Aufbauanleitung beschreibe ich wie ich aus mehreren Komponenten ein IoT (Internet of Things) Szenario aufgebaut habe. Ein grundsÃ¤tzliches technisches VerstÃ¤ndnis hilft dabei die einzelnen Schritte nachzuvollziehen, ein IT-Studium oder eine Ã¤hnliche Ausbildung ist aber definitiv nicht erforderlich!\nDas Ziel ist es mit einer handelsÃ¼blichen (gerne auch aus dem untersten Preissegment, oder noch besser: Elektroschrott) Infrarot Fernbedienung verschiedene Aktionen in einem Smart Home auszulÃ¶sen. Ich will von einer einfachen Hardwareumgebung aus, beim Eintreffen eines Infrarotsignals, dessen Eingabewerte an meine Heimautomatisierung Ã¼bergeben.\nAls Hardware nutzen wir den kleinen Bruder des weltberÃ¼hmten Raspberry Pi, den Zero W und wenig weiteres ZubehÃ¶r. Alles davon kann auch ohne MÃ¼hen fÃ¼r andere Zwecke genutzt werden. Dank tausenden von verfÃ¼gbaren â€œBastelanleitungenâ€ im Netz ist die eventuelle Neuanschaffung auf keinen Fall umsonst.\nBei der Software wÃ¤hle ich wie immer mÃ¶glichst ausschlieÃŸlich Open Source Quellen, wie z.B. Linux und Node-RED. Die gigantische Community dahinter leistet uns eine 24/7 UnterstÃ¼tzung bei jeglicher Art von Problemstellung.\nHardware Viel mehr als in der Zielsetzung beschrieben braucht es wirklich nicht. Falls du die Komponenten nicht herumliegen hast, hier zunÃ¤chst ein paar Kaufempfehlungen.\nAb ca. 16 â‚¬ gibt es den Kleinstcomputer Raspberry Pi Zero W mit passendem GehÃ¤use z.B. hier: https://www.berrybase.de/raspberry-pi/raspberry-pi-computer/kits/raspberry-pi-zero-w-zusammenstellung-zero-43-case. Der Zero ist fÃ¼r unseren Zweck besonders gut geeignet, weil wir zum einen keine besonders groÃŸe Leistung benÃ¶tigen, zum anderen aber Strom sparen wollen. Denn wir planen einen Dauerbetrieb. Mit einer Spannung von 2,3 bis 5,0 V und einer StromstÃ¤rke von 100 bis 140 mA, mÃ¼sste der tatsÃ¤chliche Stromverbrauch zwischen 0,23 Watt und 0,7 Watt liegen. Das ist weniger als so mancher Fernseher im Standby â€œfrisstâ€. Das klingt vielleicht alles super â€œbilligâ€, winzig und schwach, aber: Es ist 2021 und das bisschen Hardware hat die gleiche Rechenleistung wie ein Spitzen-Heimcomputer kurz nach der Jahrtausendwende. Und Windows XP wollen wir ja gar nicht installierenâ€¦\nDamit der Raspberry funktioniert braucht es noch Strom, am besten aus einem alten Handynetzteil mit Micro-USB Anschluss. Falls gerade keines verfÃ¼gbar ist, bekommt man so etwas Ãœberall fÃ¼r wenige Euro, so z.B. auch im Supermarkt. Hinzu kommt noch eine Micro-SD Karte, die vielleicht auch noch irgendwo zuhause herumliegt. Alles ab 8GB GrÃ¶ÃŸe ist OK, nur sollte sie Ã¼ber den Zusatz â€œXDâ€ oder â€œHCâ€ verfÃ¼gen, damit man sicher ist nicht eine â€œlahme Enteâ€ zu verwenden. Da geht es bei ca. 3 â‚¬ los und auch hier wird man in den meisten SupermÃ¤rkten fÃ¼ndig.\nEin kleines Adapterkabel hilft uns GerÃ¤te mit normalgroÃŸem (Typ A) USB-Anschluss, wie den InfrarotempfÃ¤nger, mit dem Raspberry Pi Zero zu verbinden. Das wiederum wird man eher selten im Supermarkt finden, aber z.B. hier: https://www.berrybase.de/raspberry-pi/raspberry-pi-computer/kabel-adapter/usb-kabel-adapter/usb-2.0-hi-speed-otg-adapterkabel-a-buchse-micro-b-stecker-0-15m-wei-223. Und damit ist man weitere 1,80 â‚¬ Ã¤rmer. Wer nun Ã¼ber gar keine dieser Komponenten verfÃ¼gt, kann sich auch ein Komplettset besorgen, z.B. hier: https://www.berrybase.de/raspberry-pi/raspberry-pi-computer/kits/raspberry-pi-zero-w-zusammenstellung-full-starter-kit Um die SD-Karte spÃ¤ter mit dem Betriebssystem bespielen zu kÃ¶nnen, brauchen wir noch ein SD-Kartenleser. Die meisten Laptops haben einen solchen integriert. Aber falls nicht, gibt es die auch gÃ¼nstig ab 5â‚¬ fast Ã¼berall und z.B. hier: https://www.heise.de/preisvergleich/sandisk-mobilemate-single-slot-cardreader-sddr-b531-gn6nn-a2356696.html\nBei der Fernbedienung hat man die Qual der Wahl. Wirklich jegliche Art von GerÃ¤t, das ein Infrarotsignal sendet, kommt hier in Frage. Es sollte natÃ¼rlich keine genommen werden, die zu einem aktiv benutzten GerÃ¤t im gleichen Raum gehÃ¶rt, aber das versteht sich hoffentlich von selbst. Ich habe eine alte Universalfernbedienung benutzt, die ich noch Ã¼brighatte. Damit habe ich es mir selbst einen kleinen Tick schwieriger gemacht als nÃ¶tig, da ich diese nun auf ein nicht vorhandenes EmpfangsgerÃ¤t voreinstellen musste. Einfacher wÃ¤re es gewesen, wirklich eine ganz â€œnormaleâ€ zu verwenden.\nZum Schluss noch der InfrarotempfÃ¤nger. Das ist tatsÃ¤chlich fast die grÃ¶ÃŸte Investition hier im Projekt. Ich hatte noch den, qualitativ hochwertigen und mit cleverer SoftwarelÃ¶sung kommenden, FLIRC USB-EmpfÃ¤nger. Den bekommt man z.B. hier: https://www.heise.de/preisvergleich/flirc-usb-rev-2-flirc-v2-a1621664.html, ab ca. 24 â‚¬\nAlles in allem also eine Investition zwischen 0â‚¬ und maximal 65â‚¬ Euro.\nSoftware Nun zum zumindest fÃ¼r AnfÃ¤nger komplizierten Teil, aber immerhin: viel Software braucht es nicht. Eigentlich nur ein Betriebssystem und zwei zusÃ¤tzliche Programme/Tools. Da sich die Fernbedienung am einfachsten Ã¼ber eine Desktopumgebung konfigurieren lassen wird, installieren wir diese auf unsere SD-Karte. Das geht mittlerweile maximal einfach. Unter https://www.raspberrypi.org/software/ gibt es das Programm â€œRaspberry Pi Imagerâ€, welches ihr dort fÃ¼r euer Betriebssystem herunterladen kÃ¶nnt. Vor dem Ã–ffnen des heruntergeladenen und installiertem Programms steckt ihr noch die SD-Karte in euren SD-Karten-LesegerÃ¤t bzw. -slot. Im Programm wÃ¤hlt ihr dann das Betriebssystem â€œRaspberry PI OS (32-Bit)â€ aus, und als SD-Karte die eurige. Mit einem Klick auf â€œSCHREIBENâ€ wird das Betriebssystem auf eure Karte kopiert.\nSobald der Vorgang abgeschlossen ist, entfernt ihr die Karte bitte noch NICHT. Denn wir wollen zuvor noch zwei Dinge erledigen, die uns das Leben anschlieÃŸend erleichtern. Falls schon, vor Ungeduld oder welchem Grund auch immer, herausgezogen ist das auch kein Problem, einfach wieder rein damit. ZunÃ¤chst erstellen wir im Hauptverzeichnis der SD-Karte eine, gerne auch leere, Datei mit dem Namen â€œsshâ€. Bitte achtet darauf, dass die Datei keine versteckte Endung wie z.B. â€œ.txtâ€ hat. Das passiert gerne unter den Standardeinstellungen von Windows. Mac und Linux Anwender haben dieses Problem in der Regel nicht. Wem das zu ungenau beschrieben ist, der guckt bitte hier: https://www.shellhacks.com/raspberry-pi-enable-ssh-headless-without-monitor/.\nJetzt mÃ¶chten wir noch gleich die Verbindungsdaten zu unserem WLAN angeben, damit die Netzwerkverbindung von Anfang an sichergestellt ist. Dazu legen wir eine zweite Datei Namens â€œwpa_supplicant.confâ€ in das Gleiche Verzeichnis. Windows Benutzer passen wieder bei der Dateiendung auf. Dieses Mal kommt es aber auf den Inhalt der Datei an:\n1country=DE 2ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev 3update_config=1 4network={ 5 ssid=\u0026#34;DerNameDeinesWLANs\u0026#34; 6 psk=\u0026#34;DeinWLANPasswort\u0026#34; 7 key_mgmt=WPA-PSK 8} Das ist nur ein Beispiel, und wir mÃ¼ssen die Werte in den Klammern bei den Zeilen â€œssidâ€ und â€œpskâ€ auf die Werte unseres WLANs setzen.\nJetzt sind wir aber fertig und die Karte kann entfernt und in den Raspberry gesetzt werden. Nun steckt ihr alle Komponenten zusammen und bringt das Ganze an seinen Bestimmungsort. Sobald ihr das GerÃ¤t mit dem Netzteil verbunden habt, seht ihr:\nErstmal nicht viel. Kein Grund zur Panik, das ist normal und ihr habt vermutlich alles richtig gemacht. Wir machen einfach weiter.\nWir sind jetzt in der Lage uns per SSH auf dem kleinen Raspberry anzumelden, um dort ein paar Dinge einzustellen. Um sich kurz auf Kommandozeilenebene die Finger schmutzig zu machen, kommen wir leider nicht herum und leider lÃ¤sst sich dafÃ¼r im Falle des Raspberry Pi Zeros auch nicht Visual Studio Code verwenden. Ein Terminal muss her.\nUnter MacOS und Linux ist eine Terminal Applikation fÃ¼r die Kommandozeile seit je her eingebaut und heiÃŸt dort auch so. Soweit ich weiÃŸ, steht das SSH Kommando auch in Windows 10 zur VerfÃ¼gung, kann das aber leider gerade nicht testen. Allen die damit nicht vertraut sind, hilft Google gerne weiter. Die Verbindung zu unserem neuen Kleinstcomputer stellen wir dann wie folgt her:\n1ssh pi@192.168.178.22 Die IP-Adresse muss natÃ¼rlich auf die eures Raspi angepasst werden. Die bekommt ihr am schnellsten Ã¼ber das Webinterface eures Routers heraus. Das Standardpasswort nachdem ihr nun gefragt werdet ist: raspberry . Wenn ihr das nach der Anmeldung gleich Ã¤ndern mÃ¶chtet, geht das mit dem Befehl â€œpasswdâ€. Es empfiehlt sich Betriebssystem und Software regelmÃ¤ÃŸig auf den neuesten Stand zu bringen, das macht man mit den Befehlen:\n1sudo apt-get update um aktuell verfÃ¼gbare Pakete zu ermitteln, um diese dann wiederum mit:\n1sudo apt-get upgrade zu installieren. Jetzt installieren wir Node-RED mit dem Befehl:\n1bash \u0026lt;(curl -sL https://raw.githubusercontent.com/node-red/linux-installers/master/deb/update-nodejs-and-nodered) und wenn wir schon dabei sind, bestimmen wir noch, dass Node-RED bei jedem Systemstart automatisch gestartet wird:\n1sudo systemctl enable nodered.service Nun starten wir das System am besten einmal neu. Das geht mit:\n1sudo reboot Nach spÃ¤testens 3-4 Minuten sollte alles wieder hochgefahren sein und wir mÃ¼ssten in der Lage sein auf Node-RED, Ã¼ber den Browser eines beliebigen GerÃ¤tes in eurem Netzwerk, zuzugreifen:\nhttps://192.168.178.22:1880 Auch hier die IP-Adresse durch die eures Raspis ersetzen. Wenn sich nun die WeboberflÃ¤che von Node-RED Ã¶ffnet, kÃ¶nnen wir unseren ersten Erfolg feiern. Wir haben aus unserem kleinen Raspi ein mÃ¤chtiges IoT Werkzeug gemacht!\nJetzt wollen wir uns daran machen unsere Fernbedienung zu konfigurieren. DafÃ¼r mÃ¼ssen wir noch einmal die Kommandozeile bemÃ¼hen und uns wie zuvor beschrieben auf den Raspi schalten. Als Erstes prÃ¼fen wir, ob uns der Desktop des Raspis spÃ¤ter zur VerfÃ¼gung stehen wird. Wie das geht, ist perfekt hier: https://www.raspberrypi.org/documentation/remote-access/vnc/ beschrieben. Dort auf den Abschnitt â€œEnabling VNC Server at the command lineâ€ fokussieren, den Rest kÃ¶nnen wir fÃ¼r den Moment ignorieren.\nAuch auf Kommandozeilenebene installieren wir die grafische OberflÃ¤che fÃ¼r unseren Flirc InfrarotempfÃ¤nger. Das geht mit (copy \u0026amp; paste):\n1curl apt.flirc.tv/install.sh | sudo bash Jetzt haben wir alles zusammen was wir brauchen und kÃ¶nnen endlich mit dem richtigen SpaÃŸ beginnen.\nKonfiguration Verglichen mit der Softwareinstallation, ist die eigentliche Konfiguration nun ein â€œKlacksâ€.\nIm ersten Schritt lernen wir unsere Fernbedienung an. Dazu schalten wir uns per VNC auf den Desktop des Raspberry PI Zero. Dort Ã¶ffnen wir dann das Programm Flirc, welches wir im StartmenÃ¼ finden. Bevor es losgeht, stellen wir die Ansicht noch auf â€œKeyboardâ€ um. Dadurch stehen uns schon ohne Tastenkombinationen mehr als 127 Kommandos (Tasten) zur VerfÃ¼gung, die wir auf unserer Fernbedienung belegen kÃ¶nnen. Da wir niemals eine echte Tastatur an den Raspi anschlieÃŸen werden, ist das Einzige, worauf wir achten mÃ¼ssen, dass wir keine Sondertasten belegen. Die â€œDruckenâ€ Taste zu belegen ist also keine gute Idee, alle Zahlen und Buchstaben: kein Problem. Um die Belegung zu starten, klicken wir im Programm auf eine der angezeigten Tasten, also z.B. â€œFâ€. Dadurch werden wir nun aufgefordert unsere Fernbedienung in Richtung des Infrarot EmpfÃ¤nger zu halten und darauf ebenfalls einen Knopf zu drÃ¼cken. Dieses Prozedere spielen wir fÃ¼r alle KnÃ¶pfe auf unserer Fernbedienung durch, die wir spÃ¤ter mit einer Aktion hinterlegen wollen. Jetzt ist unsere Fernbedienung â€œangelerntâ€ und wir kÃ¶nnen die VNC Session wieder beenden. Das FLIRC Programm kann vorher auch beendet werden, es lÃ¤uft im Hintergrund weiter und unsere Kommandos stehen auch nach einem Neustart weiterhin zur VerfÃ¼gung.\nAb hier darf jetzt Node-RED zeigen was es kann. Und das Programm macht es uns maximal einfach! Wir Ã¶ffnen wieder im Browser eines beliebigen EndgerÃ¤ts unseres Netzwerks die Node-RED GUI:\nhttps://192.168.178.22:1880 Unser Flow (Ablauf) benÃ¶tigt nur zwei Knoten aus dem â€œStandardsortimentâ€. Als erstes ziehen wir aus der linken Leiste einen â€œrpi â€“ keyboardâ€ Knoten in den Flow. Der macht nichts anderes als, wie der Name es erahnen lÃ¤sst, auf Tastatureingaben am Raspi zu lauschen. Durch unsere Konfiguration von FLIRC hÃ¶rt er so automatisch auch auf die Eingaben unserer Fernbedienung. Wer das Testen oder einfach sehen mÃ¶chte, ob es funktioniert, fÃ¼gt einen Debug nun in den Flow ein und verbindet diesen mit dem Ausgang unseres â€œrpi â€“ keyboardâ€ Nodes. Nach einem Klick auf Deploy oben rechts sollte nun nach jedem druck auf der Fernbedienung auf einen angelernten Knopf, eine Nachricht im Debug Fenster auftauchen. So weit so gut.\nNun wollen wir uns mit der AuÃŸenwelt verbinden. DafÃ¼r stehen uns unterschiedlichste MÃ¶glichkeiten zur Auswahl. So kÃ¶nnen wir die Ereignisse bereits mit Node-Red Bordmitteln in eine Datei schreiben, mit einem Webservice Aufruf koppeln oder auch eine Nachricht Ã¼ber einen Websocket Ã¼bertragen. Auch stehen MQTT, TCP, UDP und sogar die serielle Ausgabe als ÃœbertragungsmÃ¶glichkeit im Standard offen. Damit sollte eigentlich schon jeder Wunsch abgedeckt sein, aber noch mehr Komfort zur Integration in bestehende Smart Home Netze gibt es durch optionale installierbare Module. So machen wir unser Setup mit dem Modul â€œnode-red-contrib-homekit-bridgedâ€ z.B. zu einem HomeKit EndgerÃ¤t nach Apple Standard. Das gleiche funktioniert auch mit OpenHab https://flows.nodered.org/node/node-red-contrib-openhab3 oder sogar Alexa https://flows.nodered.org/node/node-red-contrib-virtual-smart-home und eigentlich allem was es so auf dem Markt gibt und Ã¼ber ein Netzwerk kommuniziert. Ich persÃ¶nlich habe mich fÃ¼r die Ãœbertragung via Websockets ohne zusÃ¤tzliches Modul entschieden.\nHalt, Stop! Warum auf einmal jetzt Websockets? In meinem ersten Aufbau hatte ich die Kommandos der Fernbedienung Ã¼ber eine â€œnormaleâ€ API zu Ã¼bertragen versucht. Das funktioniert zwar ebenso prÃ¤chtig ist aber zu langsam! Beim DrÃ¼cken eines Knopfes auf einer Fernbedienung erwartet jeder eine sofortige Reaktion bzw. Aktion. 1 Sekunde VerzÃ¶gerung fÃ¼hlt sich unnatÃ¼rlich an, 2 Sekunden sind bereits unertrÃ¤glich. Man stelle sich vor man will z.B. die LautstÃ¤rke des Fernsehers stÃ¼ck fÃ¼r stÃ¼ck erhÃ¶hen. Ganz schnell habe ich 20-mal oder Ã¶fter gedrÃ¼ckt und die API-Kommandos werden langsam abgearbeitet. So arbeitet man erfolgreich an einem HÃ¶rsturz. Jeder der jetzt gerade Kopfkino spielt: Ja, genau so ist es mir ergangenâ€¦\nVor Ã¤hnlichen Erfahrungen mÃ¶chte ich euch schÃ¼tzen und deswegen greifen wir gleich auf Websockets zurÃ¼ck. Ohne zu tief in die Grundlagen der Informatik eingehen zu wollen, handelt es sich bei Websockets um eine Interprozesskommunikation (IPC) Ã¼ber das HTTP (WEB) Protokoll. Anders formuliert: Mit Websockets bauen wir einen Tunnel zwischen zwei oder mehr GerÃ¤ten zur Echtzeitkommunikation auf. Genau was wir hier brauchen!\nDer zweite Knoten, den wir in den Flow ziehen ist also ein â€œWebsocket outâ€ aus dem Netzwerkbereich. Auf diesen hÃ¶rt wiederum ein â€Websocket inâ€œ in meiner HeimautomatisierungslÃ¶sung, die ebenfalls mit Node-Red realisiert ist. Wenn ihr ein anderes System zur Heimautomatisierung einsetzt, wÃ¤re die Eingabe des â€œrpi â€“ keyboardâ€ Nodes/Knotens an das entsprechende Modul weiterzuleiten. So lÃ¤sst sich eben auch, wie in der Einleitung angekÃ¼ndigt, jedes andere GerÃ¤t in euer IoT einbinden.\nFazit In unserem kleinen Versuch haben wir spielerisch aus einer alten Infrarot Fernbedienung eine sinnvolle Erweiterung unserer Smart Homes geschaffen. Da unser Raspi Zero nur wenig Strom verbraucht, wÃ¤re theoretisch auch eine Energieversorgung Ã¼ber einen Akku oder eine Powerbank denkbar, womit das ganze Setup sogar mobil werden wÃ¼rde.\nDer ein oder andere â€œProfiâ€ wird darÃ¼ber geschimpft haben, warum ich auf dem kleinen Raspberry Pi Zero eine Desktopumgebung installiert habe, anstelle darauf der geringen Ressourcen zuliebe zu verzichten. Das habe ich probiert und das Anlernen der Fernbedienung Ã¼ber FLIRC hat auf diese Art reichlich wenig SpaÃŸ gemacht. AuÃŸerdem sagt uns htop, dass auf dem Raspi auch bei laufendem Desktop nur ca. 100 von 512 MB Arbeitsspeicher belegt sind.\nIch hoffe der Artikel war interessant und hat zu der ein oder anderen neuen Idee fÃ¼r private oder auch berufliche Projekte gefÃ¼hrt!\n","tags":["deutsch"],"section":"blog"},{"date":"1667260800","url":"/blog/boinc/","title":"Rechenkraft fÃ¼r die Wissenschaft mit BOINC","summary":"Wenn man sich zum Einem fÃ¼r wissenschaftliche Projekte begeistern kann und zum Anderen ein wenig Rechenleistung Ã¼brig hat, dann sollte man sich das Open Source Projekt und Werkzeug Boinc einmal genauer anschauen. In diesem Artikel berichte ich darÃ¼ber, was Boinc eigentlich ist und wie man es einfach installieren kann.\nVolunteer-Computing / Ehrenamtliches Rechnen? Volunteer-Computing (zu deutsch: ehrenamtliches / freiwilliges Rechnen) beschreibt eine Technik der Anwendungsprogrammierung bei der einzelne Computernutzer RechnerkapazitÃ¤ten wie Rechenzeit und Speicherplatz auf freiwilliger Basis einem bestimmten Projekt zur VerfÃ¼gung stellen, um unter Anwendung des verteilten Rechnens ein gemeinsames Ergebnis zu berechnen.\n","content":"Wenn man sich zum Einem fÃ¼r wissenschaftliche Projekte begeistern kann und zum Anderen ein wenig Rechenleistung Ã¼brig hat, dann sollte man sich das Open Source Projekt und Werkzeug Boinc einmal genauer anschauen. In diesem Artikel berichte ich darÃ¼ber, was Boinc eigentlich ist und wie man es einfach installieren kann.\nVolunteer-Computing / Ehrenamtliches Rechnen? Volunteer-Computing (zu deutsch: ehrenamtliches / freiwilliges Rechnen) beschreibt eine Technik der Anwendungsprogrammierung bei der einzelne Computernutzer RechnerkapazitÃ¤ten wie Rechenzeit und Speicherplatz auf freiwilliger Basis einem bestimmten Projekt zur VerfÃ¼gung stellen, um unter Anwendung des verteilten Rechnens ein gemeinsames Ergebnis zu berechnen.\nâ€” Wikipedia Das heisst, dass jeder der ein wenig Rechenleistung entbehren kann, sich an wissenschaftlichen Projekten beteiligen kann, um diesen dabei zu helfen bestimmte Problemstellungen zu lÃ¶sen. Um das zu tun, muss man sich bloÃŸ ein bestimmtes Programm installieren, dass auf einem Computer lÃ¤uft der mÃ¶glichst oft/dauerhaft betrieben wird. Wer also z.B. zuhause einen Raspberry Pi oder so wie ich, sowieso einen Server betreibt, kann wissenschaftliche Arbeit leisten in dem er teile seiner freien ProzessorkapazitÃ¤t zur VerfÃ¼gung stellt.\nHinweis Vor einem Betrieb auf einem Laptop oder einem klassischen PC sollte man bedenken, dass durch erhÃ¶hten CPU-Verbrauch die Lebensdauer, wenn vielleicht auch nicht signifikant, heruntergesetzt bzw. der Stromverbrauch gesteigert wird. Von einem Betrieb auf Cloud-Server Angeboten wie Microsoft Azure oder Amazon AWS, ist wegen der auf CPU-Leistung und DatenÃ¼bertragung basierenden Abrechnungsmodelle, ebenfalls abzuraten.\nSonst kann es teuer werden!\nWas ist Boinc? Die Schaffer des Tools Boinc haben es sich zur Aufgabe gemacht ca. 30 wissenschaftliche Volunteer-Computing Projekte Ã¼ber eine OberflÃ¤che verfÃ¼gbar und konfigurierbar zu machen. Auch Einstellungen wie die zur VerfÃ¼gung zu stellenden Ressourcen und Reports Ã¼ber bereits geleistete Arbeit stellt das Tool zur VerfÃ¼gung. Eine genaue Liste mit Beschreibung der Projekte findet sich hier: https://boinc.berkeley.edu/projects.php . Dort wird auch schon ersichtlich, dass nicht jedes Projekt fÃ¼r jede Computerplattform existiert und andere nicht fÃ¼r jede Art von Plattform geeignet sind. So benÃ¶tigen einige von Ihnen z.B. eine schnelle dezidierte Grafikkarte, da die angestellten Berechnung auf diesen Prozessortyp (GPU) optimiert sind.\nOK, gefÃ¤llt mir. Wie starte ich? Die Installation kann entweder manuell und direkt Ã¼ber das auf der Boinc Homepage verfÃ¼gbare Installationsprogramm erfolgen oder, wie von mir bevorzugt und folgend beschrieben, als Docker Container mit Docker Compose.\nDie manuelle Installation funktioniert auf jeder Art von Rechner und kann zum Beispiel so konfiguriert werden, das BOINC als Bildschirmschoner lÃ¤uft und so nur dann Ressourcen des PCs beansprucht, wenn dieser gerade nicht genutzt wird. Die Installation kann man unter https://boinc.berkeley.edu/download.php herunterladen.\nFÃ¼r den Betrieb auf einem Server in einem Container hier eine beispielhafte compose.yml Datei:\n1version: \u0026#34;2.1\u0026#34; 2services: 3 boinc: 4 image: lscr.io/linuxserver/boinc 5 hostname: \u0026#34;boinc\u0026#34; 6 container_name: boinc 7 environment: 8 - PUID=1000 9 - PGID=1000 10 - TZ=Europe/Berlin 11 - PASSWORD=xxx 12 volumes: 13 - ./config:/config 14 restart: unless-stopped 15 #ports: 16 # - 8080:8080 17 logging: 18 options: 19 max-size: \u0026#34;10m\u0026#34; 20 max-file: \u0026#34;3\u0026#34; 21 networks: 22 - dmz 23 deploy: 24 resources: 25 limits: 26 cpus: 1.00 27 memory: 2048M 28networks: 29 dmz: 30 external: true Die Angabe der zur VerfÃ¼gung gestellten Ressourcen Ã¼ber deploy â€“ resources â€“ limits, ist optional. Eine BeschrÃ¤nkung der dem Program bereitgestellten Ressourcen lÃ¤sst sich, wie schon zuvor erwÃ¤hnt, auch direkt im Programm konfigurieren. Ich gehe hier auf Nummer sicher und schrÃ¤nke den Zugriff von vorn herein ein. Ein CPU Wert von 1 entspricht einem CPU-Thread. Bei z.B. vorhandenen 8 Threads, kann der hier definierte Container also auf 12,5% der verfÃ¼gbaren CPU-Leistung zugreifen. Genau das von mir gewÃ¼nschte â€œGrundrauschenâ€. Die Angabe des Ports ist nur erforderlich, wenn wir keinen Reverse Proxy nutzen. Dann wÃ¤re unsere BOINC Instanz auf dem Host Ã¼ber den entsprechenden Port erreichbar. Das Passwort sollte auf jeden Fall gesetzt werden und ist dann zur Anmeldung am virtuellen Desktop Ã¼ber den Browser erforderlich. Der Benutzer ist Ã¼brigens fest auf abc eingestellt.\nAnwendung und Fazit Entweder durch starten der Anwendung oder durch Zugriff auf das Webinterface im Conatinerbetrieb, kÃ¶nnen wir uns nun im Program fÃ¼r die Projekte die uns intressieren anmelden und die Prozesse starten. Danach passiert eigentlich nichts spannendes mehr und eine weiteres Eingreifen unserseits ist nicht erfordlich. Unseren Beitrag zum jeweiligen Projekt kÃ¶nnen wir Ã¼ber die im Program integrierten Berichte oder auf der jeweiligen Projektseite jederzeit einsehen. Dort oder unter kÃ¶nnen wir uns auch ein Zertifkat gennerieren lassen, wenn wir das mÃ¶chten.\nEinfacher geht es nicht, viel SpaÃŸ beim rechnen (lassen)!\n","tags":["deutsch"],"section":"blog"},{"date":"1664668800","url":"/blog/headless-cms-2022/","title":"Headless CMS, APIs und IoT fÃ¼r die Website","summary":"Wie im Artikel â€œDie (beinahe) perfekte Website â€“ Teil 1: Anforderungenâ€ bereits angerissen, kommen bei Gestaltung und Umsetzung eines modernen Internetauftritts eine Vielzahl von Faktoren zusammen. FÃ¼r alle denen eine einfache WordPress Installation nicht reicht, oder fÃ¼r die die mÃ¶glichen Alternativen kennen lernen mÃ¶chten, stelle ich in diesem Post meine Umsetzung mit Hilfe der Tools Cockpit, Node-RED und weiteren Helferlein vor.\nWas ist das Ziel? Ich wollte erreichen dass meine Seite(n) beim Besucher weit unterhalb von 2 Sekunden geladen werden und dort hÃ¼bsch und zeitgemÃ¤ÃŸ dargestellt werden. AuÃŸerdem wollte ich dazu moderne Techniken und Tools verwenden. Soviel vorab: das Meiste davon ist mir gelungen :-). Bei der Auswahl der Tools habe ich grÃ¶ÃŸtenteils mir vertraute LÃ¶sungen genommen, mit dem headless CMS Cockpit aber auch etwas Neues ausprobiert. Aber der Reihe nach.\n","content":"Wie im Artikel â€œDie (beinahe) perfekte Website â€“ Teil 1: Anforderungenâ€ bereits angerissen, kommen bei Gestaltung und Umsetzung eines modernen Internetauftritts eine Vielzahl von Faktoren zusammen. FÃ¼r alle denen eine einfache WordPress Installation nicht reicht, oder fÃ¼r die die mÃ¶glichen Alternativen kennen lernen mÃ¶chten, stelle ich in diesem Post meine Umsetzung mit Hilfe der Tools Cockpit, Node-RED und weiteren Helferlein vor.\nWas ist das Ziel? Ich wollte erreichen dass meine Seite(n) beim Besucher weit unterhalb von 2 Sekunden geladen werden und dort hÃ¼bsch und zeitgemÃ¤ÃŸ dargestellt werden. AuÃŸerdem wollte ich dazu moderne Techniken und Tools verwenden. Soviel vorab: das Meiste davon ist mir gelungen :-). Bei der Auswahl der Tools habe ich grÃ¶ÃŸtenteils mir vertraute LÃ¶sungen genommen, mit dem headless CMS Cockpit aber auch etwas Neues ausprobiert. Aber der Reihe nach.\nEs ist beinahe lustig dass das 2022 (wieder) der Fall ist, aber Microsoft Visual Studio Code(VS Code) ist tatsÃ¤chlich beinahe das Wichtigste dieser Werkzeuge. Back to the Roots und ein klares Nein an die Generatoren von chaotischem und langsamem Quellcode wie WordPress, Jimdo usw. es nun mal sind. AuÃŸerdem ist VS Code weit mehr als nur ein erweiterter Text-Editor, sondern auch Open Source Tool zur Administration ganzer Containerlandschaften und natÃ¼rlich vorzÃ¼glich zum Debugging. Vor einer Weile hatte ich dazu auch folgendes Anwendungsbeispiel geschrieben: Quellcode auf GitHub verwalten mit Visual Studio Code .\nSeit VerÃ¶ffentlichung von Version 5 des Open Source Frontend Toolkits Bootstrap von Twitter, ist dieses schneller den je zuvor. Man hat sich endlich des unnÃ¶tigen Overheads des jQuery Frameworks entledigt und setzt nun vollstÃ¤ndig auf natives â€œVanillaâ€ Javascript. Das gleiche habe ich bei den wenigen selbst geschriebenen Funktionen im Frontend natÃ¼rlich auch gemacht. Bootstrap selbst stellt CSS Klassen, Icons, Komponenten und vieles mehr zur VerfÃ¼gung, die bei der Entwicklung von UI/UX unwahrscheinlich viel Zeit und Aufwand sparen.\nMit den Open Source Web Content Management System (WCMS) Cockpit von Agentejo, gelingt die saubere Trennung von Back- und Frontend. Neben den fÃ¼r ein CMS Ã¼blichen FunktionalitÃ¤ten zur Pflege- und Verwaltung von Web-Content und einer Medienverwaltung macht die integrierte API den Unterschied. Dazu spÃ¤ter etwas mehr.\nDas ursprÃ¼nglich von IBM initiierte Open Source Projekt Node-RED ist vielleicht die zur Zeit beste Implementierung einer Internet of Things (IoT) Low-Code Platform. Sowohl die EinschrÃ¤nkung â€œLow-Codeâ€ als auch auf IoT, werden dem Tool aber nicht ganz gerecht. Zwar ist es mal fÃ¼r diese Szenarien entwickelt worden, lÃ¤sst sich aber ohne MÃ¼hen auch fÃ¼r ganz viel Code und zur Abbildung aller mÃ¶glichen vorstellbaren Prozesse nutzen. Ein Beispiel folgt.\nEinige weitere benÃ¶tigte Werkzeuge und Helfer wir den Nginx Reverse Proxy Manager, Letâ€™s Encrypt SSL Zetifikate, Matomo Analytics, Docker(-Compose) und weitere wÃ¼rden hier vom Fokus ablenken, weshalb ich sie erstmal auÃŸen vor lasse.\nWarum? Die Frage zum warum ist einfach zu beantworten; vor allem beruflich setze ich mich tÃ¤glich mit den HÃ¼rden und Schwierigkeiten beim Umgang mit monolithischen Systemen auseinander. Andererseits propagieren Unternehmen wie die SAP neue Headless AnsÃ¤tze. Das und meine innere Neugier haben mich angetrieben, einfach fÃ¼r mich selbst solch ein Setup vorzunehmen um evtl. neue Risiken, aber vor allem MÃ¶glichkeiten kennen zu lernen. Oder in kurz: Weil ich es kÃ¶nnen will und es gar nicht so schwierig ist.\nWie genau? Die Installation von Cockpit CMS und Node-RED in eigenstÃ¤ndigen Containern geht schnell von der Hand und sollte in maximal 20 Minuten erledigt sein. Im Appendix, als Hilfestellung, meine Docker-Compose Dateien.\nJetzt geht es ans â€œEingemachteâ€. Nach dem Entwurf des Layouts und der Umsetzung des Prototypen mit Bootstrap, lagere ich sÃ¤mtlichen statischen HTML Content nach Cockpit CMS in Form von dort sogenannten â€œSingletonsâ€ aus. Die haben den Vorteil, dass sie mit Attributen wie Sprache und Version, etc. versehen werden kÃ¶nnen. Theoretisch kÃ¶nnte man das auch mit den CSS und Javascript Dateien machen, aber die bearbeite ich dann doch irgendwie lieber in Visual Studio Code. Als nÃ¤chstes erstellen wir nun eine sogenannte â€œCollectionâ€ in Cockpit, die sozusagen den Rahmen um unsere Posts bildet. So lieÃŸe sich z.B. mit einer Cockpit Installation der Content fÃ¼r viele Websites verwalten. Klasse! Bilder, Grafiken und der Gleichen lassen sich auch mit Cockpit komfortabel verwalten und vor allem von dort aus auch automatisch als Thumbnails umwandeln. Noch besser! Der Zugriff auf das Ganze erfolgt dann Ã¼ber Web APIs, auf die ich gleich weiter eingehe. Und noch etwas; Ã¼ber sogenannte Webhooks lassen sich auch genau andersherum von Cockpit aus Aktionen starten. So starte ich z.B. bei jedem speichern/Ã¤ndern der Collection automatisch ein Deployment der kompletten Website. Wow! Eigentlich war Cockpit einfach nur mein erster Google Treffer bei der Suche nach einem Headless CMS, aber mir fÃ¤llt gerade kein Grund mehr ein warum ich wechseln bzw. mir andere ansehen sollte.\nUnd nun kommt Node-RED ins spiel, denn ich muss meine Inhalte ja nun irgendwie unter unter niklas-stephan.de verfÃ¼gbar machen. Eigentlich geht es mir darum, mit dem Ziel maximaler Performanz, statische Seiten zu generieren. Bei einer Google Suche nach â€œStatic Site Generatorsâ€ ist Node-RED garantiert nicht auf der ersten Seite der Suchergebnisse zu finden. DafÃ¼r gibt es unzÃ¤hlige andere Tools, die aber auch wieder eine Einarbeitung und meistens auch das Erlernen einer eigenen Syntax erfordern. Durch meine umtriebigen AktivitÃ¤ten im Umfeld der Heimautomatisierung, ist mir aber nun Node-RED bestens vertraut und eigentlich will ich doch nur einen relativ einfachen Prozess abbilden. Und der sieht wie folgt aus:\nStart des Deplyoments manuell oder durch Webhook Sammeln aller statischen Quellen Sammeln aller vorhanden Singletons und Posts Dynamische Generierung von Meta-Daten Rendern der einzelnen Seiten Speichern der Seiten auf Platte und im Arbeitsspeicher Wer sich die MÃ¼he machen mÃ¶chte mal kurz im Console Log dieser Seite zu gucken sieht, dass ich bei jedem Deployment Zeitpunkt und Dauer protokollieren lasse. Da sehen wir dann auch dass der beschriebene Prozess insgesamt 0,099 Sekunden dauert. Und das auf einem Server der nebenbei u.a. noch Monero schÃ¼rft und meine Cloud Umgebung bereit stellt. Also eigentlich schon recht fix, aber um noch â€œeins Draufâ€ zu setzen lege ich die erzeugten Dateien im letzten Schritt im â€œ/dev/shmâ€, sprich dem Arbeitsspeicher des Servers ab, von wo aus sie der Reverse Proxy beim Zugriff zur VerfÃ¼gung stellt. Da brauchtâ€™s vorerst kein CDN mehr und mein Ziel ist erreicht. ğŸ™‚\nAppendix compose.yml fÃ¼r Cockpit CMS\n1services: 2 cms-db: 3 image: \u0026#39;mongo:latest\u0026#39; 4 restart: always 5 volumes: 6 - \u0026#39;./db:/data/db\u0026#39; 7 networks: 8 - default 9 logging: 10 options: 11 max-size: \u0026#34;10m\u0026#34; 12 max-file: \u0026#34;3\u0026#34; 13 cms: 14 image: agentejo/cockpit 15 restart: always 16 volumes: 17 - \u0026#39;./cms/config.php:/var/www/html/config/config.php\u0026#39; 18 - \u0026#39;./cms/defines.php:/var/www/html/config/defines.php\u0026#39; 19 - \u0026#39;./storage:/var/www/html/storage\u0026#39; 20 environment: 21 COCKPIT_SESSION_NAME: SESSION 22 COCKPIT_SALT: SALT 23 COCKPIT_DATABASE_SERVER: \u0026#39;mongodb://cms-db:27017\u0026#39; 24 COCKPIT_DATABASE_NAME: DBNAME 25 depends_on: 26 - cms-db 27 ports: 28 - \u0026#34;8080:80\u0026#34; 29 networks: 30 - default 31 logging: 32 options: 33 max-size: \u0026#34;10m\u0026#34; 34 max-file: \u0026#34;3\u0026#34; 35volumes: 36 mongo-vol: null compose.yml fÃ¼r Node-RED\n1services: 2 node-red: 3 image: nodered/node-red:latest 4 restart: always 5 environment: 6 - TZ=Europe/Berlin 7 ports: 8 - \u0026#34;1880:1880\u0026#34; 9 networks: 10 - default 11 logging: 12 options: 13 max-size: \u0026#34;10m\u0026#34; 14 max-file: \u0026#34;3\u0026#34; 15 volumes: 16 - ./data:/data 17 - ./templates:/templates 18 - ./static:/static 19 - /etc/localtime:/etc/localtime:ro 20 - /dev/shm/nsde:/ramdisk 21 - ./development:/development ","tags":["deutsch"],"section":"blog"},{"date":"1656806400","url":"/blog/perfect-website-2022/","title":"Die (beinahe) perfekte Website - Anforderungen","summary":"Willkommen zum ersten, wirklich langen Beitrag meiner Website, in dem ich darauf eingehe, was alles erforderlich ist um einen Internetauftritt besonders gut zu gestalten. Um zu beweisen, dass die gestellten Anforderungen auch erfÃ¼llt werden kÃ¶nnen, nutzen wir meine Domain niklas-stephan.de um das Gelernte, soweit mÃ¶glich, auch gleich direkt umzusetzen.\nWas ist perfekt? Was ist schon perfekt und wer definiert das? Im Falle eines Internetauftritts ist diese Frage vielleicht einfacher zu beantworten als in anderen FÃ¤llen. Denn, egal wie gut Gestaltung und Umsetzung gelungen sind, muss die Seite erst einmal von einem Besucher gefunden werden. Mit einem Markanteil von Ã¼ber 90% (https://blog.hubspot.de/marketing/google-trends-suche) ist da die Google Suchmaschine eigentlich der einzige ausschlaggebende Faktor. Als Schlussfolgerung kann mann also sagen, dass die Optimierung einer Website auf von Google definierte Faktoren gleichzeitig nah an eine perfekte Website fÃ¼hrt. Welche Faktoren das sind und welche Weiteren noch eine wichtige Rolle spielen, sehen wir uns nun im ersten Teil unserer Artikelserie an.\n","content":"Willkommen zum ersten, wirklich langen Beitrag meiner Website, in dem ich darauf eingehe, was alles erforderlich ist um einen Internetauftritt besonders gut zu gestalten. Um zu beweisen, dass die gestellten Anforderungen auch erfÃ¼llt werden kÃ¶nnen, nutzen wir meine Domain niklas-stephan.de um das Gelernte, soweit mÃ¶glich, auch gleich direkt umzusetzen.\nWas ist perfekt? Was ist schon perfekt und wer definiert das? Im Falle eines Internetauftritts ist diese Frage vielleicht einfacher zu beantworten als in anderen FÃ¤llen. Denn, egal wie gut Gestaltung und Umsetzung gelungen sind, muss die Seite erst einmal von einem Besucher gefunden werden. Mit einem Markanteil von Ã¼ber 90% (https://blog.hubspot.de/marketing/google-trends-suche) ist da die Google Suchmaschine eigentlich der einzige ausschlaggebende Faktor. Als Schlussfolgerung kann mann also sagen, dass die Optimierung einer Website auf von Google definierte Faktoren gleichzeitig nah an eine perfekte Website fÃ¼hrt. Welche Faktoren das sind und welche Weiteren noch eine wichtige Rolle spielen, sehen wir uns nun im ersten Teil unserer Artikelserie an.\nAus Sicht des Besuchers Neben viel Literatur und Informationen gibt Google mit dem Werkzeug â€œLighthouseâ€, welches in jede Installation des Browsers Chrome integriert ist, einen Einblick auf die Kriterien die sie an eine Website Stellen. Diese sind:\nPerformance (Geschwindigkeit) Accessibility (Erreichbarkeit/ZugiffsfÃ¤higkeit) Best Practices (Empfohlene MaÃŸnahmen) Search Engine Optimization (SEO â€“ Suchmaschinenoptimierung) Es taucht noch ein weiter Punkt â€œProgressive Web Appâ€ (PWA) in Lighthouse auf. Um aus HTML5 eine echte WebApp zu machen mÃ¼ssen nÃ¤mlich eine Menge von Anforderungen erfÃ¼llt werden. Dazu spÃ¤ter mehr in einem anderen Beitrag zu meiner App zur Hausautomatisierung. FÃ¼r eine Website im eigentlichen Sinn ist der Punkt PWA jedenfalls irrelevant. Der Screenshot hier zeigt eine Lighthouse Bewertung einer im Internet tausendfach frequentierten Seite, die aber offensichtlich nicht die gestellten Kriterien zu Googles voller Zufriedenheit erfÃ¼llt. Dies ist durchaus Ã¼blich bzw. nicht ungewÃ¶hnlich, denn noch viele weitere Faktoren entscheiden Ã¼ber den Erfolg eines Internetauftritts.\nWen es interessiert wie diese Bewertung zustande kommt, der kann wie gesagt Ã¼ber die Entwicklertools von Google Chrome (Ã–ffnen sich mit Taste F12) im Tab â€œLighthouseâ€ eine beliebige Seite bewerten lassen und sich das Ergebnis ansehen. Praktischerweise gibt Lighthouse auch gleich noch Tipps dazu aus, wie sich die Punktzahl weiter erhÃ¶hen bzw. die Seite optimieren lÃ¤sst.\nDeshalb orientieren wir uns in den nÃ¤chsten Kapiteln genau an diesen Faktoren.\nPerformance Seit dem die Ãœbertragungsgeschwindigkeit der EndgerÃ¤te des Webseitenbesuchers nicht mehr der limitierende Faktor sind, mÃ¼ssen sich Webseiten mindestens genauso â€œschnellâ€ anfÃ¼hlen wie eine Applikation die auf unserem EndgerÃ¤t fest installiert ist. Google gibt in seinem Playbook so den dringlichen Rat dazu, dass eine Seite innerhalb von 2 Sekunden geladen sein muss um die Benutzererfahrung nicht zu schmÃ¤lern. Um unter diesen 2 Sekunden zu bleiben kann man sich vieler Stellschrauben bedienen:\nStarke Infrastruktur / Hardware mit einer guten Netzwerkanbindung Ein mÃ¶glichst geringes Datenvolumen einer Seite Kompression von Dateien und wÃ¤hrend der DatenÃ¼bertraung Reverse-Proxy Konfiguration und Caching Content Delivery Networks (CDN) fÃ¼r lokales Caching Kein/wenig Server-Side Rendering â€œSaubererâ€ und sparsamer Quellcode Auf das (nach)laden, besonders von externen Inhalten, sollte mÃ¶glichst verzichtet werden Jeder der aufgelisteten Punkte ist mindestens ein eigenes Kapitel Wert und wir werden in unsere Kapitelserie dementsprechend darauf eingehen.\nAccessibility Egal wie schnell sich die von uns aufgesuchte Seite auch aufgebaut hat, wenn der Autor meinte es sei eine gute Idee alles in einer SchriftgrÃ¶ÃŸe von 5 Pixeln in hellgrauer Schrift unter Verwendung eines Comic Sans Fonts auf weiÃŸem Hintergrund anzuzeigen, ist der SpaÃŸ vorbei. Auch Laufschriften mit Telefonnummern zum Abschreiben und groÃŸzÃ¼gige Nutzung von Neon-Farben sind Effekte die in einem hÃ¶chstens die Gewaltbereitschaft steigern. Vielleicht nicht ganz so Ã¼bertrieben aber Ã¤hnlich hat sicherlich jeder von uns schon einmal Erfahrungen im Internet gemacht. Um solchen Situationen entgegenzuwirken achtet man als Webseitenbetreiber, neben der generellen User Experience (UX â€“ Berufserfahrung), auch auf die Accessibility, sprich die Erreichbarkeit / ZugriffsfÃ¤higkeit seiner Inhalte. Dabei gilt es zu bedenken:\nNicht jeder Mensch kann gut sehen Manche Menschen kÃ¶nnen Ã¼berhaupt nicht sehen und lassen sich die Inhalte von einer Maschine vorlesen Menschen nutzen unterschiedliche EndgerÃ¤te mit unterschiedlichen AuflÃ¶sungen Auf die Verwendung evtl. verwirrender Effekte sollte man mÃ¶glichst verzichten, wenn sie nicht ein Kernelement der Seite darstellen. Die Seitenstruktur sollte klar und hierarchisch sein, z.B. nicht am Anfang der Seite mit einer h4 Ãœberschrift anfangen und dann mitten im Text h2 verwenden. Es sollten mÃ¶glichst keine oder zumindest nur wenige MaÃŸnahmen getroffen werden, die den Nutzer daran hindern die Darstellung seinen BedÃ¼rfnissen nach anzupassen. Wenn mann sich daran hÃ¤lt, wird man auf modernen EndgerÃ¤ten/Browsern automatisch durch die Bereitstellung seiner Seite mit der â€œReaderâ€ Option belohnt, die ein eBook-artiges lesen der Artikel ermÃ¶glicht.\nBest Practices Es gibt eine Reihe von empfohlenen MaÃŸnahmen um die korrekte Darstellung der Seite beim Besucher zu GewÃ¤hrleisten und um bestimmte Sicherheitsstandards einzuhalten. AuÃŸerdem prÃ¼ft Lighthouse hier noch, dass die Seite keine offensichtlichen Fehler in der Programmierung enthÃ¤lt und gÃ¤ngige sowie akutelle Standardtechniken umgesetzt wurden. Insgesamt werden bis zu 17 Kriterien geprÃ¼ft, von denen ich hier die wichtigsten Aufliste.\nVerwendung von https zur VerschlÃ¼sselung der DatenÃ¼bertragung zwischen Nutzer und Betreiber. Schon vor 2021 eigentlich ein absolutes Muss.\nVerzicht auf nicht unbedingt erforderliche JavaScript Features Keine JavaScript Fehler auf der Seite. Korrekte Formatierung des HTML Codes bezÃ¼glich wesentlicher Merkmale wie z.B. der eingesetzten Sprache. Das klingt eigentlich nach nicht viel und leicht umzusetzen. Wer sich aber einmal die MÃ¼he macht sich die Fehlerprotokolle durchzulesen die von einer groÃŸen Anzahl gÃ¤ngiger Websites â€œausgespucktâ€ werden, staunt nicht schlecht. Offenbar ist es selbst groÃŸen Unternehmen nur schwer mÃ¶glich, ihre Entwickler dahingehend zu motivieren, dass solche Fehler ausgeschlossen sind. Zur Verteidigung der Entwickler aber vor allem deren Motivation, muss man allerdings sagen dass verschiedene Browser gerade mit JavaScript verschieden umgehen und manchmal Kompromisse geschlossen werden mÃ¼ssen.\nSEO Wie schon zu Beginn des Artikels erlÃ¤utert, bringt uns die schÃ¶nste Website nichts, wenn sie niemand findet. Falls wir dann auch noch nicht gewillt sind tief in unsere Taschen zu greifen, um Werbekampagnen zu finanzieren, dann aber eigentlich auch immer mÃ¼ssen wir uns mit Search Engine Optimization (SEO) beschÃ¤ftigen. FrÃ¼her bestand die Optimierung fÃ¼r Suchmaschinen zum groÃŸen Teil darin, brav ein paar SchlÃ¼sselwÃ¶rter (Keywords) in die Meta Informationen unseres Auftritts einzubauen. Das reicht heute bei weitem nicht mehr aus und die vergebenen Keywords werden sogar in der Regel von den gÃ¤ngigen Suchmaschinen gar nicht mehr beachtet. DafÃ¼r sind nun eine Vielzahl anderer Faktoren entscheidend dafÃ¼r ob wir (gut) gefunden werden oder nicht. Einige der wichtigsten davon sind hier aufgelistet:\nDie Seite enthÃ¤lt Metainformationen, wie: Beschreibung, Titel, Bild, Link, usw. Die Seite hat einen lÃ¤ngeren wirklichen Inhalt in Textform MÃ¶glichst alle Links auf der Seite haben eine Beschreibung (Text) Alle Bilder auf der Seite haben eine Bezeichnung (alt) die Seite ist z.B. Ã¼ber eine robots.txt Datei so eingestellt, dass sie von Suchmaschinen indiziert werden darf die robots.txt enthÃ¤lt, wenn vorhanden, keine Fehler Es werden keine â€œMiniâ€ SchriftgrÃ¶ÃŸen verwendet um Inhalte vorzutÃ¤uschen (eine Zwischenzeitlich verbreitete Masche unseriÃ¶ser Anbieter) Die Seite / Der Beitrag enthÃ¤lt interne und externe Links zu anderen Seiten Wie man sieht zielen diese Kriterien darauf ab, dass neben den Metainformationen die auch fÃ¼r Soziale Netzwerke relevant sind, der komplette Seiteninhalt bei der Indizierung der Seite durch Suchmaschinen berÃ¼cksichtigt wird. Das ist auch ein weiteres Argument, neben der Performance, Inhalte nicht dynamisch nachzuladen, weil eben diese dann von Google\u0026amp;Co nicht gefunden werden.\nSocial Media Aus Sicht eines Unternehmens heute beinahe Ã¼berlebensnotwendig, aus Sicht einer Privatperson einfach eine hilfreiche UnterstÃ¼tzung um Inhalte bekannt zu machen und mit der Welt zu teilen, das ist Social Media Integration fÃ¼r eine Website. Die schon zuvor genannten Meta-Informationen ermÃ¶glichen es, dass beim Teilen eines Links automatisch ein Bild, eine Ãœberschrift und eine Kurzbeschreibung generiert werden. AuÃŸerdem ermutigen SchaltflÃ¤chen mit den Symbolen der verschiedenen Netzwerke in der NÃ¤he eines Beitrags zum Teilen. Man kann auch berÃ¼cksichtigen, dass je nach EndgerÃ¤t unterschiedliche KanÃ¤le genutzt werden. So macht ein Share Button zu WhatsApp in der Regel nur fÃ¼r mobile EndgerÃ¤te Sinn. Auch werden von unterschiedlichen Altersgruppen unterschiedliche Netzwerke bevorzugt. Dabei kann man sich hieran orientieren:\nFacebook: Wird primÃ¤r von Menschen mittleren bis hÃ¶heren Alters genutzt, ist aber wohl jedem als Pionier bekannt. Instagram: Junge Erwachsene bis Menschen mittleren Alters nutzen dieses Netzwerk hauptsÃ¤chlich zum Teilen von Bildern und Geschichten dazu.\nTikTok: FÃ¼r die ganz jungen Mitmenschen. Da wird wohl hauptsÃ¤chlich getanzt, geklatscht und dazu gesungen, um das Ganze dann auch noch als Video zu teilen.\nTwitter(X): zum Austausch von Kurznachrichten und so richtig im sogenannten Arabischen FrÃ¼hling weltweit bekannt geworden. Bis vor kurzem auch Stammsitz einer orangen Ente. Zielpublikum sind inzwischen eher Menschen mittleren bis hÃ¶heren Alters.\nYouTube: Mit etwas Aufwand auch als Soziales Netzwerk nutzbar, wird dafÃ¼r aber altersgruppenÃ¼bergreifend genutzt\nWhatsApp: eher zum persÃ¶nlichen Austausch in kleineren Zielgruppen, dafÃ¼r aber ebenfalls altersgruppenÃ¼bergreifend.\nNatÃ¼rlich gibt es auch regionale Unterschiede bei der Nutzung der Netzwerke, so sieht in China oder Indien das Bild schon wieder ganz anders aus. Z.B. tritt dort der Anbieter WeChat mit seiner App als eierlegende Wollmilchsau auf.\nBrowser KompatibilitÃ¤t Hier muss man eine Entscheidung treffen. Leider nutzen nach wie vor viele Menschen den vÃ¶llig veralteten Internetbrowser â€œInternet Explorerâ€ (IE11) von Microsoft. Dieser wird schon seit lÃ¤ngerem nicht mehr von Microsoft weiterentwickelt und wurde durch â€œEdgeâ€ ersetzt. Da nun aber viele trotzdem noch den IE11 nutzen, ist die zu treffende Entscheidung: Will ich diese Besucher ebenfalls mit einer fehlerfreien Darstellung bedienen oder schlieÃŸe ich sie aus, damit ich mich bei der Erstellung der Seite an aktuelle Standards halten kann und diverse Altlasten nicht berÃ¼cksichtigen muss. Ich habe mich gegen die UnterstÃ¼tzung des IE11 entschieden, da selbst Microsoft nach Supportende von Windows 7, an welches der IE11 gekoppelt ist, diesen als Sicherheitsrisiko einzustufen plant. Die anderen Ã¼blichen Browser wie Google Chrome, Chromium und Microsoft Edge oder auch Apple Safari liefern stets gute Ergebnisse. Das Open Source Projekt Firefox der Mozilla Gruppe ist die Wahl des Nutzers mit Wunsch nach UnabhÃ¤ngigkeit. Auch Firefox stellt die meisten Inhalte des aktuellen HTML Standards korrekt dar und unterstÃ¼tzt nur z.B. einige exotischere Parameter in CSS nicht.\nUser Experience und Layout FÃ¼r ein Unternehmen sicherlich nicht nur ein Unterpunkt fÃ¼r mich als Privatperson mehr eine Hilfe. Unter User Experience (UX) wird die komplette Berufserfahrung die der Besucher wÃ¤hrend des Besuchs unserer Website macht zusammengefasst. Dazu gehÃ¶rt das Layout oder auch User Interface (UI) unserer Seite als Kernelement. Sich wiederholende Elemente sollten auf allen (Unter)Seiten gleich aussehen und generell sollte wÃ¤hrend des kompletten Besuchs eine Wiedererkennungswert geschaffen werden. So sind Buttons immer an der gleichen Stelle zu finden, verhalten sich identisch und sehen gleich aus. Auch das Farbschema sowie die komplette Formatierung der Inhalte sollten durchgÃ¤ngig und ansprechend sein. Ich vertraue hierzu als Einstieg schon seit einigen Jahren auf das Open Source Projekt â€œBootstrapâ€ (Link) von Twitter. Dieses beinhaltet im wesentlichen vorgefertigte CCS (Styles) die nach belieben verwendet werden und ggfs. auch angepasst werden kÃ¶nnen. Bootstrap hatte zwischenzeitlich etwas an seinem guten Ruf eingebÃ¼ÃŸt, weil es lange auf das JavaScript Framework â€œjQueryâ€ gesetzt hat. Mit der aktuellen Version 5 ist das Geschichte und wir freuen uns Ã¼ber einen kleineren Quellcode bei der Einbindung.\nAus Sicht des Betreibers In den vorherigen Kapiteln haben wir herausgefunden, was aus Sicht von Google bei der Ausspielung einer Website relevant ist. NatÃ¼rlich haben wir aber neben den Anforderungen aus Besuchersicht auch noch sowohl als Autor als auch als Administrator weitere Anforderungen. Einige davon werde ich hier anfÃ¼hren, andere spÃ¤ter in der Artikelserie auch noch tiefgreifender ausfÃ¼hren.\nServer Wie zuvor im Punkt Performance schon beschrieben ist ein schneller Server mit einer sehr guten Netzwerkanbindung ans Internet eines der Kernkriterien fÃ¼r eine performante Website. Anbieter fÃ¼r virtuelle oder physische Server gibt es viele. Einige der Bekanntesten sind Hetzner, 1blu, Netcup oder Strato. Man hat also die Qual der Wahl. Heute immer mehr Ã¼blich ist anstelle des Betriebs eines eigenen Servers, die Nutzung von Ressourcen eines Cloud Anbieters. Hier Stellen Google, Amazon und Microsoft die Platzhirsche dar. Aus Sicht des Datenschutzes stellen alle drei Anbieter mittlerweile auch Ressourcen aus Deutschland bereit und halten sich dementsprechend an die hier gÃ¼ltigen Regulierungen. Ich persÃ¶nlich bin aber aufgrund der variablen Abrechnung kein Fan der Cloud Anbieter und bezahle lieber einen monatlichen fixen Betrag der sÃ¤mtlichen Traffic sowie die Hardwarekosten beinhaltet.\nMonitoring \u0026amp; Alerting Das Monitoring, also die Ãœberwachung des Serves ist nicht nur in ProblemfÃ¤llen wichtig, sondern hilft auch dabei Problemen bereits entgegen zur wirken bevor sie auftreten. Ich nutze zum Monitoring das mit dem CentOS Betriebsystem ausgelieferte Open Source Tool Cockpit, welches eine schicke WeboberflÃ¤che fÃ¼r die meisten anliegen liefert. So lassen sich dort automatische Updates und eine Langzeit-RessourcenÃ¼berwachung inkl. Dashboard mit ein paar Mausklicks aktivieren.\nViele setzen ein Alerting, also die Alarmierung im Fehlerfall, mit einem Monitoring gleich. Das ist so nicht ganz korrekt. Ein Alerting kann durchaus vÃ¶llig unabhÃ¤ngig vom Monitoring konfiguriert und genutzt werden. So nutze ich z.B. nur Alerts die mir eine E-Mail senden wenn meine Website keinen gÃ¼ltigen Inhalt zurÃ¼ck liefert. Wem das reicht, der braucht keine schwergewichtige SoftwarelÃ¶sung zu installieren. Wichtig ist nur: Die Ãœberwachung sollte auf keinen Fall auf dem gleichen Serversystem wie unsere Website laufen. Der Grund ist klar: Wenn der Server ausfÃ¤llt kann er mir auch keine E-Mails mit der Info Ã¼ber den Ausfall liefernâ€¦ Deshalb kommen meine Mails von einem kleinen Raspberry PI der zuhause auch schon vorher seinen 24-Stunden Dienst verrichtet hat. Und im Normalfall kommen eigentlich auch gar keine Mails, weil der Server einfach nicht ausfÃ¤llt ğŸ™‚\nDatensicherung Das eine Datensicherung zwingend erforderlich ist, braucht man heute glÃ¼cklicherweise niemanden mehr zu erklÃ¤ren. Ich nutze fÃ¼r die Website genauso wie vor andere Komponenten auf meinem Server eine mehrschichtige Backupstrategie: Eine tÃ¤gliche Spiegelung der Inhalte innerhalb der Servers lÃ¤sst mich versehentlich gelÃ¶schte Inhalte sehr schnell wiederherstellen. Diese Spiegelung ist dann auch die Quelle um die Daten inkrementell abgesichert an den zuvor erwÃ¤hnten Raspberry Pi zuhause zu Ã¼bertragen. Genau genommen an ein dahinter liegendes Hardware RAID Plattensystem. So habe ich zumindest alles mir mÃ¶gliche getan um sowohl das versehentliche LÃ¶schen, einen Datenverlust oder einem DatentrÃ¤gerausfall entgegen zu wirken.\nDomain und DNS Die Adresse(n) unter denen wir im Internet erreichbar sein wollen werden als Domains bezeichnet. Diese sichert man sich fÃ¼r geringes Geld bei einer Vielzahl von Anbietern, wenn man einen kreativen und nicht bereits belegten Namen gefunden hat. Und genau da liegt die Schwierigkeit, die meisten â€œhÃ¼bschenâ€ Namen sind bereits vergeben und man muss wirklich kreativ werden. Eine Nacht darÃ¼ber zu schlafen bevor man die Registrierung anstÃ¶ÃŸt kann wirklich helfen.\nDer DNS Eintrag ist das Bindeglied im Internet zwischen unseren registrierten Domains und der IP-Adresse unseres Servers. Oft Ã¼bernehmen Anbieter von Domains auch gleichzeitig die Registrierung und Verteilung des DNS Eintrags fÃ¼r genauso geringes Geld.\nSicherheit Neben dem eigentlich schon verpflichtenden Einsatz von VerschlÃ¼sselung Ã¼ber https://, welche auf meinem Server Ã¼ber das Open Source Tool â€œNginx Proxy Managerâ€ (Link) einrichte, kann man noch mehr fÃ¼r die Sicherheit von Besuchern und uns als Betreibern tun. Ein groÃŸer Mehrwert zur Sicherheit ist wÃ¤hrend der Umsetzung als Nebenprodukt entstanden. Da ich ausschlieÃŸlich statische Seiten generiere und auf den Einsatz von Cookies verzichte, beschrÃ¤nkt sich der Besucher dieser Seiten fast vollstÃ¤ndig auf den bloÃŸen Download von Dateien ohne Nachladen weiterer Inhalte. Eine Ausnahme spielen die Assets (Bilder, Dokumente, etc.), aber dazu spÃ¤ter mehr.\nAuÃŸerdem sollte natÃ¼rlich der Zugang zum Server gesichert werden. So ist eine Firewall zu aktivieren und Standardports z.B. fÃ¼r SSH sind zu vermeiden. Auch empfiehlt es sich z.B. mit dem Open Source Tool Google Authenticator eine 2-Faktor Anmeldung am Server zu erzwingen. Eine VerschlÃ¼sselung der Daten auf dem DatentrÃ¤gern des Servers ist, wenn mÃ¶glich, sicherlich auch keine schlechte Idee. FÃ¼r den Root user deaktivieren wir noch die MÃ¶glichkeit sich aus der Ferne anzumelden und fÃ¼hren selbst unsere Kommandos niemals direkt als dieser aus.\nContent Mangagment System Eine immer populÃ¤rer werdende Praxis ist der Einsatz eines sogenannten Headless Content Management Systems (CMS) zur Datenhaltung, Verwaltung und ggfs. zum editieren von Beitragen wie diesem hier. Headless bedeutet dass das CMS die in ihm gepflegten Daten nicht direkt sondern nur Ã¼ber Webschnittstellen bereit stellt. Das hat, neben den Vorteilen der Modularisierung und UnabhÃ¤ngigkeit, den Vorzug dass wir unsere Inhalte zur Generierung von statischen Seiten an jedem Punkt unserer Prozesses abfragen kÃ¶nnen ohne dabei auf Limitierungen des CMS selbst gefesselt zu sein. Ich nutze hierfÃ¼r das Open Source Tool â€œCockpitâ€ (Link), und bin nach einer kurzen Einarbeitungszeit von vielleicht 6 Stunden hellauf begeistert. Das einzig verwirrende ist der Name. Wir erinnern uns: Unser Monitoring Tool heisst ebenfalls Cockpit. Die beiden haben so rein gar nichts miteinander zu tun, sondern teilen sich einfach den Namen.\nAsset Management Im Falle unserer Website sind unsere Assets vornehmlich Bilder. Und diese mÃ¶chten wir gerne, Ã¤hnlich wie unsere Inhalte, autark gespeichert und verwaltet wissen. AuÃŸerdem soll unser Asset Management System dazu in der Lage sein, aus den von uns hochgeladenen Bildern automatisch Miniaturen (Thumbnails) zu erstellen. GlÃ¼cklicherweise kann unser CMS System â€œCockpitâ€ (Link) das alles von Haus aus und bietet fÃ¼r den Zugriff auch noch Web Services, mit deren Verwertung wir bereits bekannt sind. In Cockpit ist das Asset Management ein vom CMS getrennter Bereich der aber von dort aus auch angesprochen werden kann, so dass wir Bilder beim Schreiben eines Artikels direkt hochladen und einfÃ¼gen kÃ¶nnen.\nBackend Oft habe ich in diesem Artikel Ã¼ber die Generierung von statischen Seiten gesprochen. Aber womit machen wir das eigentlich? Hier bin ich einen unÃ¼blichen Weg gegangen, und verwende das Open Source Tool Node-RED (Link). Node-RED ist eine sogenannte Low-Code Programmierumgebung auf die Ã¼ber den Browser zugegriffen wird. Low-Code heisst das man AblÃ¤ufe von einfacher bis mittlerer KomplexitÃ¤t ohne eigentliche Programmierung abbilden kann. Allerdings lassen sich auch jederzeit eigene Funktionen einbinden, wovon ich auch hÃ¤ufig gebrauch machen. Verschiedene Prozessschritte lassen sich grafisch miteinander verbinden, so dass als Nebenprodukt gleich noch ein Ablaufdiagramm entsteht. Ich nutze das Tool schon seit mehren Jahren z.B. zur Heimautomatisierung, was die Vielseitigkeit widerspiegelt. Es gibt aber auch noch eine Vielzahl anderer Tools die auf die Generierung von statischen Websites spezialisiert sind.\nReporting \u0026amp; Web Analytics Wenn man so viel Arbeit in die Erstellung von Inhalten und Struktur steckt, mÃ¶chte man natÃ¼rlich auch wissen ob und wie diese bei den Besuchern ankommen. Auch wie viele Besucher es Ã¼berhaupt in welchem Zeitraum gibt und ob sie lÃ¤ngere Zeit auf unseren Seiten bleiben interessiert uns. Wenn unsere Inhalte auÃŸerdem in mehreren Sprachen oder zumindest auch in Englisch oder Spanisch angeboten werden, dann mÃ¶chten wir auch wissen woher die Besucher kommen. Um all diese Fragen zu beantworten gibt es sogenannte Web Analytics Tool. Solch ein Tool bereitet die Antworten auf diese Fragestellungen in der Regel auch gleich noch grafisch auf. Das bekannteste unter ihnen ist sicherlich Google Analytics, welches sich kostenfrei in eine Website einbinden lÃ¤sst. Allerdings ist Google Analytics aus Gesichtspunkten des Datenschutzes nicht besonders sparsam und lÃ¤sst sich auch nicht selbst betreiben, sondern greift auf die Google Cloud Services zurÃ¼ck. Als Freund des DatenschÃ¼tzers hat sich hingegen das Open Source Tool â€œMatomoâ€ (Link) erwiesen. AuÃŸerdem lÃ¤sst es sich selbst betreiben, ist flink und geht auf alle genannten BedÃ¼rfnisse ein. In Matomo lÃ¤sst sich auch einstellen, dass Daten nur anonymisiert erhoben werden und z.B. keine IP-Adressen der Besucher gespeichert werden.\nDatenschutz \u0026amp; Rechtliche Bestimmungen Wo wir schon das Thema Datenschutz angesprochen haben, ohne darauf gegenÃ¼ber dem Webseitenbenutzer einzugehen geht es in Deutschland bzw. der EU nicht. Das ich mich gegen den Einsatz jeglicher Art von Cookies entscheide, vereinfacht diesen Teil aber ungemein (Stichwort: Cookie-Madness). So muss auf der gesamten Webseite aber trotzdem ein stets und einfach auffindbarer Bereich/Link zu den Datenschutzbestimmungen verfÃ¼gbar gemacht werden. Das gute ist aber: DatenschÃ¼tzer sind keine Unmenschen, im Gegenteil sogar. Sie wollen uns nur vor unlauteren GeschÃ¤ftspraktiken der Betreiber schÃ¼tzen und sind gegen lange komplizierte â€œParagraphenauflistereiâ€. Das heisst, dass die Datenschutzbestimmungen mÃ¶glichst kurz und verstÃ¤ndlich formuliert sein sollen. Sie sollten auÃŸerdem keine Abschnitte enthalten die fÃ¼r die Website gar nicht relevant sind und im Wesentlichen folgendes enthalten:\nWelche persÃ¶nlichen Daten werden gespeichert? Wo werden die Daten gespeichert? Von wem werden die Daten gespeichert? Wer ist Verantwortlicher Ansprechpartner? Wer darf die Daten auswerten? Welche Drittparteien sind evtl. involviert? Wie kann eine LÃ¶schung meiner evtl. vom Betreiber gespeicherten Daten initiiert werden? Wie gesagt mÃ¼ssen aus der Liste nicht alle Punkte relevant sein, z.B. wenn Ã¼berhaupt keine personalisierten Daten gespeichert werden. Das heisst dass vorgefertigte Texte nicht unbedingt gut sind und vor allem meistens Passagen enthalten die gekÃ¼rzt/entfernt werden sollten. NatÃ¼rlich bin ich kein Anwalt und die rechtlichen Grundlagen Ã¤ndern sich zur Zeit quartalsweise, aber wenn man sich an diese Grundlagen zum Datenschutz hÃ¤lt ist man auf jeden Fall gute dabei und handelt bedachter als viele andere Webseitenbetreiber.\nAus rechtlicher Sicht muss ebenso immer eine Impressum verfÃ¼gbar sein. Hier kann man sich durchaus an Vorlagen aus dem Internet bedienen, da es sich im Gegensatz zur DatenschutzerklÃ¤rung hier eher um unsere Absicherung gegenÃ¼ber dem Nutzer und keine Dienstleistung fÃ¼r ihn handelt. Pflichtangaben sind:\nVerantwortlicher fÃ¼r das Angebot gemÃ¤ÃŸ Â§ 5 TMG / Â§ 18 MStV Inhaltlich Verantwortlicher gemÃ¤ÃŸ Â§ 18 Abs. 2 MStV Beide Angaben sollten den vollstÃ¤ndigen Namen, ein Adresse und zumindest eine E-Mail Anschrift enthalten. Hier zÃ¤hlen leider keine Argumente aus Sicht des Datenschutzes fÃ¼r den Betreiberâ€¦\nFazit und Ausblick Wir kennen nun viele Kriterien die eine perfekte Website ausmachen und kÃ¶nnen bereits sagen, dass sich diese so mit einem Homepagebaukasten oder einem Web Content Managementsystem wie z.B. WordPress nicht umsetzen lassen. In den Folgeartikeln meiner Serie werden wir allerdings auch sehen, dass zur Umsetzung teilweise tiefgreifendes IT-VerstÃ¤ndnis benÃ¶tigt wird. Deshalb Rate ich allen die sich nicht so tief mit dieser Materie auseinander setzen mÃ¶chten trotzdem zum Einsatz von WordPress. Bei WordPress handelt es sich um ein millionenfach eingesetztes WCMS mit dem man schnell zu passablen Ergebnissen kommen kann und der Umsetzungsaufwand sich auch von Laien bewÃ¤ltigen lÃ¤sst.\nZusammengefasst ist folgender Anforderungskatalog:\nHosting (Bereitstellung) auf einem flinken Server als Basis Eine performante Netzwerkanbindung dieses Servers an das Internet DNS EintrÃ¤ge unser Domain(s)beim einem vertrauenswÃ¼rdigem Anbieter Virtualisierung einzelner Komponenten in sogenannten Containern Server Monitoring mit Cockpit (nicht das CMS Tool) Alerting mit einer geeigneten schlanken LÃ¶sung Eine Reverse Proxy Konfiguration inkl. Zertifikatsmanagement mit NGINX Proxy Manager Ein Backend auf Basis von Node-Red zur Generierung statischer Seiten Content und Asset Management mit Cockpit (nicht das Monitoring Tool) Web Analytics mit Matomo Einen Texteditor und Zugriff auf eine Shell mit Microsoft Visual Studio Code HTML5 Quellcode mit UnterstÃ¼tzung von Twitter Bootstrap 5 (also ohne jQuery) Komponenten Individuelle Stylesheets mit CSS Variablen Vanilla (keine Framework wie jQuery oder Angular) Javascript im Frontend In dieser Reihenfolge werde ich die Themen in den folgenden Artikeln auch beschreiben. Um noch etwas Spannung aufzubauen: Eine Lighthouse Berwertung von 100% Punkten in allen Bereichen lÃ¤sst sich umsetzen, diese Seite hier ist der Beweis :-).\n","tags":["deutsch"],"section":"blog"},{"date":"1652313600","url":"/blog/move-3-trackpad/","title":"Reanimation: Auf MacOS mit 3 Fingern Fenster schieben","summary":"Aus einem mir nicht erklÃ¤rlichen Grund, ist auf dem Mac seit geraumer Zeit das verschieben von Fenstern und Objekten mittels 3-Finger-Geste nicht mehr im Standard aktiviert. Das scheint nicht nicht nur mich gestÃ¶rt zu haben, da Apple einen relativ frisch aktualiserten Artikel zur Wiederherstellung dieses Verhaltens bereit stellt. Inhalt siehe hier:\nAnleitung WÃ¤hle das Apple-MenÃ¼ (ï£¿) \u0026gt; â€Systemeinstellungenâ€œ. Klicke auf â€Bedienungshilfenâ€œ. Klicke auf â€Zeigersteuerungâ€œ oder â€Maus \u0026amp; Trackpadâ€œ. Klicke auf die Taste â€Trackpad-Optionenâ€œ. Aktiviere die Option â€Trackpad zum Ziehen verwendenâ€œ oder â€Bewegen aktivierenâ€œ. WÃ¤hle im EinblendmenÃ¼ fÃ¼r den Zugstil die Option â€Mit drei Fingern bewegenâ€œ aus. Klicke auf â€OKâ€œ. ","content":"Aus einem mir nicht erklÃ¤rlichen Grund, ist auf dem Mac seit geraumer Zeit das verschieben von Fenstern und Objekten mittels 3-Finger-Geste nicht mehr im Standard aktiviert. Das scheint nicht nicht nur mich gestÃ¶rt zu haben, da Apple einen relativ frisch aktualiserten Artikel zur Wiederherstellung dieses Verhaltens bereit stellt. Inhalt siehe hier:\nAnleitung WÃ¤hle das Apple-MenÃ¼ (ï£¿) \u0026gt; â€Systemeinstellungenâ€œ. Klicke auf â€Bedienungshilfenâ€œ. Klicke auf â€Zeigersteuerungâ€œ oder â€Maus \u0026amp; Trackpadâ€œ. Klicke auf die Taste â€Trackpad-Optionenâ€œ. Aktiviere die Option â€Trackpad zum Ziehen verwendenâ€œ oder â€Bewegen aktivierenâ€œ. WÃ¤hle im EinblendmenÃ¼ fÃ¼r den Zugstil die Option â€Mit drei Fingern bewegenâ€œ aus. Klicke auf â€OKâ€œ. ","tags":["deutsch"],"section":"blog"},{"date":"1641081600","url":"/blog/ks-four-seasons/","title":"Vier Jahreszeiten im Kasseler Bergpark","summary":"Seit nun Ã¼ber einem Jahr leben wir mit dem Kasseler Bergpark direkt nebenan. In diesem Beitrag mÃ¶chte ich gerne die Impressionen und SehenswÃ¼rdigkeiten, die sich im Lauf der Jahreszeiten bieten, teilen.\nDas im Artikel beschriebene Gebiet bezieht sich auf ein grÃ¶ÃŸeres Areal im Ã¤uÃŸeren Westen Kassels, welches sich, auch wenn man alle Highlights sehen mÃ¶chte, gut in einem oder zwei TagesausflÃ¼gen fuÃŸlÃ¤ufig erkunden lÃ¤sst. Eine Ãœbersicht Ã¼ber das Gebiet lÃ¤sst sich hier finden: Link zu OpenStreetMap.\n","content":"Seit nun Ã¼ber einem Jahr leben wir mit dem Kasseler Bergpark direkt nebenan. In diesem Beitrag mÃ¶chte ich gerne die Impressionen und SehenswÃ¼rdigkeiten, die sich im Lauf der Jahreszeiten bieten, teilen.\nDas im Artikel beschriebene Gebiet bezieht sich auf ein grÃ¶ÃŸeres Areal im Ã¤uÃŸeren Westen Kassels, welches sich, auch wenn man alle Highlights sehen mÃ¶chte, gut in einem oder zwei TagesausflÃ¼gen fuÃŸlÃ¤ufig erkunden lÃ¤sst. Eine Ãœbersicht Ã¼ber das Gebiet lÃ¤sst sich hier finden: Link zu OpenStreetMap.\nDer Wikipedia Artikel zum Bergpark WilhelmshÃ¶he liefert Interessierten viele weitere Details, zu dem seit 2013 als UNESCO Welterbe definierten Areal. Hier mÃ¶chte ich aber mehr auf die persÃ¶nlichen und subjektiven EindrÃ¼cke, als auf objektive Fakten eingehen.\nEinmal alles neu im FrÃ¼hling Wenn Mutter Natur langsam wieder aus dem Winterschlaf erwacht, wird auch das Treiben im Bergpark wieder lauter. An jeder Ecke sieht man das GrÃ¼n sprieÃŸen und die Tiere aus ihren Verstecken hervorkommen. Auch die Menschen scheinen frische Energie getankt zu haben und Ihre guten VorsÃ¤tze fÃ¼r das Neue Jahr sind wohl auch noch nicht ganz beiseite gelegt. Man sieht es an Ihren GesichtsausdrÃ¼cken, wenn sie einem beim Spaziergang den Weg kreuzen. Das Bild zum Abschnitt ist auch gleich eine der hÃ¤ufigst gemachten Aufnahmen im Bergpark: der Blick vom Herkules herunter auf die Stadt. Die Stufen sind wohl auch der Ort im Park, an dem man den anderen Besuchern gezwungenermaÃŸen am nÃ¤chsten kommt. Wer das oder generell Treppen nicht mag, kann diese beim Abstieg auch meiden, in dem er den Seitenpfad von oben aus rechts gesehen wÃ¤hlt.\nSommer, Sonne, Sonnenschein Warme Temperaturen, lebendige GerÃ¼che und reges Treiben sorgen auch im Bergpark fÃ¼r gute Laune. Zu dieser Jahreszeit sieht man auch die meisten Touristen hier, welche aus aller Welt herbeistrÃ¶men. So erkennt man beim Lauschen deren Unterhaltungen vornehmlich franzÃ¶sisch, spanisch, englisch, tÃ¼rkisch und viele der deutschen Dialekte. Im frÃ¼hen Sommer lohnt es sich besonders eine Picknickdecke mitzubringen, da die GrÃ¤ser auf den vielen Wiesen noch nicht so hoch sind und der SchÃ¤fer seine Schafe noch nicht regelmÃ¤ÃŸig durch den Park fÃ¼hrt. Ein ruhiges PlÃ¤tzchen ohne Passanten findet sich dann recht schnell. Das mit den Schafen hat insofern mit der Gastlichkeit der Wiesen zu tun, als dass die Tiere neben interessanten GerÃ¼chen auch diverse Ausscheidungen hinerlassen. Das viele der Wiesen nicht gemÃ¤ht werden, dient natÃ¼rlich dem Naturschutz. Vor dem SchloÃŸ liegt aber eine sehr groÃŸe und zentrale GrasflÃ¤che, die den ganzen Sommer Ã¼ber zum Sonnen und Liegen einlÃ¤dt. Dort ist es dann aber natÃ¼rlich nicht ganz so still und es ist dementsprechend schwerer der Natur zuzuhÃ¶ren. Das Bild ist auf dem Weg vom SchloÃŸ, links vorbei am SteinhÃ¶fer Wasserfall, auf dem Weg zur LÃ¶wenburg entstanden.\nMelancholischer Herbstblues Wenn die Tage wieder kÃ¼rzer werden, sind die meisten Parkbesucher entweder einheimische oder Menschen aus der nÃ¤heren Umgebung. Zu dieser Jahreszeit erinnern mich viele Teile des Bergparks an ein irisches Landschaftsbild. So z.B. auch der auf dem Bild zu sehende Wasserverlauf, wo eigentlich nur noch ein paar Kobolde fehlen. Auch im spÃ¤teren Herbst verschwindet das GrÃ¼n des Parks nie vollstÃ¤ndig. Entsprechende Kleidung vorausgesetzt, kann einem ein Besuch einer der WasserfÃ¤lle, gerade bei Regen, die Natur und ein ganz besonderes GefÃ¼hl der Verbundenheit nÃ¤her gebracht werden. Die im deutschen Herbst Ã¼blichen Farbenspiele gibt es natÃ¼rlich trotzdem, besonders in den Bereichen in denen viele LaubbÃ¤ume stehen (wo auch sonst).\nIm Winterschlaf Wenn man sich trotz Lethargie und MÃ¼digkeit zu einem Spaziergang im Park aufraffen kann, wird man dafÃ¼r stets belohnt. Es ist interessant zu sehen, dass eben doch nicht alle Tiere in wÃ¤rmere Regionen gezogen sind und auch die Pflanzenwelt nicht vÃ¶llig ruht. Aufgrund der hÃ¶heren Lagen im Vergleich zum Rest Kassels, stehen die Chancen auf Schnee bei entsprechenden Temperaturen nicht schlecht. Einmal ins Winterkleid eingehÃ¼llt erscheinen uns die vorher noch so anders bekannten Szenenbilder auf einmal so als seien sie aus einer finsteren mittelalterlichen TV-Serie wie â€œGame of Thronesâ€ entnommen. Die BaukrÃ¤ne, zuerst am Herkules und spÃ¤ter an der LÃ¶wenburg, muss man sich dafÃ¼r natÃ¼rlich weg denken.\nFazit Ich hoffe ich konnte evtl. einige noch unbekannte Aspekte unseres schÃ¶nen Berkparks aufzeigen und werde diesen Beitrag in Zukunft noch um eine Bilderserie ergÃ¤nzen. Zum Schluss noch die Bitte um einen kleinen Gefallen: Wer sich zufÃ¤llig vor Ort wiederfindet, der mÃ¶ge doch bitte kurz Laut â€œMaxâ€ und/oder â€œSarinaâ€ rufen. Im Falle einer Antwort, bitte ich um eine kurze, sowie persÃ¶nliche Nachricht an mich.\n","tags":["andere"],"section":"blog"}]