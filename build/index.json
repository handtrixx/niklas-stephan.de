[{"date":"1746057600","url":"/blog/oss-mobiles-2025/","title":"Mobiltelefone 2025: Der King in Datenschutz und Nachhaltigkeit","summary":"Ein Überblick über nachhaltige Mobiltelefone 2025, die die Daten des Benutzers schützen.","content":"Ein Überblick über nachhaltige Mobiltelefone 2025, die die Daten des Benutzers schützen.\nStändig werden neue Handys auf den Markt geworfen und von unabhängigen Agenturen, Zeitschriften und Personen getestet. Der Fokus dieser Tests liegt jedoch meist auf der Hardwareleistung des Geräts. Beim Betriebssystem wird in 99 % der Fälle einfach hingenommen, dass auf dem Gerät sowohl von Google als auch vom Hardwarehersteller eventuell unerwünschte und vor allem datenhungrige Applikationen vorinstalliert werden. In vielen Fällen lassen sich diese dann nicht einmal deinstallieren.\nDeshalb konzentriert sich dieser unabhängige Test ganz auf den Aspekt des Datenschutzes und behandelt die anderen Kriterien nur beiläufig. Zuvor räumen wir aber noch mit falschen Vorstellungen über den Zusammenhang zwischen Open Source, Linux, Android und Google auf.\nFür ungeduldige Leser: Das Vergleichsergebnis Hier die Ergebnisse des Vergleichs im Überblick. Die genaue Beschreibung der einzelnen Handys und die Vergleichstabelle findest du weiter unten.\nGesamtsieger:\nFairphone 5 (30 Punkte) – Beste Kombination aus Datenschutz, Nachhaltigkeit und Alltagstauglichkeit.\nTechnischer Sieger:\nGoogle Pixel 8 Pro (27 Punkte) – Überragende Kamera, Top-Hardware, lange Updates.\nPreis-Leistungs-Sieger:\nXiaomi Poco X3 NFC (24 Punkte) – Sehr gutes Gesamtpaket zum günstigen Preis, mit LineageOS-Unterstützung.\nRanking (Punkte):\nFairphone 5 (30) Volla Phone X23 (28) Google Pixel 8 Pro (27) Volla Quintus (26) Google Pixel 9 (26) OnePlus 11 (25) Motorola Edge 40 Pro (25) Xiaomi Poco X3 NFC (24) Motorola Moto G32 (23) Jolla C2 Community Phone (19) Open Source, Linux, Android und Google Open Source bedeutet nicht automatisch Datenschutz oder Sicherheit. Viele Menschen setzen Open-Source-Software mit sicherer und datenschutzfreundlicher Software gleich, was jedoch nicht immer der Realität entspricht. Es ist wichtig, die Unterschiede und Zusammenhänge zwischen Open Source, Linux, Android und den Diensten von Google zu verstehen.\nOpen Source bedeutet zunächst einmal, dass der Quellcode einer Software öffentlich einsehbar ist. Dadurch können Entwickler und Experten den Code überprüfen, Schwachstellen identifizieren und zur Verbesserung beitragen.\nLinux ist ein Open-Source-Betriebssystem, das als Grundlage für viele andere Systeme dient, einschließlich Android. Es bietet eine hohe Flexibilität und Anpassungsfähigkeit, jedoch hängen Datenschutz und Sicherheit stark von der jeweiligen Implementierung und den darauf aufbauenden Diensten ab. Da auch Android auf Linux basiert, erbt es zwar einige der Vorteile von Open Source, ist jedoch stark von den Entscheidungen und Diensten von Google geprägt, was den Datenschutz beeinflussen kann.\nDas heißt: Ein Mobiltelefon, auf dem Android ohne Google-Dienste betrieben wird, kann als deutlich datenschutzfreundlicher betrachtet werden.\nAlternativen zu Android Es gibt natürlich auch noch andere Optionen als Android, wie beispielsweise Apples iOS oder andere Linux-basierte Betriebssysteme.\niOS ist jedoch proprietär (nicht Open Source) und stark in das Apple-Ökosystem eingebunden. Auf Apple-Mobiltelefonen ist es außerdem nicht möglich, alternative Betriebssysteme zu installieren.\nDeshalb stellen in diesem Test Apples Telefone keine Option dar.\nEine weitere Möglichkeit sind Linux-basierte Betriebssysteme wie /e/OS, LineageOS oder Ubuntu Touch, die ohne Google-Dienste auskommen und den Fokus auf Datenschutz legen.\nHier muss man auch wieder unterscheiden, ob es sich beim Betriebssystem um ein von den Google-Diensten befreites Android handelt, wie das z. B. bei LineageOS der Fall ist, oder um eine eigenständige Distribution wie z. B. Sailfish OS.\nWährend sich auf den Android-Systemen grundsätzlich alle dafür entwickelten Anwendungen ausführen lassen und man ggf. nur auf einen nicht von Google betriebenen App-Store ausweichen muss, sieht die Sache bei einer eigenständigen Distribution anders aus.\nDa sich aktuell kaum Entwickler an den Entwicklungen zu eigenständigen Distributionen beteiligen und diese auch nur ein winziges Zielpublikum bedienen, herrscht bei der Auswahl von Anwendungen größtenteils gähnende Leere. An dieser Situation wird sich auch mittelfristig nichts ändern, weshalb wir diese in unserem Test ebenfalls ignorieren werden.\nTestkriterien Die Bewertung basiert auf folgenden Kriterien (jeweils 0–5 Punkte):\nGoogle-freies Android ab Werk oder ohne Funktionsverlust installierbar Zeitgemäße Hardware (CPU, RAM, Speicher) Nachhaltigkeit (Austauschbarkeit von Komponenten) Kameraqualität Akkukapazität Maximal sind 30 Punkte erreichbar.\nDie Testkandidaten im Überblick Fairphone 5 Das Fairphone 5 ist das nachhaltigste Gerät im Test. Es überzeugt mit modularer Bauweise, langer Ersatzteilverfügbarkeit und einem wechselbaren Akku. Die Hardware ist solide, die Kamera gut, und es gibt offiziellen Support für alternative Android-Versionen wie /e/OS und LineageOS. Wer Wert auf Datenschutz und Reparierbarkeit legt, findet hier die beste Option – auch wenn der Preis im oberen Bereich liegt.\nTechnische Daten:\nBetriebssystem: Android (LineageOS-Support)\nDisplay: 6,46\u0026quot;, 2770x1224 Pixel\nKamera hinten: 50 MP\nKamera vorne: 50 MP\nSoC: Qualcomm QCM6490, 8 Kerne\nRAM: 8 GB\nSpeicher: 256 GB, microSD-Slot\nAkku: 4200 mAh, wechselbar\nBesonderheiten: IP55, MIL-STD-810H, Stereo-Lautsprecher\nPreis: 578 €\nVolla Phone X23 Das Volla Phone X23 kommt ab Werk mit einem Google-freien Android (Volla OS) und bietet einen wechselbaren Akku sowie robuste Bauweise (IP68, MIL-STD-810H). Die Hardware ist solide, die Kamera gut, und der Preis ist fair. Für Datenschutz-Fans, die ein robustes und alltagstaugliches Gerät suchen, eine sehr gute Wahl.\nTechnische Daten:\nBetriebssystem: Volla OS\nDisplay: 6,1\u0026quot;, 1560x720 Pixel\nKamera hinten: 48 MP + 8 MP\nKamera vorne: 16 MP\nSoC: MediaTek Helio G99, 8 Kerne\nRAM: 6 GB\nSpeicher: 128 GB, microSD-Slot\nAkku: 5000 mAh, wechselbar\nBesonderheiten: IP68, MIL-STD-810H\nPreis: 564 €\nGoogle Pixel 8 Pro Das Pixel 8 Pro bietet Top-Hardware, exzellente Kamera und lange Update-Garantie. Dank LineageOS-Support lässt sich ein Google-freies Android installieren, allerdings ist der Weg dahin nicht ganz trivial und der Akku ist nicht wechselbar. Wer Wert auf Kamera und Performance legt und bereit ist, das System umzubauen, bekommt hier ein sehr gutes Gesamtpaket.\nTechnische Daten:\nBetriebssystem: Android 15 (Update), LineageOS-Support\nDisplay: 6,7\u0026quot;, 2992x1344 Pixel\nKamera hinten: 50 MP + 48 MP + 48 MP\nKamera vorne: 10,5 MP\nSoC: Google Tensor G3, 9 Kerne\nRAM: 12 GB\nSpeicher: 128 GB\nAkku: 5050 mAh, fest verbaut\nBesonderheiten: IP68, 7 Jahre Updates\nPreis: 545 €\nVolla Quintus Das Volla Quintus ist das Flaggschiff von Volla, kommt ebenfalls mit Google-freiem Android und sehr guter Hardware. Der Akku ist wechselbar, das Display groß und hell. Der Preis ist allerdings recht hoch, was das Preis-Leistungs-Verhältnis etwas schmälert.\nTechnische Daten:\nBetriebssystem: Volla OS (Android)\nDisplay: 6,78\u0026quot;, 2400x1080 Pixel, AMOLED, 120 Hz\nKamera hinten: 50 MP + 8 MP + 2 MP\nKamera vorne: 15,9 MP\nSoC: MediaTek Dimensity 7050, 8 Kerne\nRAM: 8 GB\nSpeicher: 256 GB\nAkku: 4600 mAh, wechselbar\nBesonderheiten: 5G, AMOLED, kabelloses Laden\nPreis: 719 €\nGoogle Pixel 9 Das Pixel 9 ist ähnlich stark wie das 8 Pro, etwas kompakter und günstiger. Auch hier gibt es lange Updates und LineageOS-Support. Die Kamera ist hervorragend, der Akku fest verbaut. Für alle, die ein aktuelles, leistungsstarkes Gerät mit Custom-ROM-Option suchen, eine sehr gute Wahl.\nTechnische Daten:\nBetriebssystem: Android 15 (Update), LineageOS-Support\nDisplay: 6,3\u0026quot;, 2424x1080 Pixel, AMOLED\nKamera hinten: 50 MP + 48 MP\nKamera vorne: 10,5 MP\nSoC: Google Tensor G4, 8 Kerne\nRAM: 12 GB\nSpeicher: 128 GB\nAkku: 4700 mAh, fest verbaut\nBesonderheiten: IP68, 7 Jahre Updates\nPreis: 569 €\nOnePlus 11 Das OnePlus 11 bietet Top-Hardware und ist für Custom-ROMs wie LineageOS geeignet. Der Akku ist allerdings nicht wechselbar und die Nachhaltigkeit ist nur durchschnittlich. Wer Wert auf Performance und ein gutes Preis-Leistungs-Verhältnis legt, findet hier ein starkes Gerät.\nTechnische Daten:\nBetriebssystem: Android (LineageOS-Support)\nDisplay: 6,7\u0026quot;, 3216x1440 Pixel\nKamera hinten: 50 MP\nKamera vorne: 16 MP\nSoC: Snapdragon 8 Gen 2, 8 Kerne\nRAM: 8 GB\nSpeicher: 128 GB\nAkku: 5000 mAh, fest verbaut\nBesonderheiten: IP64, Stereo-Lautsprecher\nPreis: 538 €\nMotorola Edge 40 Pro Das Edge 40 Pro ist ein leistungsstarkes Gerät mit sehr guter Kamera und LineageOS-Support. Der Akku ist fest verbaut, die Reparierbarkeit eingeschränkt. Für Power-User, die ein Custom-ROM nutzen wollen, eine interessante Option.\nTechnische Daten:\nBetriebssystem: Android (LineageOS-Support)\nDisplay: 6,67\u0026quot;, 2400x1080 Pixel\nKamera hinten: 50 MP\nKamera vorne: 60 MP\nSoC: Snapdragon 8 Gen 2, 8 Kerne\nRAM: 12 GB\nSpeicher: 256 GB\nAkku: 4600 mAh, fest verbaut\nBesonderheiten: IP68, kabelloses Laden\nPreis: 499 €\nXiaomi Poco X3 NFC Das Poco X3 NFC ist ein günstiges, aber leistungsfähiges Gerät mit LineageOS-Support. Die Installation eines Google-freien Systems ist möglich, aber etwas umständlich. Die Hardware ist solide, die Kamera gut, der Akku groß. Für Sparfüchse mit Datenschutz-Anspruch eine sehr gute Wahl.\nTechnische Daten:\nBetriebssystem: Android 11 (Update), LineageOS-Support\nDisplay: 6,67\u0026quot;, 2400x1080 Pixel, 120 Hz\nKamera hinten: 64 MP + 13 MP + 2 MP + 2 MP\nKamera vorne: 20 MP\nSoC: Snapdragon 732G, 8 Kerne\nRAM: 6 GB\nSpeicher: 128 GB, microSD-Slot\nAkku: 5160 mAh, fest verbaut\nBesonderheiten: IP53, Stereo-Lautsprecher\nPreis: ca. 250 €\nMotorola Moto G32 Das Moto G32 ist das günstigste Gerät im Test und bietet solide Hardware für den Alltag. Es gibt LineageOS-Support, aber die Kamera und Performance sind nur durchschnittlich, und der Akku ist nicht wechselbar. Für Sparfüchse, die Wert auf Datenschutz legen, dennoch eine brauchbare Wahl.\nTechnische Daten:\nBetriebssystem: Android 12 (ab Werk), LineageOS-Support\nDisplay: 6,5\u0026quot;, 2400x1080 Pixel\nKamera hinten: 50 MP + 8 MP + 2 MP\nKamera vorne: 16 MP\nSoC: Snapdragon 680, 8 Kerne\nRAM: 6 GB\nSpeicher: 128 GB, microSD-Slot\nAkku: 5000 mAh, fest verbaut\nBesonderheiten: UKW-Radio, IP52\nPreis: 421 €\nJolla C2 Community Phone Das Jolla C2 Community Phone setzt auf Sailfish OS, ein Linux-basiertes System. Die Hardware ist solide, aber die Akkulaufzeit schwach, kein 5G, und die App-Auswahl ist sehr eingeschränkt. Für experimentierfreudige Nutzer mit Fokus auf Open Source, aber im Alltag nur bedingt empfehlenswert.\nTechnische Daten:\nBetriebssystem: Sailfish OS\nDisplay: 6,52\u0026quot;, 1600x720 Pixel\nKamera hinten: 64 MP\nKamera vorne: 16 MP\nSoC: ARM Cortex, 8 Kerne\nRAM: 8 GB\nSpeicher: 128 GB, microSD-Slot\nAkku: 4000 mAh, fest verbaut\nBesonderheiten: Kein GPS, kein 5G\nPreis: 284 € + 60 €/Jahr ab dem 2. Jahr\nDie Bewertungstabelle im Detail Kriterium Fairphone 5 Volla X23 Pixel 8 Pro Volla Quintus Pixel 9 OnePlus 11 Edge 40 Pro Poco X3 NFC Moto G32 Jolla C2 Google-freies Android (ab Werk/Installierbar) 5 5 3 5 3 3 3 3 3 5 Hardware (CPU, RAM, Speicher) 4 4 5 5 5 5 5 4 3 3 Nachhaltigkeit (Austauschbarkeit) 5 4 2 3 2 2 2 2 2 2 Kameraqualität 4 4 5 4 5 5 5 4 3 2 Akkukapazität 4 5 5 4 4 5 4 5 5 2 Gesamtpunkte 30 28 27 26 26 25 25 24 23 19 Legende:\n5 = erfüllt das Kriterium voll und ganz\n4 = sehr gut, kleine Einschränkungen\n3 = durchschnittlich / akzeptabel\n2 = unterdurchschnittlich / größere Einschränkungen\n1 = ungenügend\nFazit Das Fairphone 5 ist die beste Wahl für alle, die Datenschutz, Nachhaltigkeit und Alltagstauglichkeit kombinieren wollen. Die Volla Phones sind für Puristen und Datenschutz-Fans sehr attraktiv. Die Pixel-Geräte bieten die beste Kamera und lange Updates, erfordern aber Eigeninitiative für ein Google-freies System. OnePlus und Motorola sind gute Allrounder, wenn Custom-ROMs und Preis-Leistung im Vordergrund stehen. Das Xiaomi Poco X3 NFC ist der Preis-Leistungs-Tipp für alle, die mit kleinen Abstrichen leben können. Das Jolla C2 ist ein spannendes Experiment, aber für den Alltag nur eingeschränkt geeignet.\nLinks:\nUbuntu Touch Geräteübersicht\nLineageOS\n","tags":["Open Source","Hardware"],"section":"blog"},{"date":"1745280000","url":"/blog/perfect-website-2025/","title":"Herr Doktor, ich habe Wordpress, kann man da noch etwas machen?","summary":"Ein umfassender Vergleich moderner CMS und Static Site Generators für 2025, erstellt von Niklas Stephan und seinem KI-Assistenten","content":"Ein umfassender Vergleich moderner CMS und Static Site Generators für 2025, erstellt von Niklas Stephan und seinem KI-Assistenten\nDie Kurzfassung: Das Ranking der besten CMS und Static Site Generators Nach umfassender Analyse von 12 Kriterien haben wir klare Gewinner identifiziert:\nHugo (52 Punkte) - Der Performance-Champion Ghost (50 Punkte) - Die ausgewogene Alternative Jekyll (49 Punkte) - Der Sicherheits-Experte WordPress landet mit nur 38 Punkten auf Platz 9 - warum? Das erfahren Sie in unserer Detailanalyse.\nDie Vergleichstabelle im Detail Kriterium Hexo Grav Strapi Cockpit Ghost Publii WordPress Hugo Jekyll Joomla Typ(en) ★★★★☆ ★★★★☆ ★★★☆☆ ★★★☆☆ ★★★★☆ ★★★★☆ ★★★★★ ★★★★☆ ★★★★☆ ★★★★★ Backend ★☆☆☆☆ ★★★★☆ ★★★★★ ★★★★☆ ★★★★☆ ★★☆☆☆ ★★★★★ ★☆☆☆☆ ★☆☆☆☆ ★★★★★ Frontend ★★★★☆ ★★★☆☆ ★★★☆☆ ★★★☆☆ ★★★★☆ ★★★★☆ ★★★☆☆ ★★★★☆ ★★★★☆ ★★★☆☆ Dateibasiert ★★★★★ ★★★★★ ★☆☆☆☆ ★★☆☆☆ ★★★☆☆ ★★★★★ ★☆☆☆☆ ★★★★★ ★★★★★ ★☆☆☆☆ Docker ready ★★★☆☆ ★★★☆☆ ★★★★★ ★★★★☆ ★★★★☆ ★★★☆☆ ★★★★☆ ★★★★☆ ★★★☆☆ ★★★★☆ Popularität ★★★☆☆ ★★★☆☆ ★★★★☆ ★★☆☆☆ ★★★★☆ ★★☆☆☆ ★★★★★ ★★★★☆ ★★★★☆ ★★★★☆ Open Source ★★★★★ ★★★★★ ★★★★★ ★★★★★ ★★★★★ ★★★★★ ★★★★★ ★★★★★ ★★★★★ ★★★★★ Moderne Codebase ★★★★☆ ★★★★☆ ★★★★★ ★★★★☆ ★★★★★ ★★★★☆ ★★☆☆☆ ★★★★★ ★★★★☆ ★★☆☆☆ Aktueller Code ★★★★☆ ★★★★☆ ★★★★★ ★★★★☆ ★★★★★ ★★★★☆ ★★★★☆ ★★★★★ ★★★★☆ ★★★★☆ Security ★★★★☆ ★★★★☆ ★★★★☆ ★★★☆☆ ★★★★☆ ★★★★☆ ★☆☆☆☆ ★★★★★ ★★★★★ ★☆☆☆☆ Performance ★★★★☆ ★★★★☆ ★★★☆☆ ★★★☆☆ ★★★★☆ ★★★★☆ ★★☆☆☆ ★★★★★ ★★★★★ ★★☆☆☆ Clean Frontend ★★★★☆ ★★★★☆ ★★★★☆ ★★★☆☆ ★★★★☆ ★★★★☆ ★☆☆☆☆ ★★★★★ ★★★★★ ★☆☆☆☆ Was bedeuten die Bewertungskriterien? Typ(en): Vielseitigkeit des Systems (SSG, Headless CMS, klassisches CMS) Backend: Qualität des Admin-Panels, API und Benutzerverwaltung Frontend: Flexibilität bei Themes, Templates und API-Ausgabe Dateibasiert: Arbeitet mit Dateien statt Datenbank (Markdown, Flat-File) Docker ready: Einfachheit der Container-Bereitstellung Popularität: Größe der Community und Verbreitung Open Source: Vollständige Quelloffenheit Moderne Codebase: Einsatz aktueller Technologien und Best Practices Aktueller Code: Aktivität der Entwicklung und Updates Security: Sicherheit gegen Angriffe und Exploits Performance: Ladegeschwindigkeit und Ressourceneffizienz Clean Frontend: Qualität des generierten HTML/CSS/JS-Codes Die drei Systemkategorien im Vergleich Static Site Generators: Die Performance-Champions Vorteile:\nHerausragende Sicherheit (keine Datenbank = weniger Angriffsfläche) Blitzschnelle Ladezeiten durch statische Dateien Sauberer, schlanker Code ohne Bloat Kostengünstigeres Hosting möglich Top-Empfehlungen:\nHugo (52 Punkte): Blitzschnell, sicher, mit sauberem Code-Output Jekyll (49 Punkte): Der Klassiker mit hervorragender Sicherheit Hexo (45 Punkte): Flexibler Generator mit Node.js-Basis Nachteile: Schwächeres Backend, erfordert mehr technisches Know-how\nModerne CMS: Die ausgewogene Mitte Vorteile:\nBalance zwischen Benutzerfreundlichkeit und technischer Qualität Moderne Codebasis mit aktuellen Technologien Bessere Sicherheit als traditionelle CMS Top-Empfehlungen:\nGhost (50 Punkte): Perfekt ausbalanciert, ohne signifikante Schwächen Strapi (47 Punkte): Führendes Headless-CMS mit exzellentem Backend Grav (47 Punkte): Dateibasiertes CMS mit modernem Ansatz Nachteile: Teilweise geringere Verbreitung und kleineres Plugin-Ökosystem\nTraditionelle CMS: Die Popularitäts-Riesen Vorteile:\nRiesiges Ökosystem an Plugins und Themes Umfangreiche Community und Support Einfacher Einstieg für Anfänger Vertreter:\nWordPress (38 Punkte): Marktführer mit großem Ökosystem Joomla (37 Punkte): Alternative mit ähnlichen Eigenschaften Nachteile: Erhebliche Sicherheitsprobleme, schlechte Performance, unsauberer Code\nDie Stärken und Schwächen der Top-Systeme Hugo: Der technische Spitzenreiter Stärken:\nBlitzschnelle Generierung (bis zu 100x schneller als andere SSGs) Hervorragende Sicherheit durch statische Dateien Extrem sauberer Output-Code Keine Datenbank nötig Schwächen:\nMinimalistisches Backend (1★) Steilere Lernkurve für Einsteiger Ghost: Die ausgewogene Alternative Stärken:\nModernes, intuitives Backend Hervorragende Editor-Erfahrung Gute Performance und Sicherheit Keine signifikanten Schwächen (mindestens 3★ in allen Kategorien) Schwächen:\nGeringere Verbreitung als WordPress Weniger Themes und Plugins verfügbar WordPress: Der populäre Problemfall Stärken:\nGrößtes Ökosystem an Plugins und Themes Höchste Verbreitung und Community Einfacher Einstieg für Anfänger Schwächen:\nKritische Sicherheitsprobleme (1★) Schlechte Performance (2★) Unsauberer Frontend-Code (1★) Veraltete Codebasis (2★) Fazit: Zeit für einen Wechsel? Die Analyse zeigt deutlich: Moderne Alternativen bieten erhebliche technische Vorteile gegenüber WordPress und Joomla. Besonders in den kritischen Bereichen Sicherheit, Performance und Code-Qualität fallen die traditionellen CMS deutlich ab.\nUnsere Empfehlungen:\nFür neue Projekte: Setzen Sie auf Hugo, Jekyll oder Ghost Für bestehende WordPress-Seiten: Erwägen Sie eine Migration zu Ghost oder Grav Für komplexe Anwendungen mit API-Bedarf: Strapi bietet ein hervorragendes Backend Die Entscheidung sollte letztlich von Ihren spezifischen Anforderungen abhängen - aber eines ist klar: Es gibt heute bessere Alternativen als WordPress. Ich selbst habe https://niklas-stephan.de inzwischen auf Hugo migriert und werde dazu eventuell in einem separaten Beitrag detailliert berichten.\nHaben Sie Fragen oder Anmkerungen zur Migration von WordPress zu einer modernen Alternative? Nutzen Sie die Kommentarfunktion um Ihre Gedanken mit uns zu teilen.\n","tags":["deutsch"],"section":"blog"},{"date":"1742342400","url":"/blog/distrobox/","title":"Distrobox - oder: \"Noch unabhängiger in Linux\"","summary":"Wenn du irgendwann genauso wie ich die Nase voll von MS Windows und Apple macOS hast, bleibt Linux als beliebteste und mittlerweile absolut konkurrenzfähige Alternative. Und schon stehst du vor der nächsten Entscheidung: Welches Linux darf es denn sein? Aber was wäre, wenn man einfach \u0026ldquo;jede\u0026rdquo; Art von Linux auf einmal laufen lassen könnte? Genau das verspricht Distrobox. Dieser Post geht kurz auf die Unterschiede zwischen Linux-Kernel, Distribution und Derivation ein und zeigt dann, wie man Distrobox installiert und konfiguriert.\n","content":"Wenn du irgendwann genauso wie ich die Nase voll von MS Windows und Apple macOS hast, bleibt Linux als beliebteste und mittlerweile absolut konkurrenzfähige Alternative. Und schon stehst du vor der nächsten Entscheidung: Welches Linux darf es denn sein? Aber was wäre, wenn man einfach \u0026ldquo;jede\u0026rdquo; Art von Linux auf einmal laufen lassen könnte? Genau das verspricht Distrobox. Dieser Post geht kurz auf die Unterschiede zwischen Linux-Kernel, Distribution und Derivation ein und zeigt dann, wie man Distrobox installiert und konfiguriert.\nKernel, Distribution und Derviation Hier die Unterschiede kurz erklärt, was hilft, den Rest des Artikels besser zu verstehen (Quelle: GPT-4o mini, Prompt: \u0026ldquo;Was ist der Unterschied in Linux zwischen: Kernel, Distribution und Derivation?\u0026rdquo;, Datum: 27.03.2025).\nKernel Der Kernel ist der zentrale Bestandteil eines Betriebssystems. Er ist die Schnittstelle zwischen der Hardware und der Software und verwaltet die Systemressourcen wie CPU, Speicher und Geräte. Der Linux-Kernel ist der Kern des Linux-Betriebssystems und wird ständig weiterentwickelt. Er ist verantwortlich für die Ausführung von Prozessen, das Management von Speicher und die Kommunikation mit Hardwarekomponenten.\nDistribution Eine Linux-Distribution ist eine vollständige Sammlung von Software, die auf dem Linux-Kernel basiert. Sie umfasst den Kernel selbst sowie eine Vielzahl von Softwarepaketen, die für die Nutzung des Systems erforderlich sind, wie z.B. Desktop-Umgebungen, Anwendungssoftware, Systemwerkzeuge und Treiber. Beispiele für beliebte Linux-Distributionen sind:\nUbuntu, Fedora, Debian, Arch Linux. Jede Distribution hat ihre eigenen Merkmale, Paketverwaltungssysteme und Zielgruppen.\nDerivation Derivation (oder abgeleitete Distribution) bezieht sich auf eine Distribution, die auf einer anderen Distribution basiert. Sie nutzt den Kernel und die Softwarepakete der ursprünglichen Distribution, fügt jedoch eigene Anpassungen, zusätzliche Software oder spezifische Konfigurationen hinzu. Ein Beispiel für eine Derivation ist Linux Mint, das auf Ubuntu basiert, aber eine andere Benutzeroberfläche und zusätzliche Funktionen bietet. Derivationen können oft als benutzerfreundlicher oder spezialisierter angesehen werden, da sie auf den Stärken der Basisdistribution aufbauen.\nUnd warum nun Distrobox? Distrobox zu nutzen, kann aus mehreren Gründen nützlich oder sogar sinnvoll sein:\nIch verwende Distribution X, kenne mich aber in Distribution Y besser aus. Einige Tools in der Linux-Welt sind auf eine bestimmte Linux-Distribution \u0026ldquo;gemünzt\u0026rdquo;. Ein prominentes Beispiel hierfür sind die Paketmanager. In Fedora und Co. hat sich da dnf durchgesetzt, während unter Ubuntu und Co. apt üblich ist. Mit Distrobox kannst du einfach beide verwenden.\nDeine Wunsch-App gibt es nur für Distribution Y, aber du nutzt Distribution X. Dann installiere sie doch einfach über Distrobox 😊.\nEin Container oder gar eine VM wäre mit \u0026ldquo;Kanonen auf Spatzen\u0026rdquo; geschossen. Zum Ausprobieren empfiehlt es sich nach wie vor eher, mit einer vollständigen Container-Management-Software wie Docker zu arbeiten, aber eine tiefgehende Integration in dein Betriebssystem lässt sich einfacher mit Distrobox umsetzen.\nWeil es lustig ist. Ja, zumindest finde ich es lustig, auch auf einem Desktop zu sehen, dass alle Linux-Distributionen auf dem gleichen Kernel basieren und im Endeffekt nur ein Gerüst darstellen.\nInstallation von Distrobox Die Installationspakete für Distrobox sind mittlerweile in allen großen Distributionen integriert und stellen die einfachste Variante dar, Distrobox zu installieren. Die Installation selbst könnt ihr entweder über die grafische Oberfläche eurer Paketverwaltung durchführen oder wie folgt über die Kommandozeile.\nFür Fedora / CentOs / RedHat:\n1sudo dnf install distrobox Für Debian / Ubuntu:\n1sudo apt-get install distrobox Alternativ, z.B. wenn du eine Distribution nutzt, die nur eine ältere Version von Distrobox anbietet, kannst du die Installation auch mit:\n1curl -s https://raw.githubusercontent.com/89luca89/distrobox/main/install | sudo sh starten. Denke dann aber daran, dass du für Updates selbst zuständig bist (diese startest du einfach durch einen erneuten Aufruf des zuvor genannten Befehls).\nErstellen eines neuen Containers Das Erstellen von Containern geht mit einem simplen Befehl auf der Kommandozeile vonstatten, z.B. so:\n1distrobox create --name ubuntu_container --image ubuntu:latest In diesem Fall würde ein neuer Container mit dem Namen \u0026ldquo;ubuntu_container\u0026rdquo; und der neuesten Ubuntu-Version angelegt werden.\nZwischen den Containern hin und her wechseln Um einen Container wie den im Beispiel zuvor erstellten \u0026ldquo;ubuntu_container\u0026rdquo; zu betreten, führt man folgendes Kommando aus:\n1distrobox enter ubuntu_container Um einen Container wieder zu verlassen, reicht ein simples\n1exit Denke daran, den Container erst zu verlassen, bevor du in einen anderen springst.\nWeitere nützliche Befehle Alle vorhandenen Container kannst du dir mit folgendem Befehl anzeigen lassen:\n1distrobox list Einen laufenden Container wie unseren \u0026ldquo;ubuntu_container\u0026rdquo; kannst du mit folgendem Befehl stoppen:\n1distrobox stop ubuntu_container Und um einen Container komplett zu löschen, benötigen wir folgendes Kommando\n1distrobox rm ubuntu_container Fazit Distrobox macht es maximal einfach, zwischen verschiedenen Linux-Distributionen hin und her zu springen, ohne eine Dual-Boot-Installation, eine virtuelle Maschine oder Docker/Podman-Container anlegen zu müssen. Das macht sogar noch Sinn, wenn du z.B. schon Docker installiert hast, weil die dort verfügbaren Images in der Regel für Serversysteme gedacht sind und deshalb für die interaktive Nutzung nicht wirklich taugen.\nBonustipp Wenn du beim betreten der Shell Kommandozeile auch ein cooles OS Logo mit Systeminformationen sehen willst: Das geht mit neofetch. Das Paket kann über alle gängigen Paketmanager installiert werden und heißt, rate mal, \u0026ldquo;neofetch\u0026rdquo;. Für einen automatischen Start nach der Installation einfach neofetch ans ende eurer ~/.bashrc oder ~/.zshrc setzen.\n","tags":["deutsch","linux"],"section":"blog"},{"date":"1740096000","url":"/blog/docker-time-machine/","title":"Apple Time Machine Backup auf einen Netzwerkspeicher mit Linux und Docker Compose","summary":"Brauchen wir 2025 wirklich noch eine spezielle Lösung für das Backup unseres Rechners? Wir haben doch iCloud/OneDrive/…! – Korrekt, aber dabei handelt es sich um Dateisynchronisation und nicht um Backups! Den Unterschied und wie man mit Hilfe eines Raspberry Pi oder ähnlichem mit Docker Compose eine Backup Lösung für seinen Mac aufsetzt, die der original Apple TimeMachine in nichts nachsteht, erkläre ich in diesem Post.\nGrundbegriffe, Backup vs. Synchronisation Mit der sogenannten TimeMachine hatte Apple es wieder einmal geschafft, einen Service der bei anderen wahnsinnig kompliziert einzurichten ist/war, kinderleicht bedienbar zu machen. Leider scheinen Datensicherungen in das lokale Netz aus der Mode gekommen zu sein, so propagiert Apple selbst nur noch das Backup direkt auf einen USB-Speicher oder eben in die eigene iCloud. Dabei haben beide Lösungen größere Nachteile. Für das lokale Backup muss ich immer daran denken, dass externe Speichermedium in den Rechner zu stecken und kommt es z.B. zu einem Kurzschluss während beide Geräte miteinander verbunden sind, sind auch beide zerstört. Dahin ist die Datensicherung, genau dann wenn man sie am meisten braucht. Die Alternativen iCloud und Co. begleiten ebenfalls mehrere Nachteile. Zum Einem vertrauen wir unsere Daten einem externen Anbieter an, der uns zwar versprechen kann dass diese dort sicher und geschützt aufbewahrt werden, aber garantieren kann uns das niemand. Außerdem handelt es sich bei iCloud und ähnlichen zunächst nur um Services zur Dateisynchronisation. D.h. meine Daten werden parallel lokal und eben in der Cloud abgelegt. Das heisst auch, wenn ich eine Datei lokal lösche, passiert das gleiche auch in der Cloud. Und wieder Adieu liebe Dateischerung. Dem entgegen wirk die Dateiversionierung die entweder Standardmäßig oder optional aktiviert werden kann, allerdings habe ich selbst die Erfahrung gemacht, dass diese auch genau dann wenn man sie bräuchte gerade mal nicht funktioniert.\n","content":"Brauchen wir 2025 wirklich noch eine spezielle Lösung für das Backup unseres Rechners? Wir haben doch iCloud/OneDrive/…! – Korrekt, aber dabei handelt es sich um Dateisynchronisation und nicht um Backups! Den Unterschied und wie man mit Hilfe eines Raspberry Pi oder ähnlichem mit Docker Compose eine Backup Lösung für seinen Mac aufsetzt, die der original Apple TimeMachine in nichts nachsteht, erkläre ich in diesem Post.\nGrundbegriffe, Backup vs. Synchronisation Mit der sogenannten TimeMachine hatte Apple es wieder einmal geschafft, einen Service der bei anderen wahnsinnig kompliziert einzurichten ist/war, kinderleicht bedienbar zu machen. Leider scheinen Datensicherungen in das lokale Netz aus der Mode gekommen zu sein, so propagiert Apple selbst nur noch das Backup direkt auf einen USB-Speicher oder eben in die eigene iCloud. Dabei haben beide Lösungen größere Nachteile. Für das lokale Backup muss ich immer daran denken, dass externe Speichermedium in den Rechner zu stecken und kommt es z.B. zu einem Kurzschluss während beide Geräte miteinander verbunden sind, sind auch beide zerstört. Dahin ist die Datensicherung, genau dann wenn man sie am meisten braucht. Die Alternativen iCloud und Co. begleiten ebenfalls mehrere Nachteile. Zum Einem vertrauen wir unsere Daten einem externen Anbieter an, der uns zwar versprechen kann dass diese dort sicher und geschützt aufbewahrt werden, aber garantieren kann uns das niemand. Außerdem handelt es sich bei iCloud und ähnlichen zunächst nur um Services zur Dateisynchronisation. D.h. meine Daten werden parallel lokal und eben in der Cloud abgelegt. Das heisst auch, wenn ich eine Datei lokal lösche, passiert das gleiche auch in der Cloud. Und wieder Adieu liebe Dateischerung. Dem entgegen wirk die Dateiversionierung die entweder Standardmäßig oder optional aktiviert werden kann, allerdings habe ich selbst die Erfahrung gemacht, dass diese auch genau dann wenn man sie bräuchte gerade mal nicht funktioniert.\nAus diesen und anderen Gründen hat Apple noch ein „Türchen“ offen gelassen und stellt es anderen Herstelleren z.B. von Netzwerkspeichern wie QNAP frei, TimeMachine in ihre Hardwarelsöungen einzubinden, auch wenn Apple selbst keine entsprechende Hardware mehr vertreibt.\nalls ihr aber so wie ich sowieso schon einen Linux Server oder RaspberryPi mit Docker Compose habt, geht es auch noch einfacher: Wir konfigurieren einfach einen Container der als TimeMachine Server im Heimnetz dient!\nServer vorbereiten Zunächst verbinden wir unseren externen Speicher, ich selbst verwende eine externe Festplatte, mit unserem Server. Falls nicht schon geschehen, formatieren wir unseren Speicher noch in einem für Linux nativen Dateisystem.\nFestplatte formatieren Um eine externe Festplatte in der Linux-Kommandozeile mit dem Dateisystem ext4 zu formatieren, kannst du die folgenden Schritte befolgen:\nSchließe die externe Festplatte an deinen Computer an. Stelle sicher, dass sie erkannt wird und einen zugewiesenen Gerätepfad hat. Du kannst dies mit dem Befehl lsblk überprüfen, der eine Liste der blockbasierten Geräte anzeigt. Öffne ein Terminal oder eine Konsole, um die Linux-Kommandozeile zu öffnen. Gib den Befehl sudo fdisk -l ein, um eine Liste der erkannten Festplatten und ihrer Partitionen anzuzeigen. Finde den Gerätepfad deiner externen Festplatte in der Liste. Normalerweise wird sie als „/dev/sdX“ bezeichnet, wobei „X“ für einen Buchstaben steht (z. B. /dev/sdb). Stelle sicher, dass du den richtigen Gerätepfad auswählst und die Daten auf der Festplatte sicher gesichert hast. Das Formatieren einer Festplatte löscht alle darauf befindlichen Daten unwiederbringlich. Gib den folgenden Befehl ein, um das Festplattenformat zu ändern und das Dateisystem ext4 zu erstellen: 1sudo mkfs.ext4 /dev/sdX Ersetze „/dev/sdX“ durch den tatsächlichen Gerätepfad deiner externen Festplatte. 6. Der Befehl wird dich fragen, ob du fortfahren möchtest, da er alle Daten auf der Festplatte löschen wird. Bestätige mit „y“ und drücke die Eingabetaste. 7. Der Formatierungsvorgang beginnt und kann je nach Größe der Festplatte einige Zeit in Anspruch nehmen. 8. Sobald der Vorgang abgeschlossen ist, erhältst du eine Bestätigungsmeldung.\nDeine externe Festplatte sollte nun erfolgreich mit dem ext4-Dateisystem formatiert sein und bereit für die Verwendung unter Linux sein.\nFestplatte/Speicher dauerhaft mounten Als nächstens wollen wir erreichen, dass der Speicher bzw. die Festplatte bei jedem Neustart des Systems automatisch „gemountet“ (an das System angehangen wird).\nUm eine mit ext4 formatierte Festplatte in der /etc/fstab-Datei zu mounten, kannst du die folgenden Schritte befolgen:\nÖffne ein Terminal oder eine Konsole, um die Linux-Kommandozeile zu öffnen. Gib den Befehl sudo blkid ein, um eine Liste der erkannten Festplatten und ihrer UUIDs anzuzeigen. Finde die UUID deiner ext4-formatierten Festplatte in der Liste. Die UUID sieht in etwa so aus: UUID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx. Erstelle einen Ordner, der als Mountpunkt für die Festplatte dienen soll. Du kannst dies mit dem Befehl sudo mkdir \u0026lt;mountpunkt\u0026gt; tun, wobei der Pfad zu dem gewünschten Ordner ist. Zum Beispiel: sudo mkdir /mnt/backup . Öffne die /etc/fstab Datei in einem Texteditor mit Root-Berechtigungen. Zum Beispiel: sudo nano /etc/fstab . Füge eine neue Zeile am Ende der /etc/fstab Datei hinzu, um die Festplatte zu mounten. Die Syntax lautet: UUID=\u0026lt;UUID\u0026gt; \u0026lt;mountpunkt\u0026gt; ext4 defaults 0 2 . Ersetze durch die UUID deiner Festplatte und mit dem Pfad zum zuvor erstellten Ordner, z.B. UUID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx /mnt/backup ext4 defaults 0 2. Speichere die Änderungen und schließe den Texteditor. Um die Festplatte sofort zu mounten, ohne den Computer neu zu starten, gib den Befehl sudo mount -a ein. Dadurch werden die Einträge in der /etc/fstab Datei gelesen und die entsprechenden Festplatten gemountet. Die Festplatte wird nun jedes Mal automatisch beim Systemstart gemountet, indem die Informationen in der /etc/fstab-Datei verwendet werden. Du kannst auf die Dateien und Ordner in der Festplatte über den angegebenen Mountpunkt zugreifen.\nDocker Container mit Compose einrichten Falls ihr Docker und Docker Compose noch nicht installiert habt, ist das vorgehen hierzu hier beschrieben. Wir nutzen das Image mbentley/timemachine vom Docker Hub um die Funktionalität der TimeMachine so nachzustellen, wie es auch die Anbieter von professionellen Backuplösungen tun.\nUm den Docker-Container „mbentley/timemachine:smb“ mit Docker Compose zu starten, erstelle bitte eine compose.yml-Datei mit den entsprechenden Konfigurationen. Hier ist ein Beispiel, das auf dem Image basiert und einen Dienst für den Container definiert:\n1version: \u0026#34;3.3\u0026#34; 2services: 3 timemachine: 4 image: mbentley/timemachine:smb 5 container_name: timemachine 6 hostname: timemachine 7 restart: unless-stopped 8 ports: 9 - 137:137/udp 10 - 138:138/udp 11 - 139:139 12 - 445:445 13 logging: 14 options: 15 max-size: \u0026#34;10m\u0026#34; 16 max-file: \u0026#34;3\u0026#34; 17 environment: 18 TZ: \u0026#39;Europe/Berlin\u0026#39; 19 CUSTOM_SMB_CONF: \u0026#34;false\u0026#34; 20 CUSTOM_USER: \u0026#34;false\u0026#34; 21 DEBUG_LEVEL: \u0026#34;1\u0026#34; 22 HIDE_SHARES: \u0026#34;no\u0026#34; 23 EXTERNAL_CONF: \u0026#34;\u0026#34; 24 MIMIC_MODEL: \u0026#34;TimeCapsule8,119\u0026#34; 25 TM_USERNAME: \u0026#34;timemachine\u0026#34; 26 TM_GROUPNAME: \u0026#34;timemachine\u0026#34; 27 TM_UID: \u0026#34;1000\u0026#34; 28 TM_GID: \u0026#34;1000\u0026#34; 29 PASSWORD: \u0026#34;DEINPASSWORT\u0026#34; 30 SET_PERMISSIONS: \u0026#34;false\u0026#34; 31 SHARE_NAME: \u0026#34;TimeMachine\u0026#34; 32 SMB_INHERIT_PERMISSIONS: \u0026#34;no\u0026#34; 33 SMB_NFS_ACES: \u0026#34;yes\u0026#34; 34 SMB_PORT: \u0026#34;445\u0026#34; 35 SMB_VFS_OBJECTS: \u0026#34;acl_xattr fruit streams_xattr\u0026#34; 36 VOLUME_SIZE_LIMIT: \u0026#34;1 T\u0026#34; 37 WORKGROUP: \u0026#34;WORKGROUP\u0026#34; 38 volumes: 39 - /mnt/backup/timemachine:/opt/timemachine Speichere die Datei als compose.yml. Dieses Beispiel verwendet den Port 445 für den SMB-Zugriff. Es bindet auch das Verzeichnis /mnt/backup/timemachine auf dem Host mit dem Verzeichnis /opt/timemachine im Container, so dass die Backups auf der externene Festplatte gespeichert werden.\nFühre dann den folgenden Befehl aus, um den Container zu starten: docker compose up -d. Dadurch wird der Container im Hintergrund gestartet. Die Option -d stellt sicher, dass der Dienst im Hintergrund (detached mode) läuft.\nJetzt läuft der Container „mbentley/timemachine:smb“ und ist über die konfigurierten Ports erreichbar. Du kannst dann eine Time Machine-Verbindung zu deinem Docker-Host mit dem entsprechenden SMB-Protokoll herstellen und das Verzeichnis /mnt/backup/timemachine verwenden, um deine Backups zu speichern.\nBackup auf dem Mac konfigurieren Nun haben wir alle vorbereitenden Schritte auf unserem Server abgeschlossen und widmen uns unserem zu sicherenden Mac. Ich habe die Erfahrung gemacht, dass sich ein Backup am leichtesten einrichten lässt, wenn man sich zuvor mit dem Netzlaufwerk auf dem Server verbindet. Um dich mit einem SMB-Laufwerk auf deinem Mac zu verbinden, befolge diese Schritte:\nÖffne den Finder auf deinem Mac. Klicke in der Menüleiste auf „Gehe zu“ und wähle „Mit Server verbinden…“ (oder verwende die Tastenkombination „Cmd + K“). Gib die Adresse des SMB-Laufwerks in das Eingabefeld ein. Die Adresse kann entweder die IP-Adresse des Servers oder sein Netzwerkname (falls verfügbar) sein. Das Format der Adresse lautet smb://\u0026lt;adresse\u0026gt;. Zum Beispiel: smb://192.168.0.100 oder smb://meinserver. Klicke auf „Verbinden“. Es wird ein Dialogfenster angezeigt, in dem du deine Anmeldeinformationen eingeben musst. Gib den Benutzernamen und das Passwort für den Zugriff auf das SMB-Laufwerk, so wie in der Docker Compose Datei konfiguriert, ein. Du hast auch die Möglichkeit, das Kästchen „Anmeldeinformationen im Schlüsselbund speichern“ anzukreuzen, um die Anmeldeinformationen für zukünftige Verbindungen zu speichern. Klicke auf „Verbinden“. Wenn die Anmeldeinformationen korrekt sind und der Zugriff gewährt wurde, wird das SMB-Laufwerk im Finder angezeigt. Du kannst nun auf die Dateien und Ordner des Laufwerks zugreifen und diese bearbeiten. Das SMB-Laufwerk wird nach der Verbindung im Abschnitt „Freigaben“ im Finder angezeigt. Du kannst auch ein Lesezeichen für das SMB-Laufwerk erstellen, indem du es zum Finder-Sidebar ziehst. Dadurch wird der Zugriff in Zukunft vereinfacht.\nWenn du die Verbindung trennen möchtest, kannst du das SMB-Laufwerk einfach aus der Seitenleiste des Finders ziehen oder mit der rechten Maustaste auf das Laufwerk klicken und „Verbindung trennen“ auswählen.\nBitte beachte, dass für eine erfolgreiche Verbindung das SMB-Protokoll auf dem SMB-Laufwerk aktiviert sein muss und die Netzwerkeinstellungen des Macs korrekt konfiguriert sein sollten.\nUm ein regelmäßiges Backup auf einen Netzwerkspeicher (Network Attached Storage, NAS) unter macOS einzurichten, kannst du Time Machine verwenden. Folge diesen Schritten:\nStelle sicher, dass dein Netzwerkspeicher ordnungsgemäß mit deinem Netzwerk verbunden ist und zugänglich ist. Gehe zu „Systemeinstellungen“ auf deinem Mac und klicke auf „Time Machine“. Klicke auf „Time Machine aktivieren“. Klicke auf „Weitere Optionen“. Wähle die Option „Backup-Datenträger auswählen“. Wähle im Finder den Netzwerkspeicher aus, den du für das Backup verwenden möchtest, und klicke auf „Auswählen“. Time Machine wird das Backup-Volume überprüfen und formatieren, falls erforderlich. Nachdem das Volume ausgewählt wurde, kehre zu den Time Machine-Einstellungen zurück. Aktiviere das Häkchen bei „Automatische Backups“. Du kannst auch den Zeitplan für die Backups anpassen, indem du das Häkchen bei „Automatische Backups“ deaktivierst und dann bestimmte Zeiten festlegst, zu denen die Backups stattfinden sollen. Du kannst außerdem die Einstellungen für „Ausgeschlossene Elemente“ anpassen, um bestimmte Ordner oder Dateien von den Backups auszuschließen. Sobald du die oben genannten Schritte abgeschlossen hast, wird Time Machine regelmäßig automatische Backups auf deinem Netzwerkspeicher durchführen. Beachte, dass die Geschwindigkeit der Backups von der Netzwerkverbindung abhängt und möglicherweise länger dauern kann als bei einem direkt angeschlossenen Speichergerät.\nEs ist auch wichtig sicherzustellen, dass der Netzwerkspeicher ordnungsgemäß konfiguriert ist und mit dem Mac kompatibel ist, um die bestmögliche Time Machine-Unterstützung zu gewährleisten. Konsultiere die Dokumentation deines NAS-Herstellers für weitere Informationen zur Einrichtung der Time Machine-Unterstützung.\n","tags":["deutsch","macos"],"section":"blog"},{"date":"1722816000","url":"/blog/matomo/","title":"Web Analytics selbstgemacht mit Matomo","summary":"Jeder, der sich mit Internetauftritten beschäftigt, hat sicherlich schon von Google Analytics (https://de.wikipedia.org/wiki/Google_Analytics) gehört. Damit kann man u.a. sehen von wo, unter welchen technischen Bedingungen, wann auf welche Seiten eben dieser zugegriffen wird. Matomo (https://matomo.org/) steht nicht für ein Mozarella-Sandwich mit Mayonaise und Tomaten, sondern ist ein Open Source Tool mit dem sich gleiches und teilweise sogar mehr erreichen lässt. In diesem Artikel gehe ich auf die Vorteile bei der Nutzung von Matomo ein und liefere auch gleich ein Beispiel, wie es sich schnell nutzen lässt.\n","content":"Jeder, der sich mit Internetauftritten beschäftigt, hat sicherlich schon von Google Analytics (https://de.wikipedia.org/wiki/Google_Analytics) gehört. Damit kann man u.a. sehen von wo, unter welchen technischen Bedingungen, wann auf welche Seiten eben dieser zugegriffen wird. Matomo (https://matomo.org/) steht nicht für ein Mozarella-Sandwich mit Mayonaise und Tomaten, sondern ist ein Open Source Tool mit dem sich gleiches und teilweise sogar mehr erreichen lässt. In diesem Artikel gehe ich auf die Vorteile bei der Nutzung von Matomo ein und liefere auch gleich ein Beispiel, wie es sich schnell nutzen lässt.\nDatenkrake – aber besucherfreundlich Der wesentliche Vorteil bei der Nutzung von Matomo besteht darin, dass man es selbst betreiben (https://hub.docker.com/_/matomo) kann, und man so die volle Kontrolle darüber hat welche Daten vom Besucher wo gespeichert werden. Ich selbst nutze das Tool nun schon seit vielen Jahren und habe nur positives darüber zu berichten. Es lässt sich ganz Datenschutzkonform (https://www.it-recht-kanzlei.de/matomo-richtig-verwenden-dsgvo.html) und trotzdem granular einstellen, welche Informationen gespeichert werden. Für aussagekräftige Analysen braucht man z.B. nicht die komplette IP-Addresse des Besuchers zu speichern, um nur eine der vielen Optionen zu nennen. Auch setzen Tools wie Google Analytics stark auf den Einsatz von digitalen Fingerabdrücken (Cookies), dazu gleich mehr. Aber so viel vorab: mit Matomo lässt sich der Einsatz von Cookies auf Wunsch sogar komplett deaktivieren!\nWie installiere und benutze ich den Matomo? Die Matomo Cloud – so verdient der Anbieter von Matomo, neben zusätzlichen erwerbsfähigen Plugins, sein Geld. Analog zu Google Analytics, vertraue ich die Daten meiner Benutzer einem Drittanbieter an. Irgendwie ja das, was wir genau nicht wollen.\nIntegriert als Plugin in ein Web Content Management System (WCMS) – Für WordPress und andere gibt es eine komplette Installation von Matomo als Plugin, welches sich über den jeweilig verfügbaren Markplatz hinzufügen lässt. Hier haben wir keine Möglichkeit der Trennung von Webseiten-Content und Analytics. Das ist deshalb auch nicht besonders performant und nur für kleinere Installationen geeignet. Außerdem geht das natürlich nur, wenn man eines der unterstützten WCMS einsetzt.\nSelbst Hosten – Das ist was ich hier beschreibe und mittels einem von Matomo selbst vorgefertigtem Container (https://hub.docker.com/_/matomo) auch ganz einfach geht.\nEine “saubere” Möglichkeit Matomo zu installieren ist also über Docker Compose Container. Im Appendix habe ich eine entsprechende Beispieldatei angehangen. Was ich hier abermals unterschlage ist die Konfiguration des Reverse-Proxies samt seiner Konfiguration. Wir gehen einfach davon aus, dass ich für meine Subdomain stats.handtrixxx.com mit dem Nginx Reverse Proxy Manager die benötigten Einstellungen vorgenommen habe und auch von dort aus ein entsprechendes Zertifikat von Let’s Encrypt bekommen habe.\nNachdem das alles eingestellt und hochgefahren ist (Dauer ca. 20 Minuten), können wir uns and der Weboberfläche von Matomo für die initiale Konfiguration anmelden. Wie das funktioniert, ist ausführlich und gut hier https://matomo.org/docs/installation/ beschrieben. Dank Docker Container können wir den ersten Teil der Anleitung komplett überspringen und setzten ungefähr an der Stelle “Super User” an.\nWie binde ich Matomo nun in meine Web-Seite/App ein? Wie, wenn wir auf https://matomo.org/docs/installation/ ein bisschen gescrollt haben, an gleicher Stelle beschrieben lässt sich nun der Einsatz auf einer ersten Website durchführen. Damit wird auch deutlich, dass mit einer einzelnen Matomo Installationen auch mehrere Websites/Apps getrennt voneinander oder zusammen anbindbar sind.\nZur Einbindung in eine Website lässt sich, wie im Standard vorgeschlagen, entweder ein JavaScript Snippet verwenden oder aber ein “unsichtbares” Bild über den sogenannten Image Tracker einfügen. Auf diese Art lässt sich nun auch einstellen, dass KEIN Cookie beim Anwender zum Tracking genutzt werden soll. Prima!\nWartung Abschließend ein paar Tipps zur Wartung unserer Installation. Wie ihr, falls ihr selbst eine Installation vorgenommen habt, evtl. schon gesehen habt wird die Verfügbarkeit eines Updates von Matomo direkt in der Web-GUI angezeigt und kann auch von dort aus durchgeführt werden.\nUm die Container selbst auch aktuell zu halten, reicht das gelegentliche und auch automatisierbare Ausführen von docker-compose pull , mit anschließendem docker-compose up -d aus dem Stammverzeichnis eurer Docker Compose Umgebung heraus.\nDas ist einfach.\nFazit Definitiv hat das Schreiben dieses Artikels länger gedauert als eine Matomo Installation. Außerdem konnte ich hoffentlich verdeutlichen, dass es im Vergleich zu anderen Web-Analytic Tools mit Matomo keine wesentlichen Einschränkungen, aber jede Menge Vorteile gibt. Ein kleiner Wermutstropfen ist vielleicht, dass sich die Art der Reports in der Web-GUI nicht besonders “verbiegen” lässt. Wahrscheinlich wäre das aber auch nur für größere Unternehmen überhaupt wünschenswert. Außerdem gibt es dafür, vielleicht als abschließender Lichtblick, eine großartige API die es uns erlaubt die über Matomo erfassten Daten in einer Applikation unserer Wahl zu verwenden! Siehe: https://developer.matomo.org/api-reference/reporting-api. Eigentlich war der Artikel nur als “kurze” Ergänzung zu einem Vorherigen (Mehr als nur Performance King – Headless CMS, APIs und IoT für die Website) gedacht, bei dem es u.a. um das Aufsetzen einer Website mit einem Headless CMS ging. Es ist aber nun doch “etwas” länger geworden. Ich bitte die vielen Fachbegriffe zu entschuldigen. Aufgrund des heterogenen Zielpublikums ist die Wortwahl nicht immer ganz einfach.\nAppendix Hier ein Beispiel einer compose.yml zur einfachen Inbetriebnahme von Matomo.\n1services: 2 stats-db: 3 image: mysql:5.7 4 logging: 5 options: 6 max-size: \u0026#34;10m\u0026#34; 7 max-file: \u0026#34;3\u0026#34; 8 container_name: stats-db 9 command: --max-allowed-packet=64MB 10 restart: always 11 networks: 12 - dmz 13 environment: 14 MYSQL_ROOT_PASSWORD: ADMINPASSWORD 15 MYSQL_DATABASE: DBNAME 16 MYSQL_USER: DBUSER 17 MYSQL_PASSWORD: DBUSERPW 18 volumes: 19 - ./db_data:/var/lib/mysql 20 stats-app: 21 depends_on: 22 - stats-db 23 image: matomo:latest 24 logging: 25 options: 26 max-size: \u0026#34;10m\u0026#34; 27 max-file: \u0026#34;3\u0026#34; 28 restart: always 29 networks: 30 - dmz 31 environment: 32 MATOMO_DATABASE_HOST: stats-db 33 MATOMO_DATABASE_ADAPTER: mysql 34 MATOMO_DATABASE_TABLES_PREFIX: matomo_ 35 MATOMO_DATABASE_USERNAME: DBUSER 36 MATOMO_DATABASE_PASSWORD: DBUSERPW 37 MATOMO_DATABASE_DBNAME: DBNAME 38 volumes: 39 - ./data:/var/www/html 40volumes: 41 db_data: {} 42networks: 43 dmz 44 external: true ","tags":["deutsch"],"section":"blog"},{"date":"1720051200","url":"/blog/mythos-motivation/","title":"Mythos Motivation ***","summary":"Du denkst ein Mitarbeiter muss nur „richtig motiviert“ werden um dauerhaft Höchstleistungen zu erbringen? Dann lies dieses Buch um zu verstehen, warum diese Vorstellung weit ab jeglicher Realität liegt. Reinhard Sprenger stellt in vielen Beispielen und Fällen mit Praxisbezug dar, warum motivierte Mitarbeiter nicht einfach gezüchtet werden können. Als das Buch 1991 erschien, muss es wohl dem ein oder anderen Manager ganz neue Erkenntnisse geliefert haben. Auch heute, oder vielleicht gerade heute, ist das Thema aktuell und der Mythos noch nicht ist der Welt geschafft. Einige der Beispiele passen nur nicht mehr so recht in die heutige Welt und insgesamt kam mir das Buch etwas zu langatmig vor.\n","content":"Du denkst ein Mitarbeiter muss nur „richtig motiviert“ werden um dauerhaft Höchstleistungen zu erbringen? Dann lies dieses Buch um zu verstehen, warum diese Vorstellung weit ab jeglicher Realität liegt. Reinhard Sprenger stellt in vielen Beispielen und Fällen mit Praxisbezug dar, warum motivierte Mitarbeiter nicht einfach gezüchtet werden können. Als das Buch 1991 erschien, muss es wohl dem ein oder anderen Manager ganz neue Erkenntnisse geliefert haben. Auch heute, oder vielleicht gerade heute, ist das Thema aktuell und der Mythos noch nicht ist der Welt geschafft. Einige der Beispiele passen nur nicht mehr so recht in die heutige Welt und insgesamt kam mir das Buch etwas zu langatmig vor.\nMeine Bewertung: 3 von 5 Sternen\nInhalt Das Sachbuch ist in drei Teile gegliedert. Es beginnt mit einem analytischen Teil, in dem die oft falsche Grundeinstellung von Führungskräften gegenüber ihren Mitarbeiter und deren Motivation und Beweggründen beschrieben wird. Z.B. der Grundverdacht, dass ein Mitarbeiter von sich aus gar nicht 100% seiner möglichen Leistung bringt, sondern da durch Incentives oder sonstige Instrumente immer noch „was geht“. Im zweiten Teil geht es darum, dass Verhalten sowohl von Führungskräften als auch Mitarbeitern zu beleuchten. So wird z.B. der Mitarbeiter der weiss, das es für „Mehrleistung“ regelmäßig Boni gibt, von sich aus dann wirklich immer erstmal nur soviel „leisten“ wie von ihm erwartet und tunlichst darauf achten jegliches Plus mit dem Boni zu verknüpfen. So wird aus dem Bonus der Normalzustand und die Motivation des Mitarbeiters in keinster Weise gesteigert. Der dritte Teil beschäftigt sich dann intensiv damit, wie man konstruktiv führt. Ein großer Abschnitt ist dem Thema Demotivation zu vermeiden gewidmet, da nach Meinung des Autors hier der größte Hebel eines Vorgesetzten liegt.\nWeitere Infos Genre: Sachbuch Erscheinungsjahr: 1991 Seitenzahl: 311 ISBN: 978-3-593-51485-7 Erhältlich in jeder Buchhandlung oder unter: https://www.medimops.de/sprenger-reinhard-k-mythos-motivation-wege-aus-einer-sackgasse-gebundene-ausgabe-M03593501562.html\n","tags":["deutsch","Bücher"],"section":"blog"},{"date":"1717372800","url":"/blog/a-scanner-darkly/","title":"Der dunkle Schirm ****","summary":"Wie bei vielen der Bücher von Philip K. Dick ist den meisten die spätere Verfilmung \u0026ldquo;A Scanner Darkly\u0026rdquo; eher ein Begriff. Aber auch Kenner des Films können Freude am Buch haben, da es viele Szenen enthält die so im Film nicht vorkommen und den Charakter der einzelnen Akteure tiefer verdeutlichen. Insgesamt handelt es sich um eine sehr düstere Geschichte in einer Welt in der der Unterschied zwischen legalen, illegalen Drogen und der Welt dazwischen zu einer bis auf die Wurzeln gespalteten Gesellschaft geführt hat. Es wird behauptet das Dick während des schreibens selbst in größeren Mengen mit Drogen \u0026ldquo;experimentiert\u0026rdquo; hat, auf jeden Fall kommt beim Lesen nie wirklich gute Laune auf, sondern eher Mitleid für die Protagonisten.\n","content":"Wie bei vielen der Bücher von Philip K. Dick ist den meisten die spätere Verfilmung \u0026ldquo;A Scanner Darkly\u0026rdquo; eher ein Begriff. Aber auch Kenner des Films können Freude am Buch haben, da es viele Szenen enthält die so im Film nicht vorkommen und den Charakter der einzelnen Akteure tiefer verdeutlichen. Insgesamt handelt es sich um eine sehr düstere Geschichte in einer Welt in der der Unterschied zwischen legalen, illegalen Drogen und der Welt dazwischen zu einer bis auf die Wurzeln gespalteten Gesellschaft geführt hat. Es wird behauptet das Dick während des schreibens selbst in größeren Mengen mit Drogen \u0026ldquo;experimentiert\u0026rdquo; hat, auf jeden Fall kommt beim Lesen nie wirklich gute Laune auf, sondern eher Mitleid für die Protagonisten.\nMeine Bewertung: 4 von 5 Sternen\nInhalt Bob Arctor ist verdeckter Ermittler auf der Suche nach Drogenhändlern die die neue Droge Substanz T in den Umlauf bringen. Die Geschichte selbst spielt in den U.S.A., größtenteils in einer durch süchtige verkommenen Gegend. Auch Arctor selbst wird Opfer der Droge und kann immer weniger zwischen seiner imaginären und der realen Identität unterscheiden. Dabei wird er bewusst von höheren Instanzen seiner Behörde missbraucht, um an die eigentliche Organisation hinter dem Drogenhandel heranzukommen. Zusätzlich handelt das Buch vom sozialen Zwischenleben Arctors und seinen \u0026ldquo;Freunden\u0026rdquo;, welches immer mehr, ebenfalls durch die Droge verursacht, in einer verhängnisvollen Interaktion mündet.\nWeitere Infos Originaltitel: A Scanner Darkly Genre: Science-Fiction Erscheinungsjahr: 1977 Seitenzahl: 376 ISBN: 3-453-87368-8 Erhältlich in jeder Buchhandlung oder unter https://www.medimops.de/dick-philip-k-der-dunkle-schirm-taschenbuch-M03453873688.html\n","tags":["deutsch","Bücher"],"section":"blog"},{"date":"1715472000","url":"/blog/vscode-web/","title":"Visual Studio Code im Webinterface mit Docker und Caddy","summary":"Microsofts Visual Studio Code hat sich über die letzten Jahre als Quasi-Standard, nicht nur für das editieren von Quellcode in allen möglichen (Programmier-)Sprachen, sondern auch für eine Vielzahl weiterer Operationen etabliert.\nEiner der Kritikpunkte bleibt die durch den Editor verursachte realtiv hohe Systemlast, die zwar begründet ist aber für einen „Texteditor“ doch etwas merkwüdig erschenint. Ein anderer Kritikpunkt ist, dass für das beliebte „Remote SSH“ Plugin, welches es ermöglicht auf einen beliebigen über SSH erreichbaren Server mit VS Code zu arbeiten, ziemlich viele dynamische Ports geöffnet werden und auch generell einige Firewalls hier Probleme machen können.\n","content":"Microsofts Visual Studio Code hat sich über die letzten Jahre als Quasi-Standard, nicht nur für das editieren von Quellcode in allen möglichen (Programmier-)Sprachen, sondern auch für eine Vielzahl weiterer Operationen etabliert.\nEiner der Kritikpunkte bleibt die durch den Editor verursachte realtiv hohe Systemlast, die zwar begründet ist aber für einen „Texteditor“ doch etwas merkwüdig erschenint. Ein anderer Kritikpunkt ist, dass für das beliebte „Remote SSH“ Plugin, welches es ermöglicht auf einen beliebigen über SSH erreichbaren Server mit VS Code zu arbeiten, ziemlich viele dynamische Ports geöffnet werden und auch generell einige Firewalls hier Probleme machen können.\nNun basiert VS Code selbst auf dem Electron Framework, welches wiederum auf JavaScript aufbaut. Warum also nicht die VS Code Installation auf einem Server hosten und per Webinterface zur Verfügung stellen? Das haben sich offensichtlich viele gefragt und deshalb ist dies mit Hilfe von Docker und Caddy schnell bewerkstelligt.\nInstallation Zur Installation verlassen wir uns wieder auf Docker Compose für die Verwaltung unserer Containers und Caddy als Reverse Proxy. Ein passendes und regelmäßig aktualisiertes VS Code Image finden wir von Linuxserver.io auf Docker Hub.\nUnsere „compose.yml“ Datai kann dann so aussehen:\n1--- 2version: \u0026#34;2.1\u0026#34; 3services: 4 code-server: 5 image: lscr.io/linuxserver/code-server:latest 6 container_name: code-server 7 environment: 8 - PUID=1001 9 - PGID=1001 10 - TZ=Etc/UTC 11 - PASSWORD=PASSWORT 12 #- HASHED_PASSWORD=#optional 13 #- SUDO_PASSWORD=#optional 14 #- SUDO_PASSWORD_HASH=#optional 15 #- PROXY_DOMAIN=#optional 16 - DEFAULT_WORKSPACE=ORDNERCONTAINER #optional 17 volumes: 18 - ./config:/config 19 - /ORDNERHOST:/ORDNDERCONTAINER 20 restart: unless-stopped 21 logging: 22 options: 23 max-size: \u0026#34;10m\u0026#34; 24 max-file: \u0026#34;3\u0026#34; 25 networks: 26 - caddy 27volumes: 28 data: 29 db: 30networks: 31 caddy: 32 external: true Die wichtigsten Parameter hier kurz erläutert:\nenvironment – PUID: Die Prozess ID die möglichst der ID eures Benutzers entsprechen sollte, damit ihr später Berechtigungsproblemen beim editieren von Dateien aus dem Wege gehen könnt. Eure eigene ID bekommt ihr einfach über die Linux Kommandozeile mit dem Befehl „id“ heraus. environment – PGID: Das Gleiche wie für die PUID, nur das hier die Gruppen ID gemeint ist. environment – PASSWORD: Das Passwort was abgefragt wird, wenn man die URL des containers aufruft. Da nichtmal ein User gesetzt wird, ist das nicht besonders sicher und es sollte eine zusätzliche Sicherheitsstufe z.B. über den Reverse Proxy geschaffen werden. Siehe Kapitel „Härtung“. environment – SUDO_PASSWORD: Hier lässt sich das sudo password für den Container setzen. Nach möglichkeit sollte dies vermieden werden, da man in einem Container nach Security Best Practices nie als root arbeiten sollte. environment – HASHED_PASSWORD und SUDO_PASSWORD_HASH: Beide Parameter sollen dabei helfen, dass keine Passwörter im Klartext in der „docker-compose.yml“ abgelegt werden müssen. Wie man solch einen Hash Wert generiert ist auf der Projektseite erläutert, hat aber zumindest für mich so nicht funktioniert. Auch deshalb ist eine zusätzliche „Härtung“ wie nachfolgend beschrieben erforderlich. environment – DEFAULT_WORSPACE: Hier geben wir den Pfad auf dem Container an, der nachfolgend unter Volumes spezifiert ist. Diesen sehen wir dann als Workspace/Ordner in VS Code. volumes – /ORDNERHOST:/ORDNDERCONTAINER: Wenn gewünscht können wir hier einen Pfad auf unserem Host(Server) spezifizieren der in VS Code auftaucht. Die Variable ORDNERCONTAINER ersetzen wir wiederum durch den Wert den wir als „DEFAULT_WORKSPACE“ angegeben haben. Nun können wir den Container mit „docker compose up -d“ starten.\nHärtung und Reverse Proxy Konfiguration In unserem Caddyfile unseres Rerverse Proxy ergänzen wir einen Block für die neue VS Code Instanz. Das kann so aussehen:\n1code.handtrixxx.com { 2 basicauth { 3 USER HASHWERT 4 } 5 reverse_proxy code-server:8443 6} Man beachte die Werte in den geschweiften Klammern hinter „basicauth“. Diese bringen zusätzlichem Schutz vor möglichen Angreifern, in dem sie ein weitere Anmeldung bereitstellen, sobald jemand die URL öffnet. Als USER könnten ihr einen Namen euerer Wahl festlegen während man für die Generierung des Hashwerts auf der Kommandoziele in den Docker Compose Ordener von Caddy wechselt und dort folgendes Kommando ausführt: docker compose exec -w /etc/caddy caddy caddy hash-password\nDen aus der Abfrage resultierenden Wert kopiert ihr einfach in das Caddyfile als HASHWERT.\nNachdem wir die Konfiguration von Caddy neu geladen haben, ist VS Code unter der angegebenen URL erreichbar und die eine zusätzliche Sicherheitsschicht aktiviert.\n","tags":["deutsch"],"section":"blog"},{"date":"1714262400","url":"/blog/chatgpt-clone/","title":"Setup your own ChatGPT clone in 5 minutes","summary":"And another post about A.I… And why this headline? OpenAI’s ChatGPT, Microsoft’s Co-Pilot and Salesforce’s Einstein are just running well! But, are they? And even if you are happy about them, are you willing to pay regular (not cheap) license fees for all of your employees, even if they just use it from time to time? Also, do you really trust Big Tech’s promises about confidentiallity, when it’s about your intellectual property? If you answer all of this by yes, you can stop reading, now. But, in case you would like to know how you easily can run your own A.I. chatbot or you are just curious like me, about how that is done, my article gives you an overview how to do that by utilizing the two amazing tools Ollama and Open WebUI.\n","content":"And another post about A.I… And why this headline? OpenAI’s ChatGPT, Microsoft’s Co-Pilot and Salesforce’s Einstein are just running well! But, are they? And even if you are happy about them, are you willing to pay regular (not cheap) license fees for all of your employees, even if they just use it from time to time? Also, do you really trust Big Tech’s promises about confidentiallity, when it’s about your intellectual property? If you answer all of this by yes, you can stop reading, now. But, in case you would like to know how you easily can run your own A.I. chatbot or you are just curious like me, about how that is done, my article gives you an overview how to do that by utilizing the two amazing tools Ollama and Open WebUI.\nPreparation and Requirements Theoretically you could even run everything on a laptop, but for sure you would face several issues at least after a while. Better is to have a server with installed docker up and running. Also a reverse proxy, and an URL would make things more smooth, but are not mandatory. As we see later in chapter performance, it would be beneficial if your server has a dedicated graphics card, but also that is not a must.\nInstallation and Configuration The guys from the „Open WebUI“ project made it extremely easy to get your chatbot running. Basically you just create a new docker-compose.yml file like the one in the example below and start the thing as usual by command „docker compose up -d“. That’s it, no joke!\n1services: 2 chat: 3 container_name: chat 4 image: ghcr.io/open-webui/open-webui:ollama 5 volumes: 6 - ./ollama:/root/.ollama 7 - ./open-webui:/app/backend/data 8 restart: unless-stopped 9 #ports: 10 # - 8080:8080 11 networks: 12 caddy: 13networks: 14 caddy: 15 external: true As you can see in my example file I customized the network configuration, and also configured my reverse proxy caddy to point access to chat.handtrixxx.com to my new container. As you can see in the following screenshot you can click on „Sign up“ to create a new user account for yourself as Administrator.\nNow, after you logged in, there are just two steps more to do to start your A.I. chats. At first you should go to the admin panel and then at the „Admin settings“ to disable registration for other users to avoid other users just create an account on your instance. Then in the settings at the models tab you will have to download one ore more language models. There are plenty to choose from. An overview is available at: https://ollama.com/library . You are done and as you see it does not have to take more than 5 minutes in case you are a bit experienced in docker and setting up tools in general.\nCosts Since everything I introduced and described is based on Open Source software, there are no costs or licensing fees at all. Great, isn’t it? But to say it is completly free is also not completly true, since you have to cover the charges for the server if you do not „have“ one anyway 🙂.\nPerformance As mentioned before, a dedicated graphics card would speed up the response times of the chatbot trendemously. By running it only on CPU, like i did in my example, every generation of a response took all the CPU power i have (and I have a lot) for some seconds. So the whole thing feels a bit like the early versions of ChatGPT. That’s no drama, but definitly noticeable.\nConclusion As conclusion i let the openchat language model answer itself to my prompt:\n","tags":["english"],"section":"blog"},{"date":"1712361600","url":"/blog/ai-on-website/","title":"My website charged with Aritificial Intelligence","summary":"After years of tinkering with various CMS systems and even creating one of my own, one thing remains constant: the struggle to craft captivating content! ⏳ But fear not, because I’m tapping into the newest technology – AI! ? Check out my latest post on how artificial intelligence maybe can revolutionize content creation, translations, and even programming. Dive into the cutting-edge with me at niklas-stephan.de ! #AI #ContentCreation #Innovation“\nUser Experience ","content":"After years of tinkering with various CMS systems and even creating one of my own, one thing remains constant: the struggle to craft captivating content! ⏳ But fear not, because I’m tapping into the newest technology – AI! ? Check out my latest post on how artificial intelligence maybe can revolutionize content creation, translations, and even programming. Dive into the cutting-edge with me at niklas-stephan.de ! #AI #ContentCreation #Innovation“\nUser Experience Curious about the look and feel of your website? Wondering what to showcase? Well, here’s a peek behind the scenes! I delved into the world of ‚classic‘ ChatGPT and other freely available assistants to uncover some best practices. But let’s be real: relying solely on standard templates can stifle creativity. That’s why I leaned on my professional experience and years of accumulated expertise. Sure, there’s the Microsoft Brand Kit Generator, but let’s face it: resorting to it might signal a creative rut, as evidenced by the rather lackluster brand card it churns out. Plus, Microsoft’s Terms of Use put the brakes on any commercial use – talk about a close call! ?\nWhat works much better is to analyze the output of Google Chromes Lighthouse report to figure out weakness of your website and then ask an A.I. assistant like ChatGPT or Github CoPilot how to solve them. More about this at chapter „Development and Programming“.\nContent Creation for Media Assets Attractive and professionally crafted photos, images, and graphics can often come with a hefty price tag, yet they are what truly bring texts to life. To create media such as teasers, profile, and post images, I frequently turn to AI. For instance, using apps like PicsArt, I was able to generate a set of profile pictures for myself for just around €6. ? After feeding the system with roughly 20 of my images, I received a collage of 50 AI-generated unique images within 30 minutes. Among these 50 images, I find about a third to be exceptionally well-done, definitely worth the minimal investment. Additionally, I utilize Adobe Firefly and Microsoft’s Image Creator, especially for creating cover images for individual posts. While the quality varies, with Adobe generally surpassing Microsoft, the time saved is always substantial, and there are no additional costs. Impressive!?\nContent Creation for Text Imagine you have a brilliant idea in mind but struggle to articulate it into compelling written content for you website. This is where AI-powered tools like ChatGPT and Copilot step in to revolutionize your writing process. Simply provide them with a prompt or topic, and watch as they seamlessly generate coherent and engaging text tailored to your needs.\nChatGPT and Microsoft’s CoPilot for instance, leverages its vast knowledge base to understand your prompt and craft responses that mimic human conversation.\nTranslations Translating content can be just as time-consuming as creating it from scratch. That’s why I’ve incorporated the powerful Libre Translate API, which leverages AI language models to offer translations for words, sentences, or entire paragraphs across a wide range of languages. By integrating this API, I’ve streamlined the translation process, eliminating the need for manual copying and pasting, and significantly accelerating productivity. However, it’s important to note that while the API facilitates efficiency, the quality of translations may vary and may not always meet optimal standards.\nDevelopment and Programming In programming, AI has become an indispensable tool for me. Gone are the days of searching for solutions to each problem individually through Google, then sifting through search results to find the right approach. Now, even just using the free version of ChatGPT significantly boosts efficiency. Additionally, I’ve subscribed to Github CoPilot, seamlessly integrated into VS Code. CoPilot not only answers my code-related questions in a chat but also provides code suggestions directly in the editor when it anticipates my intent. It’s truly remarkable! Moreover, error analysis and correction are now much faster and more effective. However, it’s important to note that while Co-Pilot enhances productivity, it cannot replace human expertise, and occasionally, it may suggest solutions that are less than ideal.\nConclusion In conclusion, our exploration into the integration of AI in content creation and development unveils a landscape ripe with innovation and efficiency. From redefining the process of crafting captivating content to optimizing user experiences and streamlining development workflows, AI emerges as a transformative force.\nBy leveraging tools such as ChatGPT, Copilot, and Libre Translate API, we’ve unlocked new dimensions of creativity and productivity. Whether it’s generating visually stunning media assets, crafting engaging text, or accelerating programming tasks, AI proves to be an indispensable ally.\nHowever, amidst the strides in efficiency, it’s crucial to remain vigilant about maintaining quality standards, particularly in translations and code accuracy. While AI enhances processes, human expertise remains irreplaceable.\nIn essence, our journey through the realms of AI-driven content creation and development underscores the potential for innovation and advancement.\n","tags":["english"],"section":"blog"},{"date":"1710201600","url":"/blog/caddy-docker/","title":"Caddy als Reverse Proxy für Docker Compose Container","summary":"Heute beschreibe ich was ein Reverse Proxy ist und warum er in deiner Docker Landschaft nicht fehlen darf. Nachdem ich mehrere Jahre lang sowohl Traefik, als auch den Nginx Proxy Manager als Reverse Proxy für meine Docker Container genutzt habe, bin ich nun bei Caddy gelandet. Am Nginx Proxy Manager störte mich, dass zur Konfiguration ausschließlich die Web UI zur Verfügung steht. Das hat mich in Backup/Restore Szenarien öfter an die Grenzen gebracht, was zum Schluss dazu führte jedes Mal aufs neue eine Klickorgie zu veranstalten. An Traefik störte mich wiederum die aufgeblähte Konfiguration, sowie die vielen Labels die an jedem zu berücksichtigenden Container ergänzt werden müssen. Caddy bietet mit einer CLI und einer API, die gewünschte Flexibilität und lässt sich auch einfach sichern/wiederherstellen.\n","content":"Heute beschreibe ich was ein Reverse Proxy ist und warum er in deiner Docker Landschaft nicht fehlen darf. Nachdem ich mehrere Jahre lang sowohl Traefik, als auch den Nginx Proxy Manager als Reverse Proxy für meine Docker Container genutzt habe, bin ich nun bei Caddy gelandet. Am Nginx Proxy Manager störte mich, dass zur Konfiguration ausschließlich die Web UI zur Verfügung steht. Das hat mich in Backup/Restore Szenarien öfter an die Grenzen gebracht, was zum Schluss dazu führte jedes Mal aufs neue eine Klickorgie zu veranstalten. An Traefik störte mich wiederum die aufgeblähte Konfiguration, sowie die vielen Labels die an jedem zu berücksichtigenden Container ergänzt werden müssen. Caddy bietet mit einer CLI und einer API, die gewünschte Flexibilität und lässt sich auch einfach sichern/wiederherstellen.\nWarum ein Reverse Proxy für Docker Container? Die Nutzung eines sogenannten Reverse Proxies für die eigene (Docker) Container Landschaft soll zum einen Flexibilität als zum anderen auch erhöhte Sicherheit bieten. Das (virtuelle) Netzwerk der Container wird in verschiedene Zonen aufgeteilt, so dass z.B. Datenbankinstanzen nicht direkt aus dem Internet erreichbar sind. Auch können so z.B. mehrere WordPress Installationen parallel betrieben werden, ohne dass sie sich irgendwie in die „Quere“ kommen. Beides wird in folgender Darstellung illustriert:\nAußerdem soll der Zugriff auf die selbst gehosteten Webseiten auschließlich über das verschlüsselte HTTPS Protokoll funktionieren. Die dafür benötigten Zertifikate verwaltet und aktualisiert eine moderne Rerverse Proxy Lösung für uns voll automatisch.\nWie installiere und betreibe ich Caddy als Reverse Proxy? Zunächst richten wir und ein dediziertes Netzwerk für Caddy und die Container die öffentlich erreichbar sein sollen unter einem gewünschten Namen (hier „Caddy“) mit folgendem Befehl ein: docker network create caddy . Diesen Namen siehst nur du in deinen Konfigurationsdateien und in Docker.\nCaddy selbst stellen wir ebenfalls über einen Docker Container zur Verfügung, auf dem wir die Ports 80 und 443 exponieren. Eine mögliche Docker Compose Datei „compose.yml“ dazu kann so aussehen:\n1services: 2 caddy: 3 container_name: caddy 4 image: caddy:latest 5 restart: unless-stopped 6 ports: 7 - \u0026#34;80:80\u0026#34; 8 - \u0026#34;443:443\u0026#34; 9 - \u0026#34;443:443/udp\u0026#34; 10 volumes: 11 - ./Caddyfile:/etc/caddy/Caddyfile 12 - ./site:/srv 13 - ./caddy_data:/data 14 - ./caddy_config:/config 15 logging: 16 options: 17 max-size: \u0026#34;10m\u0026#34; 18 max-file: \u0026#34;3\u0026#34; 19 networks: 20 - caddy 21volumes: 22 caddy_data: 23 caddy_config: 24networks: 25 caddy: 26 external: true Bevor wir den Container starten, legen wir im gleichen Verzeichnis eine Datei „Caddyfile“ an. Diese musst du auf deine Bedürfnisse anpassen und orientiert sich hier am oben gezeigten Beispiel:\n1{ 2 # TLS Options 3 email deine.email@provider.com 4} 5 6(wordpress) { 7 header { 8 Cache-Control \u0026#34;public, max-age=3600, must-revalidate\u0026#34; 9 } 10} 11 12quickscot.de, www.quickscot.de { 13 import wordpress 14 reverse_proxy wordpresscontainer1:80 15} 16 17niklas-stephan.de, www.niklas-stephan.de { 18 import wordpress 19 reverse_proxy wordpresscontainer3:80 20} Nun können wir den Container mit folgendem Befehl starten: docker compose up -d. Falls irgendetwas nicht funktioniert hilft eine Überprüfung der Logdatei mit: docker compose logs .\nBei Problemen hilft auch ein Blick in die Caddy Community, die viele Problembehandlungen und Konfigurationsbeispiele bereitstellt: https://caddy.community/.\nFalls ihr Änderungen/Ergänzungen an dem Caddyfile vornehmt müsst ihr nicht jedes Mal den kompletten Container durchstarten und könnt so Dowtimes durch folgenden Befehl, der die Konfiguration live neu läd, aktualisieren:\n1docker compose exec -w /etc/caddy caddy caddy reload Nun gilt es noch unsere Applikationen im Caddy Netzwerk sichtbar zu machen. Hier ein Beispiel einer WordPress Installation in der der Applikationsserver von Caddy aus erreicht werden kann, aber nicht der Datenbankserver:\n1services: 2 wordpresscontainer1: 3 container_name: wordpresscontainer1 4 depends_on: 5 - wordpresscontainer1-db 6 image: wordpress:latest 7 logging: 8 options: 9 max-size: \u0026#34;10m\u0026#34; 10 max-file: \u0026#34;3\u0026#34; 11 volumes: 12 - ./data:/var/www/html 13 - /etc/localtime:/etc/localtime:ro 14 restart: unless-stopped 15 environment: 16 WORDPRESS_DB_HOST: wordpresscontainer1-db:3306 17 WORDPRESS_DB_USER: EUERUSER 18 WORDPRESS_DB_PASSWORD: EUERSUPERPASSWORT 19 networks: 20 - default 21 - caddy 22 wordpresscontainer1-db: 23 container_name: wordpresscontainer1-db 24 image: mysql:5.7 25 logging: 26 options: 27 max-size: \u0026#34;10m\u0026#34; 28 max-file: \u0026#34;3\u0026#34; 29 volumes: 30 - ./db:/var/lib/mysql 31 restart: unless-stopped 32 environment: 33 MYSQL_ROOT_PASSWORD: ROOTSUPERPASSWORT 34 MYSQL_DATABASE: wordpress 35 MYSQL_USER: EUERUSER 36 MYSQL_PASSWORD: EUERSUPERPASSWORT 37 networks: 38 - default 39volumes: 40 data: 41 db: 42networks: 43 caddy: 44 external: true Fazit Bis jetzt ist Caddy genau der Kompromiss nach dem ich suchte. Die Konfiguration über eine einzelne Datei lässt sich super einfach sichern und bei Bedarf wiederherstellen. Bei Wunsch nach mehr, stellt Caddy alternativ auch die Konfiguration über eine API als Option bereit. Ein Blick in die technische Dokumentation unter https://caddyserver.com/docs/ offenbart, dass Caddy auch noch viel mehr kann als das gezeigte. Noch nicht herausgefunden habe ich, ob ich ähnlich wie bei Traefik auch zusätzliche Module/Plugins wie Crowdsec, für eine erweiterte Sicherheit aktivieren kann. Generell würde ich nie wieder zurück zum Nginx Proxy Manager wechseln, halte aber Traefik bei komplizierteren Szenarien, evtl. für die bessere Wahl. Für den Hobby Fullstack Entwickler wie mich, ist Caddy aber erstmal eine beinah rundum glücklich Lösung.\nUpdate 12.03.2025 Ich nutze immernoch Caddy und bin nach wie vor absolut damit zufrieden. Ein wechsel z.B. zu Traefik ist auch nach vielen Monaten nicht nötig.\n","tags":["deutsch"],"section":"blog"},{"date":"1707350400","url":"/blog/ssh-copy-id/","title":"Passwordless SSH Login to a Remote Server with Visual Studio Code","summary":"Following article describes how to enable You to login to a Remote Server with the industry standard SSH network protocol by using a key and Visual Studio Code, so you do not have to enter your user/password every time you want to connect. It was written for/on MacOS, but the procedure should be the same on Linux and similar for Windows.\nPreparation At first download, install and open VS Code.\n","content":"Following article describes how to enable You to login to a Remote Server with the industry standard SSH network protocol by using a key and Visual Studio Code, so you do not have to enter your user/password every time you want to connect. It was written for/on MacOS, but the procedure should be the same on Linux and similar for Windows.\nPreparation At first download, install and open VS Code.\nThen install the „Remote – SSH“ Extension by changing to the Extensions tab, searching for it and click on „Install“.\nFun with Keys At next open a terminal session by option „New Terminal“.\nAt the terminal prompt (normally shown at the bottom of your window) connect to your server by entering following line and pressing enter key.\n1ssh -p PORT USERNAME@SERVER So for example your line could be like „ssh -p 22 heinz@mydomain.com“. Then accept/confirm any dialogues you are prompted to for confirmation as well es entering the password.\nNow, just leave the ssh session by entering following command to the terminal:\n1exit At next you create a key for ssh (in case you do not already have one) on your local PC. Be sure you really exited/disconnected from the server before:\n1ssh-keygen Once done, you can copy the public key part to the server by utilizing following command:\n1ssh-copy-id -p PORT USERNAME@SERVER Next time if you login to the server with ssh you will not be asked for the password anymore but the key will be automatically verified in the background.\nSetting up the environment So let’s try that by login at the server (ssh -p PORT USERNAME@SERVER) in the terminal again and by creating a workspace directory for the files you want to edit/control with VS code later on.\n1mkdir DIRECTORYNAME The directory name can be anything you prefer, so e.g. „mkdir data“ could be suitable. Jump into this directory by sending command:\n1cd DIRECTORYNAME e.g. „cd data“. Then print the full directory path to the terminal screen by:\n1pwd The result will be something like „/home/heinz/data“. Copy this path to your clipboard or note/remember it. You are more then 50% done, now.\nConfigure VS Code Open the Remote SSH tab in VS Code and click on „+“ at SSH in the Remote Explorer.\nEnter the same command you used before to connect to your server at the new prompt shown on top of the window:\n1ssh -p PORT USERNAME@SERVER Choose the local ssh config file to update, normally the first proposal is fine.\nNow, in the Remote Explorer you should see the new option for connection, like:\nIf it’s not directly shown, just click the refresh button as shown on top right of the screenshot. Then click on the „-\u0026gt;“ arrow to connect.\nFinally, switch to the File Explorer Tab and click on „Open Folder“. At the prompt you enter or select the directory you remembered or copied before and click on „OK“:\nIn case you are asked if you trust the authors of the directory it makes sense to choose yes, since you are the author 🙂\nWe are done. You most likely see an empty folder structure since we just created the folder and you can create new files of any kind, like the „README.MD“ on the next screenshot, and folders as you wish.\nIt’s also possible to copy files from your PC to the server by drag-and-drop as well as downloading files/folders by a right-click on it at the VS Code File Explorer.\nResult Next time you open VS Code the connection will be established automatically. In case you switch between different environments you can always go back to the Remote Explorer tab of VS Code and connect to the server. Be sure to select the folder for connection and not the server itself, otherwise you will be prompted to select a folder at the File Explorer Tab again.\nI hope that short tutorial helped you out a bit 🙂\n","tags":["english"],"section":"blog"},{"date":"1704931200","url":"/blog/next-js-18/","title":"Learn Next.js - Chapter 18: Docker Environment","summary":"If you went successfully through Vercel\u0026rsquo;s free and amazing Learn Next.js online course, you have end up in a nice and \u0026ldquo;fast\u0026rdquo; (the database access is actually slow, but that was on purpose for better understanding of the tutorial) Demo of a modern web application. You have also learned about plenty considerations when developing an web app. So now, you maybe feel like \u0026ldquo;How can I use all of this for a real project?\u0026rdquo;. Also there are still some constraints left like; \u0026ldquo;How can I get independent from Vercel\u0026rsquo;s nice but (in their free plan) slow hosting offer?\u0026rdquo;.\n","content":"If you went successfully through Vercel\u0026rsquo;s free and amazing Learn Next.js online course, you have end up in a nice and \u0026ldquo;fast\u0026rdquo; (the database access is actually slow, but that was on purpose for better understanding of the tutorial) Demo of a modern web application. You have also learned about plenty considerations when developing an web app. So now, you maybe feel like \u0026ldquo;How can I use all of this for a real project?\u0026rdquo;. Also there are still some constraints left like; \u0026ldquo;How can I get independent from Vercel\u0026rsquo;s nice but (in their free plan) slow hosting offer?\u0026rdquo;.\nMy tutorial here is supposed to be an extension to the course but you can also use it, since I share my code on Github, without going through Vercel\u0026rsquo;s course. At first we will \u0026ldquo;dockerize\u0026rdquo; our development area, then host our own database to finally be able to push all of that to a webserver of our choice.\nStarting Point At first let\u0026rsquo;s have a look how our setup looks like after we finished chapter 17 of the tutorial:\nYou have a local directory \u0026ldquo;nextjs-dashboard\u0026rdquo; somewhere on your local machine, where you can edit your code with your favorite IDE (I would recommend VS Code) and look at the result localy on your browser, because you also installed node.js on your client. Additionally, just like a professional, you commit your code to an GIT instance which is Microsoft\u0026rsquo;s popular GitHub in our case. Beside that you learned how to setup and configure Vercel\u0026rsquo;s offer of a database instance as well as a public webserver.\nThat already reveals some flaws and pain (at least in my stomach):\nYou had to install node.js on your client and working with files directly part of your local file system node.js compiles everything depending on your clients architecture, but your production server most likely (will) run(s) on Linux. You have to rely on Vercel\u0026rsquo;s deployment and recompiling for production, which is not 100% transparent (at least to me). The database is reachable from the public internet (even if secured a bit) You literally use the same database for development and production purposes, which totally is not how it should be in a real-life example. Summarized: The setup is not like it would be, if you are a (semi-)professional developer.\nThat was totally on purpose to keep the \u0026ldquo;Learn Next.js\u0026rdquo; tutorial as simple as possible, but will be changed by us, now.\nDocker Containers?! While in former articles I already wrote about the benefits of the Docker container infrastructure from the view of an administrator, it also brings big advantages for software developers. Even or especially large software companies rely on container architecture for software development, they just do not use Docker but Kubernetes which was designed by Google for running in big datacenters. Nevertheless, Docker provides most of the functionalities similar to Kubernetes and is absolutly fine for private developers as well as small(er) companies.\nInstall Docker on your PC To install docker you can follow the well described tutorials on their website: https://www.docker.com/get-started/ . I recommend you to install the Docker Desktop App in case you are using Windows or Mac OS. In case you are using Linux i find it easier and much more leightweigth to install Docker via commandline, the same as you would do on a server. Once you have successfully installed Docker and the engine is running, it is time for the next steps.\nDockerize your Deveopment Area In case you want to skip the Learn Next.js online course or you do not have the resources of the result available, you can clone my Repository by git clone https://github.com/handtrixx/nextjs-dashboard.git or by the GitHub Desktop App.\nThen in your project\u0026rsquo;s root directory you create/enter folder \u0026ldquo;docker\u0026rdquo;. In folder docker we create/enter a subfolder called \u0026ldquo;dev\u0026rdquo;.\nInside the folder \u0026ldquo;dev\u0026rdquo; you create file \u0026ldquo;.env\u0026rdquo;:\n1POSTGRES_URL=\u0026#34;xxx\u0026#34; 2POSTGRES_PRISMA_URL=\u0026#34;xxx\u0026#34; 3POSTGRES_URL_NON_POOLING=\u0026#34;xxx\u0026#34; 4POSTGRES_USER=\u0026#34;xxx\u0026#34; 5POSTGRES_HOST=\u0026#34;xxx\u0026#34; 6POSTGRES_PASSWORD=\u0026#34;xxx\u0026#34; 7POSTGRES_DATABASE=\u0026#34;xxx\u0026#34; 8 9AUTH_SECRET=xxx Please replace xxx by the values of your postgres database from Vercel. In case you don\u0026rsquo;t have one or do not want one leave the content like described here, just remember when you startup the app it will not be completly functional then, since there is no database connection.\nThen inside the folder \u0026ldquo;dev\u0026rdquo; you create or open the file \u0026ldquo;Dockerfile\u0026rdquo; with following content:\n1# Use an official Node.js runtime as a base image 2FROM node:latest 3 4# Update OS mostly to enjoy latest security updates 5RUN apt-get update \u0026amp;\u0026amp; apt-get upgrade -y 6 7# Set the working directory inside the container 8WORKDIR /usr/src/app 9 10# Copy the rest of the application code to the working directory 11COPY --chown=node:node ../../. . 12 13# update npm package manager if required 14RUN npm install -g npm 15 16# Copy the startup script to the container 17COPY --chown=node:node app-startup.sh /usr/src/app/app-startup.sh 18 19# Give execute permissions to the script 20RUN chmod +x /usr/src/app/app-startup.sh 21 22# set user to node 23USER node 24 25# Expose the port your app runs on 26EXPOSE 3000 27 28# Define the startup command 29CMD [\u0026#34;sh\u0026#34;,\u0026#34;/usr/src/app/app-startup.sh\u0026#34;] What did we just do? At first the \u0026ldquo;Dockerfile\u0026rdquo; pulls the official node.js repository from the Docker Hub repository for further use. Then it installs the latest security and functional updates into our container. Then it copies the content of our projects directory into directory /usr/src/app inside the container. By \u0026ldquo;RUN npm install -g npm\u0026rdquo; the newest version of npm is installed. Then our startup script is copied and the user used inside the container for all of that is changed to \u0026ldquo;node\u0026rdquo; to reduce security flaws. At the end the file exposes our development port 3000 and instructs the system to start our startup script everytime the container will be launched.\nThe next step is to open/create the file \u0026ldquo;docker-compose.yml\u0026rdquo; in the same directory with following content:\n1services: 2 nextjs-dashboard: 3 build: . 4 ports: 5 - 3000:3000 6 restart: unless-stopped 7 volumes: 8 - ../../:/usr/src/app/ 9 - ./app-startup.sh:/usr/src/app/app-startup.sh 10 networks: 11 - default 12 env_file: 13 - .env Docker compose helps us to build up a real dev environment and later to additionally add a database server. For the time being the file contains the info to use the Dockerfile in the same directory to build our \u0026ldquo;nextjs-dashboard\u0026rdquo; service, exposes port 3000 to the host and would restart the service in case it crashed without manual intervention. On top of that, by the \u0026ldquo;volumes\u0026rdquo; are we specify that our complete project directory on the host is mapped inside the container at /user/src/app and does the same for our startup script. By defining network \u0026ldquo;default\u0026rdquo; it will create a encapsuled virtual network for the service as another layer of security. The last step by providing \u0026ldquo;env_file\u0026rdquo; is to forward the content ouf our \u0026ldquo;.env\u0026rdquo; file in the same directory as environment variables into the container.\nWe are nearly done, the last step is to open/create file app-startup.sh again in the same folder with following content:\n1echo Installing node modules from package.json 2npm install --no-progress 3npm install next@latest --no-progress 4echo Starting your Node.js application 5npm run dev 6 7# Keep the script running in case of errors to be able to inspect the container 8tail -f /dev/null The script runs every time you will startup the dev container and installs any missing node modules before starting the webserver. That\u0026rsquo;s it your file structure should now look like this:\nContainer Operations While you can still use git like before to push your code to Github you have several other options to control your container. If you are inside your /dev folder, with\n1docker compose up -d your container will be build and strated for the first time. Any time you make changes of the docker-compose.yml file the same command will refresh and restart the configuration.\nThe command\n1docker compose down will shutdown your whole environment. While with\n1docker compose logs the log output including the console.log will be shown. And if you are not sure if you container is running or not you can test this with command:\n1docker compose ps In case you want to change/update your Dockerfile, after saving you have to run:\n1docker compose build 2docker compose up -d to make these changes active.\nSummary In this tutorial you successfully \u0026ldquo;dockerized\u0026rdquo; your development area, but maybe can\u0026rsquo;t already see big advantages beside that you do not have to install node.js and keep it up to date. That will change during the next tutorials when wie first add a database service to become independent from Vercels database instance, to divide dev and prd data and to increase security.\n","tags":["english"],"section":"blog"},{"date":"1704672000","url":"/blog/mindset/","title":"Mindset *****","summary":"Du denkst du bist immer offen gegenüber allem und jedem und auch so total „open-minded“? Das dachte ich auch und dann las ich dieses Buch. Dr. Carol Dweck gelingt es auf weniger als 300 Seiten, uns selbst soweit zu entlarven dass man erkennt, dass man vielleicht in einigen oder sogar vielen Disziplinen ein wirklich offener Mensch, aber in anderen Bereichen genau in das Gegenteil verfallen ist.\nSchöne Beispiele aus Dr. Dwecks privaten Leben und beruflichen Laufbahn veranschaulichen wie uns die Macht der gewohnheit, Erziehung und das Leben selbst in vielen Lagen zu Gewohnheitstieren gemacht hat. Erfreulicherweise liefert sie auch gleich Methoden und Möglichkeiten aus diesem Hamsterrad in unserem Verstand zu entkommen.\n","content":"Du denkst du bist immer offen gegenüber allem und jedem und auch so total „open-minded“? Das dachte ich auch und dann las ich dieses Buch. Dr. Carol Dweck gelingt es auf weniger als 300 Seiten, uns selbst soweit zu entlarven dass man erkennt, dass man vielleicht in einigen oder sogar vielen Disziplinen ein wirklich offener Mensch, aber in anderen Bereichen genau in das Gegenteil verfallen ist.\nSchöne Beispiele aus Dr. Dwecks privaten Leben und beruflichen Laufbahn veranschaulichen wie uns die Macht der gewohnheit, Erziehung und das Leben selbst in vielen Lagen zu Gewohnheitstieren gemacht hat. Erfreulicherweise liefert sie auch gleich Methoden und Möglichkeiten aus diesem Hamsterrad in unserem Verstand zu entkommen.\nMeine Bewertung: 5 von 5 Sternen.\nInhalt Die U.S. amerikanische Professorin hat es an der Standford University zur Weltbekanntheit gebracht. Um so erstaunlicher war es für mich, dass ich von diesem Buch zuvor noch nichts gehört hatte. In insgesamt 9 Kapiteln wird zunächst versucht einem klar zu machen, dass unser Mindset in vielen lebenslagen gar nicht so offen ist, wie wir es vielleicht denken. Es folgen viele Beispiele aus Sport, Beruf, Liebesleben und weiteren, die verdeutlichen wie eine Änderungen der Betrachtungsweise zu größerem Erfolg und Wohlebefinden helfen kann. Im letzten Kapitel folgen dann konkrete Empfehlungen wie man sich selbst eine wirklich offenes Mindset zulegen und dieses auch bewahren kann.\nWeitere Infos Originaltitel: Mindset – Changing the way you think to fulfil your potential Genre: Sachbuch Erscheinungsjahr: 2006 Seitenzahl: 288 ISBN: 978-1-4721-3995-5 Erhältlich in jeder Buchhandlung oder unter https://www.medimops.de/dweck-dr-carol-mindset-updated-edition-changing-the-way-you-think-to-fulfil-your-potential-taschenbuch-M0147213995X.html ","tags":["deutsch","Bücher"],"section":"blog"},{"date":"1697068800","url":"/blog/install-docker-engine/","title":"Docker Engine auf einem Linux Server installieren","summary":"Docker hilft uns durch sogenannte Container dabei die technische Serverinfrastruktur, die für verschiedenste Projekte benötigt wird, schnell und vom Basissystem isoliert aufzusetzen. Außerdem veringern wir durch den Einsatz von Docker den Wartungsaufwand z.B. für Systemaktualsierungen und Backups. Tausende von Projekten stehen vorgefertigt zur Verfügung, die mit einem einfachen Kommando nur gestartet werden können. In diesem Artikel beschreibe ich wie Docker und das zusätzlich hilfreiche Docker Compose installiert werden. Docker Compose hilft uns dabei ganze Containerlandschaften zur orchestrieren. Ein Beispiel: Bei der Installation von WordPress über Docker werden mehrere Container benötigt, einer für die App und einer für die Datenbank. Wenn wir diese nun alle einzeln starten und evtl. dabei noch bestimmte Parameter übergeben wollen, wird es schnell uünbersichtlich. Mit Compose können wir alle benötigten Parameter und Container in einer strukturierten Datei zusammenfassen und diese Umgebung mit einfachen Befehlen steuern.\n","content":"Docker hilft uns durch sogenannte Container dabei die technische Serverinfrastruktur, die für verschiedenste Projekte benötigt wird, schnell und vom Basissystem isoliert aufzusetzen. Außerdem veringern wir durch den Einsatz von Docker den Wartungsaufwand z.B. für Systemaktualsierungen und Backups. Tausende von Projekten stehen vorgefertigt zur Verfügung, die mit einem einfachen Kommando nur gestartet werden können. In diesem Artikel beschreibe ich wie Docker und das zusätzlich hilfreiche Docker Compose installiert werden. Docker Compose hilft uns dabei ganze Containerlandschaften zur orchestrieren. Ein Beispiel: Bei der Installation von WordPress über Docker werden mehrere Container benötigt, einer für die App und einer für die Datenbank. Wenn wir diese nun alle einzeln starten und evtl. dabei noch bestimmte Parameter übergeben wollen, wird es schnell uünbersichtlich. Mit Compose können wir alle benötigten Parameter und Container in einer strukturierten Datei zusammenfassen und diese Umgebung mit einfachen Befehlen steuern.\nVorraussetzungen Wir gehen davon aus, dass wir ein Linuxsystem auf Basis von Debian als Betriebssystem nutzen. Das können z.B. das Raspberry Pi OS, Ubuntu oder eine der vielen anderen Varianten davon sein. Grunsätzlich funktioniert Docker aber auch unter allen anderen Linux Systemen und auch unter Windows oder MacOS.\nInstallation der Docker Engine Wir wollen immer die aktuellsten Versionen nutzen und orientieren uns deshalb an den Anleitungen auf den offiziellen Docker Seiten dazu.\nDocker installieren: https://docs.docker.com/engine/install/debian/ Schritte zur komfortableren Nutzung von Docker: https://docs.docker.com/engine/install/linux-postinstall/ Wenn man sich nicht von der augeinscheinlich großen Masse der Schritte erschlagen lässt, sehen wir dass wir im Endeffekt nur 3-4 Kommandos per Copy \u0026amp; Pase in unsere Kommandozeile kopieren mussten.\nAm besten führen wir abschließend noch einmalig einen Reboot unseres System durch.\nVerwendung von Docker Compose Der Aufbau einer Docker-Composse Konfiguration ist im endeffeckt immer gleich und basiert auf einer einzelnen Datei, der compose.yml. Diese legt man am besten nicht einfach irgendwo hin, sondern überlegt sich eine Ordnerstruktur, da man ja evtl. viele Umgebungen parallel betreiben und den Überblick behalten möchten. Ein Beispiel:\nwir legen im Verzeichnis /opt/ ein Unterverzeichnis /opt/docker/an. Und hier dann wiederum Unterverzeichnisse z.B. nach Projekten oder URLs gegliedert. Also zum Beispiel /opt/docker/wordpress/. In dieses Untervzeichnis gehört dann unsere Datei zur Definition der Umgebung, also /opt/docker/wordpress/compose.yml Wenn wir beim Beispiel WordPress bleiben bekommen wir auf https://hub.docker.com/_/wordpress ein Beispiel für den Inhalt der compose.yml Dieses Beispiel wollen wir noch etwas für uns anpassen und für den dauerhaften Betrieb optimieren. Das kann dann so aussehen (Achtung die Formatierung mit den freien Zeichen als Tabs ist bindend):\n1services: 2 wordpress-app: 3 container_name: wordpress-app 4 hostname: wordpress-app 5 image: wordpress 6 restart: always 7 ports: 8 - 8080:80 9 environment: 10 WORDPRESS_DB_HOST: wordpress-db 11 WORDPRESS_DB_USER: exampleuser 12 WORDPRESS_DB_PASSWORD: examplepass 13 WORDPRESS_DB_NAME: exampledb 14 TZ: Europe/Berlin 15 volumes: 16 - ./wordpress:/var/www/html 17 logging: 18 options: 19 max-size: \u0026#34;10m\u0026#34; 20 max-file: \u0026#34;3\u0026#34; 21 22 wordpress-db: 23 container_name: wordpress-db 24 hostname: wordpress-db 25 image: mysql:5.7 26 restart: always 27 environment: 28 MYSQL_DATABASE: exampledb 29 MYSQL_USER: exampleuser 30 MYSQL_PASSWORD: examplepass 31 MYSQL_RANDOM_ROOT_PASSWORD: \u0026#39;1\u0026#39; 32 TZ: Europe/Berlin 33 volumes: 34 - ./db:/var/lib/mysql 35 logging: 36 options: 37 max-size: \u0026#34;10m\u0026#34; 38 max-file: \u0026#34;3\u0026#34; Zur Erklärung:\nservice, container_name, hostname: Nur den Service selbst zu benennen ist Pflicht. Die anderen beiden Variablen setzen wir um unseren Container je nach Zugriffsart schnell finden zu können, wenn wir ihn mal suchen. image: Die Anwendung welche wir verwenden möchten, im Standard ein Image von Docker Hub https://hub.docker.com/search?q= restart: always bedeutet, dass der Container neu gestartet wird, egal warum auch immer er zuvor abgebrochen/abgestürzt ist. environment: Verschiedene Umgebungsvariablen die an den Container weitergegben werden. Das können je nach Image ganz unterschiedliche sein. volumes: Persistente Daten, die wir z.B. auch für ein Backup berücksichtig bzw. grundsätzlich unter unserer Kontrolle haben und nicht im Container “versteckt” haben wollen. durch ./bestimmen wir, dass es sich dabei um einen Uneterverzeichnis des aktuellen Ordners handelt. Falls nicht vorhanden würde dieser automatisch angelegt werden, besser aber vorher schon selber anlegen (Berechtigungen, etc…). logging: Manche Container/Applikationen sind recht freudig im Schreiben von Protokolldateien. Hiermit schränken wir dies auf 3 10MB große Dateien ein, wobei der älteste Eintrag beim überschreiten der Grenze überschrieben wird. ports: Falls kein Reverse Proxy zum Einsatz kommt, gibt z.B. 8080:80 an, dass der im Container laufende Port 80 and das Host System unter Port 8080 weitergleitet wird. D.h. wenn wir in unserem Beispiel WordPress aufrufen möchten, dann mit: http://HOST:8080 . Zur Kommunikation der Container Untereinander also z.B. von App zur DB wird keine Portfreigabe benötigt! Das erhöht die gleichzeitig die Sicherheit unserer Umgebung. Nützliche Befehle Aus dem Verzeichnis heraus, in dem wir unsere compose.yml angelegt haben können wir unsere Umgebung nun mit docker compose up -d starten. Wenn wir alle Container aus der Compose Datei herunterfahren möchten, geht das mit docker compose down. Mit dem Befehl docker compose pull ziehen wir die aktuellste Version unserer Images von Docker Hub. Nach dem Pull ist ein erneutes docker compose up -d erforderlich um die Container in der neuen Version zu starten. Die persistenten Daten aus den definierten Volumes bleiben dabei natürlich erhalten. Gefährlich wird es nur bei größeren Versionssprüngen innerhalb der Anwendung. Dann gibt es entweder die Möglichkeit die Umgebung herunterzufahren, das letzte geladene Image zu löschen und dann wieder zu starten. Alternativ lässt sich in der compose.yml auch eine version des Images fest einstellen. Damit verzichtet man in der Regel aber auch Sicherheitsupdates.\n","tags":["deutsch"],"section":"blog"},{"date":"1694390400","url":"/blog/windows10-password-reset/","title":"Passwort vergessen in Windows 10? (Fast) kein Problem!","summary":"In einer besonders unglücklichen Situation bin ich kürzlich von einem Freund kontaktiert worden: Er hat sein Windows 10 Anmeldepasswort unwiederbringlich vergessen. Also auf zu Google und siehe da; es gibt unzählige Lösungsvorschläge um ein Win10 Passwort zurückzusetzen. Dann aber die Ernüchterung: Microsoft hat schon nachgebessert und alle Tricks, die ich gefunden habe, funktionieren leider nicht mehr. Was nun?\nZeit für Eigeninitative 🙂 In dieser Anleitung beschreibe ich in Kürze, wie man ein Administratorkonto, auch in der aktuellsten Windows 10 Version, zurücksetzen kann.\n","content":"In einer besonders unglücklichen Situation bin ich kürzlich von einem Freund kontaktiert worden: Er hat sein Windows 10 Anmeldepasswort unwiederbringlich vergessen. Also auf zu Google und siehe da; es gibt unzählige Lösungsvorschläge um ein Win10 Passwort zurückzusetzen. Dann aber die Ernüchterung: Microsoft hat schon nachgebessert und alle Tricks, die ich gefunden habe, funktionieren leider nicht mehr. Was nun?\nZeit für Eigeninitative 🙂 In dieser Anleitung beschreibe ich in Kürze, wie man ein Administratorkonto, auch in der aktuellsten Windows 10 Version, zurücksetzen kann.\nSo viel im Voraus: Microsoft muss hier weiter nachbessern, denn die beschriebene Methode kann dazu genutzt werden administrativen Zugriff auf jegliche Win10 PCs zu erlangen. Einzige Vorraussetzung ist der physische Zugriff auf den Rechner.\nVorbereitung Mein Freund hat mir den betroffenen Computer vorbeigebracht, also hatte ich direkten Zugriff auf den Rechner. Das Benutzerkonto auf das er nicht mehr zugreifen konnte, war ein lokales Konto.\nHinweis: Für ein Benutzerkonto, dass mit der Microsoft Cloud verknüpft ist, funktioniert diese Anleitung nur teilweise – Dort ist das zurücksetzen des Passworts aber auch kein Problem und kann von einem anderen Endgerät aus, Online erledigt werden.\nAußerdem habe ich mir ein Windows 10 Installationsmedium auf einem USB-Stick erzeugt, weil ich während der ersten Recherechen darauf gestoßen bin, dass das erforderlich ist. Den offiziellen Windows 10 Download findet man unter in den Quellen am Ende des Artikels. Zum erstellen des Sticks brauchen wir natürlich irgendwo einen zweiten Rechner auf den wir Zugriff haben. Die heruntergeladene ISO Datei lässt sich je nach Betriebssystem unterschiedlich auf den Stick bringen. Google hilft.\nVorgehensweise Folgende Schritt-für-Schritt Anleitung umreißt die als funktionierend gestestete Vorgehensweise, um das Passwort eines lokalen Windows 10 Benutzers zurückzusetzen.\nZunächst booten wir von unserem zuvor erzeugten Bootstick. Im ersten Dialogfenster der Sprachauswahl etc. drücken wir gleichzeitig die SHIFT + F10 Tasten. In der daraufhin erscheinenden Kommandozeile (oha; dass dürfte doch eigentlich gar nicht funktionieren), wechseln wir auf die Partition unserer lokalen Windows 10 Installation, also z.B. mit den Befehl “C:”.\nNun “hangeln” wir uns ins lokale Systemverzeichnis. Das geht z.B. mit den Befehlen: “cd Windows” Enter Taste, dann “cd system32” Enter Taste.\nJetzt erstellen wir unser “magisches Tor” um später, beim normalen Hochfahren, ebenfalls in die Kommandozeile wechseln zu können. Mit “cp utilman.exe utilman_old.exe” sichern wir zunächst den Eingabehilfeassistenten um ihn später wiederherstellen zu können.\nMit “copy cmd.exe utilman.exe” erzeugen wir eine Kopie des Kommandzeilenprogramms unter dem Namen des Eingabehilfeassistenten.\nDas war der erste Streich. Wir können nun die Windows Installationsroutine schließen und den Rechner wieder normal hochfahren.\nWir müssen am Anmeldebildschirm einen erneuten Reboot im abgesicherten Modus herbeiführen, da zumindest mir, sonst Aufrufe der Kommandozeile blockiert wurden (vermutlich vom im Hintergrund laufenden Systemschutz-Program…). Also die “SHIFT” Taste gedrückt halten, unten Rechts auf das Powersymbol drücken und Neustart auswählen. Im nach kurzer Zeit erscheinenden Zwischendialog wählen wir “Advanced Options” bzw. “Erweiterte Optionen”, und dann “Startup Settings” bzw. “Starteinstellungen”. Nun auf “Restart” bzw. “Neustart” klicken. (Sorry, habe gerade nur Englische Screenshots) Sobald der Rechner wieder oben ist, drücken wir die Taste 6 um den abgesicherten Modus mit Kommandozeile zu starten. Leider startet sich die Kommandozeile nicht direkt, da wir zunächst unser Passwort eingeben sollen (haha), aber unser Trick den wir durch Schritt 6 ermöglicht haben, kann nun ausgeführt werden: Wir klicken auf das Symbol für die Eingabehilfe (das zweite Symbol rechts unten) und Voilà: Die Kommandozeile öffnet sich, und dieses mal mit vollen Berechtigungen! Durch Eingabe des Befehls “net user”, bekommen wir eine Liste aller lokalen Benutzerkonten angezeigt. Da fehlt mir anscheinend das Know-How, aber das Benutzerkonto um das es mir eigentlich geht, wird gar nicht angezeigt?! Macht nichts: Wir reaktivieren einfach das vorhandene “Administrator” Benutzerkonto und arbeiten mit diesem weiter. Wie? nächster Schritt: Der Befehl “net user Administrator /active:yes” aktiviert das Konto und der Befehl “net user Administrator PASSWORD” setzt das Passwort PASSWORD für diesen. (mit angepassten Befehlen lässt sich so natürlich auch ein komplett neuer User als Admin einrichten, falls der User “Administrator” nicht verfügbar ist.) Weil es so schön ist: Zeit für einen Reboot\nm Anmeldebildschirm unten links sehen wir nun mehrere Benutzer zur Auswahl, und eben auch den “Administrator” den wir soeben aktiviert haben. Auf diesen klicken wir jetzt und melden uns dann als dieser User am System, mit dem zuvor gestetzten Passwort, an.\nNachdem wir die Windows “Ersteinrichtungshölle” hinter uns gelassen haben, sehen wir jetzt einen mehr oder weniger augeräumten Desktop und haben uns erfolgreich mit Adminstratorberechtigungen angemeldet!\nJetzt können wir über die Systemsteuerung -\u0026gt; Benutzer -\u0026gt; Andere Konten das Passwort unseres eigentlichen Benutzer komfortabel ändern und ja; den PC neu starten.\nFertig: Jetzt können wir im Anmeldebildschirm von Windows wieder unseren eigentlichen Benutzer auswählen und uns mit dem neu gesetzten Passwort anmelden.\nAufräumen Unsere gesetzten Spuren machen wir über folgende Befehle in der Windows Kommandozeile wieder rückgängig:\nnet user administrator /active:no – Wir deaktivieren den “Administrator” Benutzer wieder, brauchen wir nun ja nicht mehr. C:”, “cd Windows”, “cd system32”, “del utilman.exe”, “copy utilman_old.exe utilman.exe” – stell die Systedateien wieder auf ihren ursprünglichen Zustand. Fazit Um dem gestellten Problem von vornherein aus dem Wege zu gehen, könnte man z.B. ein Microsoft Account (wenn man dem Unternehmen traut) erstellen und mit dem Konto verknüpfen. Dann ist das zurücksetzen evtl. Online möglich, insofern der betroffene PC eine Internetverbindung hat.\nAuch kann man mit kürzeren PINs arbeiten, die Microsoft seit geraumer Zeit für das Anmelden, neben der weiteren Option eines Erinnerungssatzes, bereitstellt.\nNichtsdestotrotz (ja, das schreibt sich wohl so): Mit der beschriebenen Vorgehensweise lässt sich nicht nur ein vergessenes Windows Passwort zurücksetzen, sondern auch Zugriff auf jeden Windows 10 PC, auf den physischer Zugriff besteht, beschaffen. Eine eklatante Sicherheitslücke die in anderen Betriebssystemen wie Linux und MacOS besser “gestopft” ist.\nNaja, meinem Freund konnte ich so jedenfalls helfen 🙂\nEnde gut, alles gut? Wie kann ich mich vor so einem “Angriff” schützen? Eigentlich dürfte das beschriebene Szenario überhaupt nicht funktionieren, weil sich so jeder administrativen Zugang auf einen Windows PC verschaffen kann.\nUrsache ist eine Verkettung von Umständen, die uns diese Sicherheitslücke öffnet. Um solch einen Angriff zu verhindern reicht es nicht aus, einfach auf ein Micorosoft Online Konto zu wechseln. Es kann sich ja trotzdem jeder über ein neues Benutzerkonto administrativen Systemzugriff verschaffen, sobald sie/er physischen Zugriff auf den Rechner hat.\nEinen solchen Angriff zu verhindern ist nicht trivial, denn folgendes müsste in Kombination gemacht werden:\nSetzen eines Passworts auf BIOS Ebene: Verhindert den Start eines anderen Bootmediums und ähnliches. Das werden viele aus Bequemlichkeit nicht machen wollen. Aktivieren der Festplattenverschlüssung, z.B. mit Bitlocker: Sonst könnte man die Festplatte einfach aus dem Rechner herausschrauben/löten und den Inhalt woanders auslesen. Kontra: Das macht den Windows PC (noch) langsamer. Microsoft selbst könnte wiederum nachbessern indem Sie in Windows:\nden Zugriff auf die Kommandozeile während der Installationroutine unterbinden. Vermutlich ist das nicht so einfach umzusetzen, da die Kommandozeile hier und da ebenfalls für Spezialfälle benötigt werden könnte. die Signaturen von Systemdateien, die vor dem erfolgreichen Login ausgeführt werden, prüfen. Das sollte sich auf jeden Fall umsetzen lassen und es ist verwunderlich das Microsoft das nicht schon lange tut. Unter Linux gibt es dafür schon seit Jahren Implementierungen wie z.B. SELinux. Quellen https://adamtheautomator.com/reset-windows-10-password-command-prompt/ – funktionierte nicht, aber nützliche Tipps zum Prozessversändnis https://www.avast.com/c-recover-windows-password#gref – funktionierte nicht, aber ebenso nützliche Tipps zum Prozessversändnis https://www.makeuseof.com/tag/3-ways-to-reset-the-forgotten-windows-administrator-password/ – funktionierte auch nicht, noch mehr nützliche Tipps zum Prozessversändnis https://www.sony.com/electronics/support/articles/00123047 – wie man einen deaktivierten Windows Account aus der Kommandzeile heraus aktiviert https://www.microsoft.com/en-us/software-download/windows10ISO – Windows 10 ISO Download ","tags":["deutsch"],"section":"blog"},{"date":"1691625600","url":"/blog/docker-fixed-ips/","title":"Feste IP-Adressen mit Docker Compose","summary":"Warum um Himmels willen sollte man Containern feste IP-Adresse zuweisen? Eine absolut berechtigte Frage auf die die Antwort lautet: Am besten nie! Denn: Docker kommt mit einer Art eingebauten DNS- und DHCP-Server und verwaltet die IP-Adresse wunderbar selbst. Leider gibt es trotzdem Spezialfälle in denen man sich das Leben vereinfachen kann, wenn man IPs händisch setzt. So ein Spezialfall wäre z.B. gegeben, wenn die Applikation die in einem Container betrieben wird nicht mit Hostnamen sondern nur mit IP-Adressen umgehen kann, um auf einen weiteren Container zuzugreifen.\n","content":"Warum um Himmels willen sollte man Containern feste IP-Adresse zuweisen? Eine absolut berechtigte Frage auf die die Antwort lautet: Am besten nie! Denn: Docker kommt mit einer Art eingebauten DNS- und DHCP-Server und verwaltet die IP-Adresse wunderbar selbst. Leider gibt es trotzdem Spezialfälle in denen man sich das Leben vereinfachen kann, wenn man IPs händisch setzt. So ein Spezialfall wäre z.B. gegeben, wenn die Applikation die in einem Container betrieben wird nicht mit Hostnamen sondern nur mit IP-Adressen umgehen kann, um auf einen weiteren Container zuzugreifen.\nFrüher war das ein relativ kompliziertes unterfangen, mit den aktuellen Docker (Compose) Versionen ist aber auch das kinderleicht.\nIP-Adressen laufender Container herausfinden Falls du bereits diverse Container innerhalb eines Netzes am laufen hast, wäre es gefährlich nur einigen davon eine feste IP-Adresse zuzuordnen. Warum? Weil, es dir dann z.B. nach einem Reboot deines Servers passieren könnte, dass sich die automatisch zugewiesene IP eines Containers mit einer von dir fest zugeordneten eines anderen Containers überschneidet. Das Resultat wäre, dass der zweite Container nicht starten kann, da die IP ja bereits vergeben ist.\nUm zu erfahren welche IP-Adresse einem Container aktuell zugewiesen ist, hilft uns das Kommando\n1docker inspect -f \u0026#39;{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; CONTAINERNAMEoderID Falls man die IP-Adressen aller laufenden Container herausfinden möchte, ist dieses Kommando sehr nützlich:\n1docker inspect $(docker ps -q ) --format=\u0026#39;{{ printf \u0026#34;%-50s\u0026#34; .Name}} {{range .NetworkSettings.Networks}}{{.IPAddress}} {{end}}\u0026#39; Ein Netz einrichten In unserem Beispiel wollen wir allen Containern die in einem für unseren Caddy Reverse Proxy erstellen Netz liegen feste IP-Adressen vergeben. Falls ihr noch kein solches Netz habt, könnt ihr dieses über den Befehl\n1docker network create --subnet=172.28.0.0/16 proxy anlegen. Nachdem dies erfolgt ist, können wir nun in den Docker Compose Definitionen der einzelnen Container die IP-Adressen 172.20.0.X verwenden. Wobei X für die Zahlen 1-255 steht.\nIP-Zuweisung mit Docker Compose Beginnen wir mit der Zuweisung einer festen IP für unseren Caddy Reverse Proxy:\n1services: 2 caddy: 3 container_name: caddy 4 image: caddy:latest 5 restart: unless-stopped 6 ports: 7 - \u0026#34;80:80\u0026#34; 8 - \u0026#34;443:443\u0026#34; 9 - \u0026#34;443:443/udp\u0026#34; 10 volumes: 11 - ./Caddyfile:/etc/caddy/Caddyfile 12 - ./site:/srv 13 - ./caddy_data:/data 14 - ./caddy_config:/config 15 logging: 16 options: 17 max-size: \u0026#34;10m\u0026#34; 18 max-file: \u0026#34;3\u0026#34; 19 networks: 20 proxy: 21 ipv4_address: 172.28.0.1 22volumes: 23 caddy_data: 24 caddy_config: 25networks: 26 proxy: 27 external: true Nach dem speichern und anschließendem Neustart der Umgebung, ist die IP fest zugeordnet. Entscheidend sind die Blöcke:\n1 networks: 2 proxy: 3 ipv4_address: 172.28.0.1 sowie\n1networks: 2 proxy: 3 external: true Nun verfahren wir genauso mit allen anderen Containern mit einer jeweils individuellen IP und haben unser vorhaben erfolgreich beendet.\nFazit Auch wenn die Zuweisung fester IP-Adressen relativ einfach ist, erscheint das Ganze als etwas „unsauber“. So widerspricht die Vorgehensweise doch dem Prinzip, dass ich die Definition eines Docker Containers nehmen und jederzeit auf einem anderen Server/Host starten kann. Das ist so nun nicht mehr möglich, weil zunächst manuell ein neues Netz angelegt werden muss. Anderseits ist die manuelle Erstellung des Netzes sowieso erforderlich, wenn man eine Art DMZ (eigenes Netz) für seinen Reverse Proxy definiert. Nur der manuell vergebene Subnetzbereich ist hier der zusätzliche Aufwand. So bleibt die Verwendung fester IP-Adressen als ein Spezialfall der nur in bestimmten Szenarien Sinn macht.\n","tags":["deutsch"],"section":"blog"},{"date":"1689638400","url":"/blog/wordpress-caches/","title":"Redis Objekt- und Caddy Seitencache für WordPress unter Docker einrichten","summary":"WordPress ist nicht gerade berühmt dafür, besonders schnell zu sein. Es bemühen sich daraus reultierend unzählige Plugins auf dem WordPress Marktplatz, mit dem versprechen daran etwas zu ändern, darum die Gunst des Administrators zu gewinnen. Oft, vielleicht sogar meistens, können diese Plugins ihr versprechen nicht halten oder reißen sogar neue Sicherheitslücken im System auf. Vom nichts tun wird die Leistung aber auch nicht besser, weshalb dieser Artikel beschreibt wie man die Performance durch einen Seitencache und einen Objektcache steigern kann, ohne die Sicherheit des Systems aufs Spiel zu setzen.\n","content":"WordPress ist nicht gerade berühmt dafür, besonders schnell zu sein. Es bemühen sich daraus reultierend unzählige Plugins auf dem WordPress Marktplatz, mit dem versprechen daran etwas zu ändern, darum die Gunst des Administrators zu gewinnen. Oft, vielleicht sogar meistens, können diese Plugins ihr versprechen nicht halten oder reißen sogar neue Sicherheitslücken im System auf. Vom nichts tun wird die Leistung aber auch nicht besser, weshalb dieser Artikel beschreibt wie man die Performance durch einen Seitencache und einen Objektcache steigern kann, ohne die Sicherheit des Systems aufs Spiel zu setzen.\nWofür sind die Caches da? Ein Seitencache sagt dem Client (Browser) des Besuchers einer Seite, dass er beim erneuten Besuch oder mehrmaligen Aufruf gleicher Dateien, diese nicht jedes Mal wieder neu vom Server laden zu braucht, sondern die Versionen im seinem lokalen Cache (Speicher) nutzen kann. Dadurch wird die Ladezeit der Seite nicht nur stark beschleunigt, sondern auch der Server auf dem die Website liegt entlastet. Der Seitencache sollte andereseits aber auch nur eine bestimmte Zeit lokal vorgehalten werden, damit Änderungen an der Seite selbst bzw. deren Inhalten auch beim Besucher ankommen. Dies lässt sich über eine direktive in allen gängigen Webserver Systemen einstellen.\nDer Objektcahce wiederum agiert auf dem Server selbst und läd häufig benutzte Elemente einer Website in seinen Arbeitsspeicher. In modernen Varianten passiert das über eine sogenannte In-Memory Datenbank, also einer Datenbank die eben ihre Inhalte im Arbeitspeicher ablegt. Das hat neben der gesteigerten Performance den positiven Nebeneffekt, dass weniger von der Festplatte/SSD geladen werden muss und die Hardware so geschont wird.\nSeiten Cache in Caddy Reverse Proxy aktivieren Um den Seiten Cache für unsere WordPress Installationen zu aktivieren fügen wir oben unter den globalen Definitionen in unserem Caddyfile folgende Regel hinzu:\n1(wordpress) { 2 header { 3 Cache-Control \u0026#34;public, max-age=36000, must-revalidate\u0026#34; 4 } 5} Nun importieren wir diese Regel in allen unseren WordPress Defintionen darunter, z.B. so:\n1domain.de { 2 import wordpress 3 reverse_proxy container:80 4} Das war es schon. Nun müssen wir die Caddy Konfiguration nach dem speichern nur einmal neu laden, schon ist der Seitencache überall aktiviert.\nRedis Objektcache für WordPress konfigurieren Um einen Redis Objektcache für WordPress zu aktivieren gibt es mehrere Möglichkeiten. Ich habe mich dafür entschieden einen eigenen Redis Docker Container zu starten der dann von WordPress über ein kleines Plugin angesprochen wird. Dazu müssen wir drei Dinge tun.\ncompose.yml anpassen Zunächst ergänzen wir unsere Compose Datei im einen weiteren Block für den Redis Service. Das kann ungefähr so aussehen:\n1 wordpress-redis: 2 image: redis:alpine 3 hostname: wordpress-redis 4 container_name: wordpress-redis 5 restart: unless-stopped 6 networks: 7 - default 8 logging: 9 options: 10 max-size: \u0026#34;10m\u0026#34; 11 max-file: \u0026#34;3\u0026#34; Bitte daran denken das Ganze vorher über docker compose down herunterzufahren und nach dem speichern wieder mit docker compose up -d zu starten. Nun steht uns der Redis Server zur Verfügung und es geht mit dem nächsten Schritt weiter.\nwp-config.php editieren In der WordPress Konfigurationsdatei „wp-config.php“ müssen wir noch zwei Zeilen ergänzen. Diese Datei liegt im Stammordner euerer WordPress Installation und ist (soll) in der Regel sowohl lese- als auch schreibgeschützt sein. D.h. zum editieren müsst ihr kurz die Berechtigungen auf die Datei so ändern, dass ihr sie beschreiben könnt. Anschließend nicht vergessen die Berechtigung wieder auf den Ausgangswert einzustellen.\n1define( \u0026#39;WP_REDIS_HOST\u0026#39;, \u0026#39;wordpress-redis\u0026#39; ); 2define( \u0026#39;WP_REDIS_PORT\u0026#39;, 6379 ); Das funktionierte bei mir nur ordnungsgemäß, wenn ich diese Zeilen relativ weit oben in der Datei eingefügt habe.\nPlugin installieren Abschließend kommen wir um die Installation eines kleinen Plugins, dass WordPress sagt den Redis Cache zu nutzen, nicht ganz herum. Installiert auch dafür aus dem Marktplatz das Plugin Redis Object Cache und aktivert dieses.\nIn der Konfiguration des Plugins (unter Einstellungen -\u0026gt; Redis) klickt ihr nun auf „Object-Zwischenspeicher aktivieren“.\nFazit Wenn bis hierhin alles funktioniert hat, seid ihr auch schon fertig und sowohl Seiten- als auch Object-Cache sind aktiv. Das könnt ihr euch auch beim betrachten des Website Zuststands in WordPress bestätigen lassen.\n","tags":["deutsch"],"section":"blog"},{"date":"1685750400","url":"/blog/buddhismus-fuer-praktiker/","title":"Buddhismus für Praktiker von Volker Zotz *****","summary":"Das Wort Buddha kommt im Buch nur genau 3mal vor, das Cover eingeschlossen. Auch sonst hält der Österreicher Volker Zotz im 1999 zum ersten Mal erschienen Buch „Mit Buddha das Leben meistern“ Abstand von Esotherik und Mystifizierung, gibt aber eine gelungene kurze und prägnante Zusammenfassung der Leitmotive des klassischen Buddhismus.\nMeine Bewertung: 5 von 5 Sternen\nInhalt Nach dem ersten viertel des Buches, welches den Lebensweg von Gautamas und seiner Begleiter skiziert geht es im wesentlichen um Übungen die in die Tat umgesetzt werden wollen, sowie um philosphische Grundsätze an denen sich kein Übel identifizieren lässt.\n","content":"Das Wort Buddha kommt im Buch nur genau 3mal vor, das Cover eingeschlossen. Auch sonst hält der Österreicher Volker Zotz im 1999 zum ersten Mal erschienen Buch „Mit Buddha das Leben meistern“ Abstand von Esotherik und Mystifizierung, gibt aber eine gelungene kurze und prägnante Zusammenfassung der Leitmotive des klassischen Buddhismus.\nMeine Bewertung: 5 von 5 Sternen\nInhalt Nach dem ersten viertel des Buches, welches den Lebensweg von Gautamas und seiner Begleiter skiziert geht es im wesentlichen um Übungen die in die Tat umgesetzt werden wollen, sowie um philosphische Grundsätze an denen sich kein Übel identifizieren lässt.\nErstaunlicherweise sind viele, wenn nicht die Meisten, Ansätze in modernen psychologischen Therapieformen wieder zu finden. Dies ist erstaunlich wenn man berücksichtig das diese bereits um die 2.500 Jahre alt sind.\nDie gerade mal knapp über 200 Seiten sind verständlich geschrieben, regen zum Denken an und können dem ein oderen Anderen sicherlich dabei helfen die persönliche Mitte (wieder) zu finden. Ob nun aus allgemeiner Neugier, auf der Suche nach Stressreduktion oder aus anderen Motiven: Das Buch kann ich guten Gewissens jedem empfehlen, der seinen Horizont erweitern möchte.\nIm folgendenen ein paar Auszüge der Übungen und Prinzipien.\nÜbungen Die Übungen werden hier nur Auszugsweise als Notiz für die Durchführung umschrieben. Sinn und Zweck entnimmst du am besten direkt aus dem Buch.\nÜbung 1 (Seite 43): Der Tagesrückblick Lass keinen Tag erinnerungslos verstreichen. Vor dem Schlafen das was am Tag geschah im Schnelldurchlauf (nicht an einzelnen Momenten hängen bleiben) Revue passieren lassen. In umgekehrter Reihenfolge, also von abends nach morgens.\nÜbung 2 (Seite 45): Das gesammelte Aufstehen Sich schon beim Aufstehen, des Moments bewusst sein. Dazu 2-3 Minuten lang mit den Bewusstsein von Füßen bis Kopf durch den Körper „wandern“ und dabei das entsprechende Körperteil z.B. durch Bewegung bewusst waahrnehmen. In modernen Therapieformen ist dies auch in einer langsameren Version als „Body Scan“ bekannt. Wer dazu eine geführte Anleitung möchte kann sich diese bei der Techniker Krankenkasse als Audiodatei herunterladen.\nÜbung 3 (Seite 46): Das Feststellen des Zeitauwandes Führe 3 bis 5 Tage genau Buch, womit du Deine Zeit verbringst. Danach das Ergebnis betrachten um eine genaue Übersicht zu erhalten mit welchen Dingen man wieviel Zeit am Tag verbringt.\nÜbung 4 (Seite 73): Die Zeit zur Meditation Durch Meditation soll Entspannung und wahre Freiheit entstehen. 15 Minuten sind ok. Wichtiger als die Länge ist die Regelmäßigkeit, am besten immer zu(m) gleichen Zeitpunkt(en). Mehr zur Vorgehensweise unter „Anleitung zur Meditation“.\nÜbung 5 (Seite 76): Glauben und Wissen Man sollte wissen dass man glaubt und nicht glauben, dass man weiss. Damit ist gemeint deine festen Überzeugungen, sowie die eigene Objektivität und Einstellung anderen gegenüber zu hinterfragen. Diese Einsichten ermöglichen erst überhaupt, den eigenen Horizont zu erweitern.\nÜbung 6 (Seite 77): Was macht mich zu dem, was ich bin? Analysiere die Ergebnisse aus Übung 3 dahingehend, inwiefern regelmäßig wiederholte Tätigkeiten im privaten und/oder Beruf deine Persönlichkeit Formen. Frage Vertraute danach wie Sie dein Verhaltene in bestimmten Situationen wahrnehmen. Vergleiche Fremdwahrnehmung mit deinem Selbstbild.\nÜbung 7 (Seite 102): Das Denken schulen Beginne dein Denken zu beobachten. Gibt es etwas das dich fühlen lässt ungenügend zu sein oder dich selbst an dir stört? Wenn ja, was ist es? Ist es eine Sache oder vieles? Wähle zunächst ein spezifisches Thema und handle nach der Beschreibung im Bereich „Schulung des Denkens“.\nÜbung 8 (Seite 104): Beobachtung der Rede Achte einen Tag lang auf:\nwie du andere grüßt wie du Fragen beantwortest was du bestimmten Menschen sagst oder nicht sagst wann und zu wem du die Unwahrheit sagst ob du zu jemanden grob/unfreundlich sprichst ob du dich manchmal oder oft nicht traust zu reden ob du den Impuls zu reden unterdrückst und wie du dich dabei fühlst Nimm diese Punkte wahr ohne sie direkt zu bewerten. In der Meditationszeit denkst du dann darüber nach was dich wie reden lässt.\nÜbung 9 (Seite 104): Selbstehrlichkeit im Wirken Gestehe dir die wahren Motiven deines Handelns ein. Wenn du an dir arbeiten möchtest, solltest du wissen was du bewusst und auch unterbewusst beabsichtigst. Denke auch an Momente aus der Vergangenheit zu denen du anders gehandelt hast als du wolltest und warum dies so war.\nÜbung 10 (Seite 129): Änderung der Blickrichtung Höre auf deinen eigenen Schmerz zu wichtig zu nehmen. Wie geht es anderen? Wenn es anderen schlechter geht als dir: Der Grund warum es dir so schlecht geht ist, dass dich andere nicht interessieren. Das nächste Mal wenn dich jemand verletzt oder dir etwas schlechtes passiert, dann ärgerst du dich nicht darüber und wirst auch nicht wütend, sondern hilfst jemanden oder bist zu einer Person besonders nett. So ist aus etwas schlechtem etwas gutes entstanden.\nÜbung 11 (Seite 132): Arbeit an der Angst Gautamas Ansicht nach, sollst du dich deinen Ängsten direkt stellen um sie nicht dein Handeln bestimmen zu lassen. Dieser Ansatz wird auch in der modernen Verhaltenstherapie bei Agoraphobien angewandt, siehe https://de.wikipedia.org/wiki/Agoraphobie .\nSchritt 1: Die Angst akzeptieren Auch das bewusstwerden der Angst und die Identifikation der Auslöser, während der Meditation ähnelt anderen modernen Ansätzen der Psychotherapie.\nSchritt 2: Einteilen der Ängste Einteilung der Ängste nach Dingen die du gar nicht oder nur unwesentlich beeinflussen kannst, wie z.B. Erdbeben und nach Dingen die du direkt beeinflussen kannst, wie z.B. eine Prüfungsangst. Wenn Ängste aber dein Leben bestimmen empfiehlt der Autor zusätzlich professionelle psychologische Konsultation.\nSchritt 3: Gedankenarbeit und Handeln Versuche bei den Dingen an denen du so gut wie nichts ändern kannst deine Denkweise darüber zu ändern.\nÜbung 12 (Seite 135): Betrachtung über den Tod Zusammengefasst sollst du dich nicht erst mit deinem Tod beschäftigen wenn er „vor der Tür steht“, sondern überlegen was du tun würdest wenn du wüsstest, dass dir nur noch ein begrenzter Zeitraum bleibt. Wenn du das gründlich zusammengefasst hast, überlege was du davon nicht vielleicht jetzt schon angehen kannst.\nÜbung 13 (Seite 162): Wo sitzen meine Fesseln? Gehe während der Meditation die ersten 9 der 10 Fesseln (siehe Kapitel „Zehn Fesseln“) durch. Am besten beschäftigst du dich an jedem Tag mit jeweils einer.\nÜbung 14 (Seite 162): Erweiterung des Horizonts Nimm dir vor sobald irgend möglich etwas neues zu lernen um deinen Horizont zu erweitern. Es ist egal um was es sich dabei handelt, es soll nur nicht aus einem zwanghaften Grund (z.B. Aufgabe von der Arbeitsstelle) geschehen und „machbar“ sein.\nÜbung 15 (Seite 163): Zeitweiliger Verzicht Identifiziere deine Süchte und verzichte auf diese zumindest zeitweise. Also z.B. kein Fernseher für 3 Tage o.ä. . Die eventuell daraus entstehende Leere sollst du positiv und bereichern nutzen. D.h. zum Beispiel die durch das nicht Fernsehen gewonnene Zeit für ein neues Hobby, Sport oder Meditation nutzen.\nAnleitung in die Meditation (Seite 165-186). Meditation = die Mitte finden und sich daran ausrichten um die größeren Zusammenhänge zu erkennen. Auch soll die Meditation dabei helfen das Leben nicht an sich, durch die vielen unbewusst ausgeführten Aktivitäten, vorbei ziehen zu lassen. Um deine Mitte zu finden wird eine herangensweise aufgeteilt in verschiedene Übungen im Buch empfohlen. Die erste Meditationsübung ist das bewusste Beobachten des eigenen Atems. Du setzt dich aufrecht hin, achtest darauf dass du ungestört bist und fängst an auf deine Ein- und Ausatmung zu achten, möglichst ohne diese dabei zu beeinflussen. Sobald du bemerkst, dass du gedanklich „abgedriftet“ bist, ärgerst du dich nicht darüber, sondern setzt die Beobachtung des Atems weiter fort. Verzage nicht, falls dir dies am Anfang sehr schwer fällt, das ist völlig normal da wir alle gewohnt sind nur auf äußere Reize und nicht unser innerstes zu achten. Umso häufiger du die Übung wiederholst, desto leichter wird sie dir fallen. Ebenso das Stille sitze kann schwer fallen, weshalb es auch so wichtig ist an einem ruhigen Ort ungestört zu meditieren. Die Sitzhaltung ist dabei sekundär, die Hauptsache ist, dass du so bequem, aufrecht, frei und ungezwungen sitzt, dass du dies für einen längeren Zeitraum ohne Schmerz und ohne einzuschlafen halten kannst. Falls du dich zu Beginn überhaupt nicht auf deinen Atem konzentrieren kannst, beobachte die Dinge in deiner Nähe und nehme sie bewusst wahr. Dem Juckreiz und Positionskorrekturen gibst du ebenso bewusst und langsam nach. Auch können die Atemzüge gezählt werden, solang bis man sich vollständig auf das Atemen selbst konzentrieren kann.\nDas Kapitel behandelt weiterhin die Meditation mit dem Fokus auf die eigenen Gefühle, Abläufe in uns selbst sowie die Meditation im täglichen Leben.\nFünf Regeln Anders als z.B. die 10 Gebote der Bibel, handelt es sich bei den 5 Regeln nicht um Gesetze sondern Ziele die man anstreben soll. Auch lässt deren Formulierung bewusst Spielraum zur eigenen Interpretation.\n(Seite 191) Kein Lebewesen bewußt töten oder verletzen (Seite 196) Nicht-Gegebenes nicht nehmen (Seite 200) Ein sittlich reine Leben führen (Seite 205) Lügen und grobe Worte vermeiden (Seite 209) Die Bewußtheit nicht durch Drogen trüben Zehn Fesseln (Seite 138) Das falsche Selbstbild (Seite 141) Zweifelsucht (Seite 144) Riten und Regeln (Seite 148) Gier nach sinnlicher Wahrnehmung (Seite 150) Groll oder Übelwollen (Seite 152) Verlangen nach Gestalt (Seite 153) Verlangen nach Gestaltlosigkeit (Seite 156) Vergleichender Dünkel (Seite 159) Aufgeregtheit (Seite 161) Nichtwissen Vier edle Wahrheiten Die Beschreibungen habe ich Wikipedia entnommen und werden im Buch detailierter und verständlicher mit Beispielen formuliert.\n(Seite 60) Unser Leid „Das Leben im Daseinskreislauf ist leidvoll: Geburt ist Leiden, Altern ist Leiden, Krankheit ist Leiden, Tod ist Leiden; Kummer, Lamentieren, Schmerz und Verzweiflung sind Leiden. Gesellschaft mit dem Ungeliebten ist Leiden, das Gewünschte nicht zu bekommen ist Leiden.\n(Seite 66) Wie Leiden entsteht Die Ursachen des Leidens sind Gier, Hass und Verblendung.\n(Seite 69) Leid ist vermeidbar Erlöschen die Ursachen, erlischt das Leiden.\n(Seite 70) Der Weg Siehe „der edle achtfache Pfad“\nDer edle achtfache Pfad Aus der 4. edlen Wahrheit, dem Weg, ergibt sich wiederum der edle achtfache Pfad.\n1. rechte Einsicht/Anschauung → Erkenntnis Rechte Erkenntnis ist die Einsicht in die Vier edlen Wahrheiten vom Leiden, der Leidensentstehung, der Leidenserlöschung und des zur Leidenserlöschung führenden Achtfachen edlen Pfades.\n2. rechte(s/r) Gesinnung/Absicht → Denken → Entschluss Rechte Gesinnung ist der Entschluss zur Entsagung, zum Nichtschädigen, zur Enthaltung von Groll. Rechtes Denken ist ohne Habgier, hasslos in der Gesinnung und großzügig.\n3. rechte Rede Rechte Rede meidet Lüge, Verleugnung, Beleidigung und Geschwätz. Wie die Gedanken ist die Rede heilsam oder unheilsam, nützlich oder unnützlich, wahr oder falsch.\n4. rechte(s) Handeln/Tat Rechtes Handeln vermeidet das Töten, Stehlen und sinnliche Ausschweifungen. Im weiteren Sinne bedeutet es ein Leben gemäß den Fünf Silas, den Tugendregeln des Buddhismus.\n5. rechter Lebenserwerb/-unterhalt Rechter (Lebens)wandel bedeutet, auf unrechten Lebenswandel zu verzichten. Namentlich werden fünf Arten von Tätigkeiten genannt, die ein buddhistischer Laienanhänger nicht ausüben sollte und zu denen er Andere nicht veranlassen sollte: Handel mit Waffen, Handel mit Lebewesen, Tierzucht und Handel mit Fleisch, Handel mit Rauschmitteln, Handel mit Giften. Im weiteren Sinn bedeutet rechter Lebenserwerb, einen Beruf auszuüben, der anderen Lebewesen nicht schadet und der mit dem Edlen achtfachen Pfad vereinbar ist.\n6. rechte(s) Streben/Üben/Anstrengung Rechtes Streben oder rechte Einstellung bezeichnet den Willen, Affekte wie Begierde, Hass, Zorn, Ablehnung usw. bei Wahrnehmungen und Widerfahrnissen zu kontrollieren und zu zügeln. Wie beim „rechten Denken“ geht es hier um das Prüfen seiner Gedanken, und das Austauschen unheilsamer Gedanken durch heilsame Gedanken.\n7. rechte Achtsamkeit/Bewusstheit Rechte Achtsamkeit betrifft zunächst den Körper: Bewusstwerdung aller körperlichen Funktionen, dem Atmen, Gehen, Stehen usw.; Bewusstwerdung gegenüber allen Sinnesreizen, Affekten und Denkinhalten. Sie sollen umfassend bewusst gemacht sein, ohne sie kontrollieren zu wollen. Die Achtsamkeit auf das „Innere“ prüft die Geistesregungen und benennt sie. Es geht um ein Bewusstwerden des ständigen Flusses der Gefühle und der Bewusstheitszustände. Die Achtsamkeit auf „das Äußere“ bewirkt, ganz im Hier-und-Jetzt zu sein, nicht der Vergangenheit nachzugrübeln und nicht in der Zukunft zu schwelgen.\n8. rechte Sammlung/Konzentration → Versenkung Rechte Sammlung bezeichnet die Fertigkeit, den unruhigen und abschweifenden Geist zu kontrollieren. Häufig auch als einspitziger Geist oder als höchste Konzentration bezeichnet, ist sie ein zentraler Teil der buddhistischen Spiritualität. Es geht hier im Wesentlichen um eine buddhistische Meditation, die vor allem die Konzentration auf ein einziges Phänomen (häufig den Atem) verwendet, wodurch der Geist von Gedanken befreit wird und zur Ruhe kommt.\nFünf Aspekte des Werdens unbelebte Materie belebte Materie menschliches Unbewußte Wirken (Karma) Dharma (ab Seite 81)\nWeitere Infos Mehr zum Buch bei Wikipedia oder über ISBN 3-499-60586-4\nZu kaufen entweder in jeder lokalen Buchhandlung oder z.B. bei https://www.medimops.de/volker-zotz-mit-buddha-das-leben-meistern-buddhismus-fuer-praktiker-taschenbuch-M03499605864.html\n","tags":["deutsch","Bücher"],"section":"blog"},{"date":"1682985600","url":"/blog/colors-of-magic/","title":"Die Farben der Magie ***","summary":"Im ersten Band der Scheibenwelt-Reihe wird auf humorvolle Art liebevoll die Geschichte von Rincewind, einem Zauberer ohne abgeschlossenes Zauberstudium, Zweiblum, dem ersten Touristen, und anderen Weggefährten erzählt. Alle ernsthaftig aus anderen Vertretern des Genres wie dem Herr der Ringe oder dem Lied von Eis und Feuer fehlt dem Buch, was grundsätzlich nicht slecht sein soll. Die Art des Humors erinnert mich ein wenig an \u0026ldquo;Per Anhalter durch die Galaxie\u0026rdquo;, leider bin ich aber schon zu alt geworden oder die Witze sind nicht ganz so gelungen wie in eben diesem. Wer eine leichte Lektüre mit viel Phantasie und Witz sucht ist mit dem Titel trotzdem gut beraten.\n","content":"Im ersten Band der Scheibenwelt-Reihe wird auf humorvolle Art liebevoll die Geschichte von Rincewind, einem Zauberer ohne abgeschlossenes Zauberstudium, Zweiblum, dem ersten Touristen, und anderen Weggefährten erzählt. Alle ernsthaftig aus anderen Vertretern des Genres wie dem Herr der Ringe oder dem Lied von Eis und Feuer fehlt dem Buch, was grundsätzlich nicht slecht sein soll. Die Art des Humors erinnert mich ein wenig an \u0026ldquo;Per Anhalter durch die Galaxie\u0026rdquo;, leider bin ich aber schon zu alt geworden oder die Witze sind nicht ganz so gelungen wie in eben diesem. Wer eine leichte Lektüre mit viel Phantasie und Witz sucht ist mit dem Titel trotzdem gut beraten.\nInhalt Durch unglückliche Umstände wird Rincewind, ein Zauberer der nur einen einzigen Zauerspruch verinnerlicht hat, dazu verdonnert auf den Touristen Zweiblum acht zu geben, damit diesem nichts geschiet. Das erweist sich alles andere als einfach, da Zweiblum die gepflogenheiten der gefährlichen Scheibenwelt völlig unklar sind, er alles als ein Abenteuer betrachtet und dazu noch furchtbar naiv ist. Rincewind muss Zweiblum allerdings nicht allein beschützen, denn Zweiblums magische Truhe aus intelligentem Birnenbaumholz und versehen mit hunderten von Füßen ist ein großer Schutzengel der Beide vor viel Unheil bewahrt. Ob gewollt oder nicht bewegen sich die drei von Abenteur zu Abenteuer, an welchen auch die Götter nicht unbeteiligt sind. Die Reise endet im ersten Band am Rand der Welt und hält alles für weitere Fortsetzungen offen.\nWeitere Infos Originaltitel: The Colour of Magic Genre: Fantasy Erscheinungsjahr: 1983 Seitenzahl: 256 ISBN: 3-89897-529-0 Erhältlich in jeder Buchhandlung oder unter https://www.medimops.de/terry-pratchett-die-farben-der-magie-ein-roman-von-der-bizarren-scheibenwelt-terry-pratchetts-scheibenwelt-taschenbuch-M03492280625.html ","tags":["deutsch","Bücher"],"section":"blog"},{"date":"1681516800","url":"/blog/searxng/","title":"Deine eigene Suchmaschine mit SearXNG","summary":"Wir alle wissen um die Macht des Konzerns Alphabet, der hinter der Google Suchmaschine steht: Wir suchen eine bestimmte Information im Internet und ein Algorithmus entscheidet intransparent für uns, welche Ergebnisse uns präsentiert werden. Natürlich könnte man auf eine andere Suchmaschine ausweichen, aber Google ist doch so schön bequem…\nIn diesem Artikel beschreibe ich, wie man sich mit SearXNG selbst eine (Meta-)Suchmaschine aufsetzen kann um dem Dilemma zumindest ein Stück weit zu entkommen.\n","content":"Wir alle wissen um die Macht des Konzerns Alphabet, der hinter der Google Suchmaschine steht: Wir suchen eine bestimmte Information im Internet und ein Algorithmus entscheidet intransparent für uns, welche Ergebnisse uns präsentiert werden. Natürlich könnte man auf eine andere Suchmaschine ausweichen, aber Google ist doch so schön bequem…\nIn diesem Artikel beschreibe ich, wie man sich mit SearXNG selbst eine (Meta-)Suchmaschine aufsetzen kann um dem Dilemma zumindest ein Stück weit zu entkommen.\nSuchmaschine, Suchindex, Meta-Suchmaschine Das Herzstück einer jeden Suchmaschine ist ein Index, hier ein Verzeichnis aller bekannten Webseiten im Internet. Wie sich jeder vorstellen kann, ist der Aufbau eines Index für das Internet kein Leichtes unterfangen und die resultierende Datenbank gigantisch groß. Ebenso muss so ein Index ständig aktualisiert werden, damit es nicht zu toten Links in den Suchergebnissen kommt.\nDer Betreiber einer echte Suchmaschine muss also einen eigenen Index aufbauen und diese dazu gehörigen Aufwände stemmen. Deshalb gibt es weit weniger echte Suchmaschinen mit eigenen Index, als man vielleicht denkt. Ein Blick auf den entsprechenden Beitrag in Wikipedia offenbart uns, dass weltweit nur um die 10 Betreiber eines öffentlich Index gibt. Reduziert auf die für uns relevanten bleiben Alphabets Google, Microsofts Bing und 1-2 Exoten.\nAlleine einen eigenen Suchindex aufzubauen ist allerdings auch utopisch, weshalb jetzt Meta-Suchmaschinen ins Spiel kommen. Eine Meta-Suchmaschine nimmt unseren Suchbegriff, holt sich die Ergebnisse bei verschiedenen Suchmaschinenbetreibern ab und stellt uns die Ergebnisse übersichtlich dar.\nSearXNG \u0026lt;href=\u0026ldquo;https://docs.searxng.org/\u0026quot; target=\u0026quot;_blank\u0026rdquo;\u0026gt;SearXNG ist eine Open Source Projekt das uns eine Meta-Suchmaschine für den eigenen Betrieb oder in öffentlich gehosteten Implementierungen zur Verfügung stellt. Dadurch entsteht eben nicht nur der Vorteil der weniger limitierten Suchergebnisse, sondern zusätzlich auch ein zusätzlich grad an Anonymisierung. D.h. Google oder Microsoft können die Suchabfragen weder direkt eurem Benutzer als auch nicht eurer IP-Adresse zuordnen. Dieser Effekt wird umso größer, desto mehr Menschen die jeweilige SearXNG Instanz nutzen. Bei SearXNG handelt es sich um einen sogenannten Fork (Abspaltung/Vergabelung) von SearX, der aktiv gewartet und in vielen Punkten verbessert wurde.\nSearXNG in einer Docker Compose Umgebung betreiben Jetzt wollen wir nicht länger warten und SearXNG auf unserem eigenen Server starten. Wir gehen davon aus, dass wir Docker und einen Reverse Proxy bereits installiert haben. Falls das bei euch nicht der Fall ist schaut euch zunächst die entsprechenden Anleitung an. Eine einfache Docker Compose Defintion in der \u0026ldquo;docker-compose.yml\u0026rdquo; kann dann wie folgt aussehen:\n1services: 2 search-redis: 3 container_name: search-redis 4 image: \u0026#34;redis:alpine\u0026#34; 5 command: redis-server --save \u0026#34;\u0026#34; --appendonly \u0026#34;no\u0026#34; 6 networks: 7 - default 8 tmpfs: 9 - /var/lib/redis 10 cap_drop: 11 - ALL 12 cap_add: 13 - SETGID 14 - SETUID 15 - DAC_OVERRIDE 16 search-searxng: 17 container_name: search-searxng 18 image: searxng/searxng:latest 19 networks: 20 - default 21 - proxy 22 volumes: 23 - ./searxng:/etc/searxng:rw 24 environment: 25 - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-search.handtrixxx.com}/ 26 restart: unless-stopped 27 cap_drop: 28 - ALL 29 cap_add: 30 - CHOWN 31 - SETGID 32 - SETUID 33 - DAC_OVERRIDE 34 logging: 35 options: 36 max-size: \u0026#34;1m\u0026#34; 37 max-file: \u0026#34;1\u0026#34; 38networks: 39 proxy: 40 external: true Nach dem hochfahren erhalten wir also 2 Contianer, einen Redis Cache und den Applikationsserver von SearXNG. Entgegen der offiziellen Beispieldokumentation haben wir hier auf einen zusätzlichen Caddy Reverse Proxy verzichtet und auch keine Ports exponiert. Dafür liegt der Applikationscontainer zusätzlich im gleichen Netz (Proxy) wie unser generischer Reverse Proxy.\nFazit Wie aus der Konfiguration hervorgehen haben wir jetzt eine laufende Installation, welche über https://search.handtrixxx.com aus dem Internet erreichbar ist. Anschließend habe ich noch die Standardsuchengine in meinen lokalen Browsern so umgestellt, dass immer meine eigene Engine verwendet wird. Leider erlaubt Apple dies auf dem iPhone/iPad nicht. Gerne könnt ihr aber ebenfalls meine Instanz verwenden und so zur weiteren Anonymisierung beitragen oder euch eben selbst an einer Installation probieren.\n","tags":["deutsch"],"section":"blog"},{"date":"1677974400","url":"/blog/ncms/","title":"Noch ein Web Content Management System?! Ich stelle vor: nCMS","summary":"Warum sollte man sein eigenes Web Content Management System (WCMS) erstellen wollen, es gibt doch schon hunderte?! Völlig korrekt und es gibt auch keinen wirklichen Grund so etwas zeitintensives zu tun, außer natürlich man ist mit dem was es gibt nicht so richtig zufrieden und/oder möchte einfach selber wissen und ausprobieren wie man so etwas angehen kann. Genau mit dieser Motivation, und der ursprünglichen Intention für meine persönliche Website ein neues Layout zu entwickeln, habe ich mir innerhalb eines Monats ein eigenes WCMS erstellt: nCMS – “niklas stephan’s Content Management System” oder auch “node-red Content Management System” oder “not another Content Management System”!\n","content":"Warum sollte man sein eigenes Web Content Management System (WCMS) erstellen wollen, es gibt doch schon hunderte?! Völlig korrekt und es gibt auch keinen wirklichen Grund so etwas zeitintensives zu tun, außer natürlich man ist mit dem was es gibt nicht so richtig zufrieden und/oder möchte einfach selber wissen und ausprobieren wie man so etwas angehen kann. Genau mit dieser Motivation, und der ursprünglichen Intention für meine persönliche Website ein neues Layout zu entwickeln, habe ich mir innerhalb eines Monats ein eigenes WCMS erstellt: nCMS – “niklas stephan’s Content Management System” oder auch “node-red Content Management System” oder “not another Content Management System”!\nWas ist nCMS und was kann es? nCMS ist ein Flat-File und Headless Web Content Management System. D.h. im Gegensatz zu z.B. zu WordPress, kommt es ohne eine Datenbank aus und besteht aus Dateien. NCMS basiert im Backend auf Node-Red und darin entwickelten Node.js JavaScript Funktionen.\nÄnderungen am Code von NCMS werden durch Deployments in Node-Red geschrieben und die Änderungen an Dateien über GIT in Github synchronisiert und versioniert. Änderungen an Inhalten wie neue Posts werden ebenfalls über Deployments, manuell oder als Webhook gestartet. Alle Dateien werden darauf hin generiert und einem Webserver als statische Dateien zur Verfügung gestellt. Das heisst dass der Abruf der Inhate über eine mit nCMS erstellte Website rasend schnell ist. Das erstellen einzelner Blog Beiträge erfolgt in Markup Syntax in einem beliebigem Texteditor.\nWeitere bis jetzt integrierte Features sind:\nMulti-Language Support für Blog Beiträge und alle Seiten Kommentarfunktion in den Blogbeiträgen Media Management über einfachen Datei-Upload Template- und Snippet-basierte Erstellung des HTML Gerüsts Explizit kein Einsatz von Frontend Frameworks wie Vue oder Angular, sondern reines “Vanilla” Javascript. Automatische Generierung von Meta-Daten für Social Media Integration und SEO Volltextsuche auf Basis eines automatisch generierten lokalen Index Freigabefunktion von Posts über ein “published” Attribut Und viele mehr, dokumentiert im GitHub Repository des Projekts: https://github.com/handtrixx/ncms\nSchneller, aufgeräumter und einfacher als WordPress und andere? Ja, ja, ja. Wie genau, erläutere ich hier.\nSchnell Im Backend – Ein Deployment (erzeugen der statischen Files für den Webserver) dauert zwischen 70 und 320 Millisekunden.\nIm Frontend – z.B. die Startseite ist trotz aller Animationen, Effekte und Bilder nur 670KB groß und kann in zwischen 100 und 300 Millisekunden vom Client vollständig geladen werden.\nAufgeräumt Im Backend: Der Einsatz von Node-Red gibt eine grafische Übersicht und gewährleistet so, dass wir den Überblick nicht verlieren. Dazu später noch mehr.\nIm Frontend: Durch den Verzicht auf JavaScript Frameworks und der Einfachheit des Systems an sich, werden wir mit sauberen HTML Code bei der Ausspielung belohnt. Für https://niklas-stephan.de habe ich zwar für das Frontend UI auf das HTML5 Grundgerüst von Bootstrap 5 zurück gegriffen, aber das fällt nicht mehr schwer ins Gewicht, und kommt mittlerweile im Standard ebenfalls ohne das aufgeblähte jQuery Framework zurecht.\nEinfach Im Backend: Assets, Medien, Templates und Snippets werden über Visual Studio Code oder einen anderen Texteditor + Dateimanager bereitgestellt. In Node-Red wird das alles zusammengefügt, durch JavaScript Code und node.js Module um Funktionalitäten erweitert und das Endergebnis schließlich in Form von statischen Dateien in ein Verzeichnis auf einem nginx webserver zur Verfügung gestellt. Das ist der Kern von nCMS.\nIm Frontend: Frontend: Die Wahl zwischen Deutsch und Englisch erfolgt entweder automatisch oder wird manuell festegelegt. Es gibt eine Startseite, eine Seite zu Suche, eine Seite zu Datenschutz und Impressum, eine Seite zum abfangen ungültiger Aufrufe und eine Seite mit der Übersicht über alle Posts. Selsbstredend ist das komplette UI auf alle Endgerätearten zur Darstellung optimiert.\nHinter den Kulissen Der komplette Aufbau von nCMS erfolgt in einer Docker-Compose Umgebung. Alle Dateien aus dem Volume src liegen ebenfalls auf https://github.com/handtrixx/ncms.\nHier meine Beispielkonfiguration auf Basis des offiziellen Node-Red Images von Docker Hub (https://hub.docker.com/r/nodered/node-red):\n1services: 2 ncms: 3 container_name: ncms 4 hostname: ncms 5 image: nodered/node-red:latest 6 restart: always 7 environment: 8 - TZ=Europe/Berlin 9 networks: 10 - dmz 11 logging: 12 options: 13 max-size: \u0026#34;10m\u0026#34; 14 max-file: \u0026#34;3\u0026#34; 15 volumes: 16 - ./data:/data 17 - ./dist:/dist:rw 18 - ./src:/src:rw 19 - /etc/localtime:/etc/localtime:ro 20networks: 21 dmz: 22 external: true Erkärungsbedürftig sind hier eigentlich nur die Netzwerkonfiguration und die verschiedenen Volumes. Wie auch in meinen anderen Beiträgen zu Matomo, Boinc und anderen setze ich auf meinem Cloud Server einen Reverse Proxy auf Basis des Nginx Proxy Managers (https://hub.docker.com/r/jc21/nginx-proxy-manager) ein. Das Volume/Verzeichnis ./distist in meinem Reverse Proxy ebenfalls über ein ln -s verlinkt, so dass man keine doppelten Deployments machen muss und ein weiterer Webserver obsolet bleibt. Node-Red selbst, also das Backend ist über eine eigene Subdomain erreichbar.\nVerzeichnisstruktur Im Verzeichnis ./src befinden sich folgende Unterordner:\nassets css fonts img js json md posts media x snippets templates In das Verzeichnis assets und dessen Unterordner gehören alle im Frontend wiederholt benötigten Dateien, wie z.B. css und javascript von Bootstrap 5 (https://getbootstrap.com/), aber natürlich vor allem auch eigene Stylesheets und Javascript Funktionen. Im Unterordner json legen wir unsere statischen Übersetzungschlüssel ab.\nDer Ordner md enthält in seinem Unterordner posts offensichtlich alle unsere in Markup geschriebenen Posts, welche auch an ihrer Dateiendung .md erkennbar sind.\nIn das Verzechnis media und maximal eine Unterodnerebene tiefer können wir alle Bilder “werfen” die wir in unseren Posts verwenden wollen. Diese werden dann während des Deployments automatisch ins platzsparende .webp Format konvertiert und ein Thumbnail für jedes Bild generiert.\nIm Ordner snippteshaben wir alle HTML Elemente die wir auf allen Seiten immer wieder benötigen gelegt.\nAnalog dazu liegen im Verzeichnis templates die Ausgangsdateien unseres Frontends, die während des Deployments mit Inhalt angereichtert werden.\nAll das lässt sich Transparent auch im Github Projekt unter: https://github.com/handtrixx/ncms nachvollziehen.\nTemplates Im Verzeichnis Templates befinden sich folgende Dateien: 404.html – Die Fehlerseite die immer dann angezeigt wird, wenn eine ungültige Abfrage auf die Website erfolgt. blog.html – Die Seite die die Übersicht über alle Posts bereit stellt. index.html – Die Startseite mit ihren Inhalten. post.html – Die Vorlage aus der die einzelen Beitragsseiten gerendert werden. privacy-policy.html – Die Seite zu Datenschutz und im Impressum auf die im Footer verllinkt ist und die somit von überall aus erreichbar ist und sein muss. robots.txt – Infos für die Crawler von Suchmaschinen. search.html – Meine Seite auf der man Suchen kann und die über einen json Index sämtliche Suchergebnisse ohne Abfrage am Server bereitstellt.\nSnippets Die Snippets die später im Deployment in alle HTML Templates eingeschläust werden sind footer.html, head.html, navbar.html, script.html . Durch die Aufteilung in diese Snippets haben wir den massiven Vorteil, dass wir im Falle einer gewünschten Anpassung z.B. im Navigationsmenü, diese nur genau einmal durchführen müssen um sie auf allen Seiten zu ändern.\nProgrammlogik mit node.js und Node-Red Node-Red basiert auf node.js und erlaubt uns in einer Art Ablaufdiagramm verschiedene Elemente und Funktionen miteinander zu verbinden. Das nennt sich in Node-Red “Flow”. In NCMS nutze ich auch nur diese Basisfunktionalität von Node-Red und keine weitere Plug-Ins aus der Palette. Stattdessen lade ich in den verschiedenen Nodes node.js Pakete nach um den Funktionsumfang des Systems zu erweitern.\nIm folgenden eine Erläuterung zu den einzelen Nodes.\n/deploy Hierbei handelt sich um einen lauschenden http in Node, um den Start eines Deplyoments durch einen Webhook zu starten. D.h. z.B. durch curl -X POST -d \u0026lsquo;key=\u0026mdash;-\u0026rsquo; https://ncms.niklas-stephan.de/deploy startet man das Deployment.\ncatch key Natürlich soll nicht einfach jeder ein Deployment starten können, deshalb noch eine kleine Sicherheitsabfrage im catch key Node.\nIm Node unter “Setup” wird das npm Modul fs-extra geladen und als fse bereit gestellt, damit wir Zugriff auf das Dateisystem haben und den im Ordner /data hinterlegten key mit dem vergleichen können, der uns für das Deployment im Webhook zur Verfügung gestellt wurde.\nDie Funktion selbst sieht dann so aus:\n1const transferedKey = msg.payload.key; 2const systemKey = fse.readFileSync(\u0026#39;/data/deploy.key\u0026#39;, \u0026#39;utf8\u0026#39;) 3 4if (transferedKey == systemKey) { 5 msg.payload = \u0026#34;Deployment Started\u0026#34;; 6 msg.statusCode = 200; 7 msg.type = \u0026#34;webhook\u0026#34;; 8 msg.starttime = Date.now(); 9 return [null,msg]; 10} else { 11 msg.payload = \u0026#34;Wrong authentication!\u0026#34; 12 msg.statusCode = 400; 13 return [msg,null]; 14} Auch hat dieser Node 2 Ausgänge. Falls die beiden keys übereinstimmen wird mit dem Deployment fortgefahren – Ausgang 2. Falls aber nicht, dann wird eine Fehlernachricht an Ausgang 1 übergeben und das Deployment damit abgebrochen.\ndeploy Startet das Deployment ebenfalls, aber eben manuell über die Node-Red Oberfläche und nicht als Webhook.\nget posts In diesem Node laden wir den Inhalt aller Posts aus den *.md Dateien und speichern diesen als Objekte in einem Array zu späteren Verwendung ab. Außerdem machen wir neben dem fs-extra Modul noch intensiven Gebrauch des Moduls markdown-it und Plugins für diesen. markdown-it (https://github.com/markdown-it/markdown-it) hilft uns dabei den Inhalt von Markup nach HTML zu konvertieren.\n1msg.baseurl = \u0026#34;https://niklas-stephan.de\u0026#34; 2msg.dist = {}; 3msg.posts = []; 4const path = \u0026#39;/src/md/posts/\u0026#39;; 5const postfiles = fse.readdirSync(path) 6const alength = postfiles.length; 7 8for (var i=0; i\u0026lt;alength; i++) { 9 var srcFile = path+postfiles[i]; 10 var distFilename = postfiles[i].split(\u0026#39;.\u0026#39;)[0]+\u0026#34;.html\u0026#34;; 11 var srcContent = fse.readFileSync(srcFile, \u0026#39;utf8\u0026#39;) 12 13 var md = new markdownIt({ 14 html: true,linkify: true,typographer: true,breaks: true}) 15 .use(markdownItFrontMatter, function(metainfo) {meta = JSON.parse(metainfo);}) 16 .use(markdownItLinkifyImages, {target: \u0026#39;_blank\u0026#39;,linkClass: \u0026#39;custom-link-class\u0026#39;,imgClass: \u0026#39;custom-img-class\u0026#39;}) 17 .use(markdownItLinkAttributes, { attrs: {target: \u0026#34;_blank\u0026#34;,rel: \u0026#34;noopener\u0026#34;,} 18 }); 19 20 distContent = md.render(srcContent); 21 22 let data = {\u0026#34;srcFile\u0026#34;:\u0026#34;\u0026#34;+srcFile+\u0026#34;\u0026#34;,\u0026#34;srcContent\u0026#34;:\u0026#34;\u0026#34;+srcContent+\u0026#34;\u0026#34;,\u0026#34;distContent\u0026#34;:\u0026#34;\u0026#34;+distContent+\u0026#34;\u0026#34;,\u0026#34;distFilename\u0026#34;:\u0026#34;\u0026#34;+distFilename+\u0026#34;\u0026#34;,...meta}; 23 24 msg.posts.push(data) 25} 26 27return msg; get snippets Über das npm Modul fs-extra laden wir den Inhalt unserer snippets und speichern diese als Array msg.snippets, damit wir später im Flow darauf zugreifen können.\n1msg.snippets = {}; 2const path = \u0026#39;/src/snippets/\u0026#39;; 3const files = fse.readdirSync(path) 4const alength = files.length; 5 6for (var i=0; i\u0026lt;alength; i++) { 7 var srcFile = path+files[i]; 8 var srcContent = fse.readFileSync(srcFile, \u0026#39;utf8\u0026#39;) 9 10 msg.snippets[files[i]] = srcContent; 11} 12 13return msg; get templates Genauso wie die Snippets speichern wir auch den Inhalt der Templates in unserem Flow zur späteren Verwendung.\n1msg.templates = {}; 2const path = \u0026#39;/src/templates/\u0026#39;; 3const files = fse.readdirSync(path) 4const alength = files.length; 5 6for (var i=0; i\u0026lt;alength; i++) { 7 var srcFile = path+files[i]; 8 var srcContent = fse.readFileSync(srcFile, \u0026#39;utf8\u0026#39;) 9 10 msg.templates[files[i]] = srcContent; 11} 12 13return msg; create index.html Nun fangen wir an die Inhalte der einzelnen Dateien zu generieren, den Start macht unsere index.html Datei. Die obere Hälfte des Codes betrifft das generieren der Meta Tags für Social Media und Suchmaschinen. Im zweiten Block fügen wir in die Platzhalter des Templates die Werte Snippets, den Seitentitel, sowie die Metadaten ein. Letztlich steht unsere fertige index.html als Objekt msg.dist.index in unserem Flow bereit um später als Datei geschrieben zu werden.\n1msg.dist.index = \u0026#34;\u0026#34;; 2var ogmetalang = \u0026#34;de_DE\u0026#34;; 3var ogmeta = ` 4\u0026lt;meta property=\u0026#34;og:type\u0026#34; content=\u0026#34;website\u0026#34;\u0026gt; 5\u0026lt;meta property=\u0026#34;og:locale\u0026#34; content=\u0026#34;`+ogmetalang+`\u0026#34;\u0026gt; 6\u0026lt;meta property=\u0026#34;og:site_name\u0026#34; content=\u0026#34;niklas-stephan.de\u0026#34;\u0026gt; 7\u0026lt;link rel=\u0026#34;canonical\u0026#34; href=\u0026#34;`+msg.baseurl+`/index.html\u0026#34;\u0026gt; 8\u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 9\u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;Projects \u0026amp; Blog - niklas-stephan.de\u0026#34;\u0026gt; 10\u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 11\u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;`+msg.baseurl+`/index.html\u0026#34;\u0026gt; 12\u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt; 13\u0026lt;meta property=\u0026#34;og:image:secure_url\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt; 14\u0026lt;meta name=\u0026#34;twitter:card\u0026#34; content=\u0026#34;summary\u0026#34;\u0026gt; 15\u0026lt;meta name=\u0026#34;twitter:description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 16\u0026lt;meta name=\u0026#34;twitter:title\u0026#34; content=\u0026#34;Projects \u0026amp; Blog - niklas-stephan.de\u0026#34;\u0026gt; 17\u0026lt;meta name=\u0026#34;twitter:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt;` 18 19msg.dist.index = msg.templates[\u0026#34;index.html\u0026#34;].replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 20msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 21msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 22msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 23msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,\u0026#34;Home\u0026#34;); 24msg.dist.index = msg.dist.index.replace(\u0026#34;\u0026lt;!-- meta tags --\u0026gt;\u0026#34;,ogmeta); 25 26return msg; create 404.html Die 404.html ist schnell zusammen gebaut. wir fügen alle Snippets in unser Template ein und geben der Seite einen Namen. Abschließend steht unser Objekt als msg.dist.errorpage zur Verfügung.\ncreate privacy-policy.html Genauso ein “Low-Brainer” ist die Seite mit Datenschutz und Impressum und schnell als msg.dist.privacy aufbereitet.\n1msg.dist.privacy = \u0026#34;\u0026#34;; 2 3msg.dist.privacy = msg.templates[\u0026#34;privacy-policy.html\u0026#34;].replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 4msg.dist.privacy = msg.dist.privacy.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 5msg.dist.privacy = msg.dist.privacy.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 6msg.dist.privacy = msg.dist.privacy.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 7msg.dist.privacy = msg.dist.privacy.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,\u0026#34;Datenschutz \u0026amp; Impressum\u0026#34;); 8 9return msg; create search.html Bevor es wieder ein wenig komplizierter wird, zunächst noch die einfach Erzeugung des Objekts msg.dist.searchindex zur späteren Verwendung.\n1msg.dist.search = \u0026#34;\u0026#34;; 2 3msg.dist.search = msg.templates[\u0026#34;search.html\u0026#34;].replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 4msg.dist.search = msg.dist.search.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 5msg.dist.search = msg.dist.search.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 6msg.dist.search = msg.dist.search.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 7msg.dist.search = msg.dist.search.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,\u0026#34;Suche\u0026#34;); 8 9return msg; create search index.json Um den Suchindex aufzubauen, der später eine Suche ermöglicht ohne eine Abfrage an den Server zu stellen, verwenden ich eine for Schleife die durch alle Elemente im Array msg.posts läuft und für jeden Beitrag einen Eintrag im Index als JSON Objekt erzeugt. Letztlich wird der Index als Objekt msg.dist.searchindex bereit gestellt.\n1var alength = msg.posts.length; 2var index = \u0026#34;[\u0026#34;; 3 4for (var i=0; i\u0026lt;alength; i++) { 5 index = index+`{\u0026#34;lang\u0026#34;:\u0026#34;`+msg.posts[i].language+`\u0026#34;,\u0026#34;link\u0026#34;:\u0026#34;/posts/`+msg.posts[i].distFilename+`\u0026#34;,\u0026#34;headline\u0026#34;:\u0026#34;`+msg.posts[i].title+`\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;`+msg.posts[i].distContent.replace(/[^a-zA-Z0-9]/g, \u0026#39; \u0026#39;)+`\u0026#34;},`; 6} 7index = index.slice(0, -1); 8index = index+\u0026#34;]\u0026#34;; 9 10msg.dist.searchindex = index; 11 12return msg; create sitemap.xml Bei der Erzeugung der Sitemap gehen wir ähnlich vor wie beim Suchindex. Anstelle einer klassischen for Schleife verwende ich die Javascript forEach() Funktion, die im Endeffekt das gleiche bewirkt, nur etwas moderner ist.\n1var xml = `\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; 2\u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\u0026#34;\u0026gt; 3 \u0026lt;url\u0026gt; 4 \u0026lt;loc\u0026gt;`+msg.baseurl+`/\u0026lt;/loc\u0026gt; 5 \u0026lt;priority\u0026gt;1.00\u0026lt;/priority\u0026gt; 6 \u0026lt;/url\u0026gt; 7 \u0026lt;url\u0026gt; 8 \u0026lt;loc\u0026gt;`+msg.baseurl+`/index.html\u0026lt;/loc\u0026gt; 9 \u0026lt;priority\u0026gt;0.80\u0026lt;/priority\u0026gt; 10 \u0026lt;/url\u0026gt; 11 \u0026lt;url\u0026gt; 12 \u0026lt;loc\u0026gt;`+msg.baseurl+`/blog.html\u0026lt;/loc\u0026gt; 13 \u0026lt;priority\u0026gt;0.80\u0026lt;/priority\u0026gt; 14 \u0026lt;/url\u0026gt;` 15 16msg.posts.forEach(postxml); 17 18xml = xml + ` 19\u0026lt;/urlset\u0026gt;` 20msg.dist.sitemap = xml; 21return msg; 22 23 24function postxml(item) { 25 if (item.published == true ) { 26 xml = xml + ` 27 \u0026lt;url\u0026gt; 28 \u0026lt;loc\u0026gt;`+msg.baseurl+`/posts/`+item.distFilename+`.html\u0026lt;/loc\u0026gt; 29 \u0026lt;priority\u0026gt;0.64\u0026lt;/priority\u0026gt; 30 \u0026lt;/url\u0026gt;` 31 } 32} create posts Wir haben zur im get posts Node bereits den Inhalt der einzelnen Posts im Array msg.posts konvertiert und bereit gestellt. Jetzt wollen wir dies noch dahingehend finalisieren, dass wir analog zu den zuvor erzeugten Dateien auch für jeden Post eine einzelne Datei erzeugen können und lege diese wiederum als Objekt im Objekt msg.dist.posts ab.\n1msg.dist.posts = {}; 2var alength = msg.posts.length; 3var data = \u0026#34;\u0026#34;; 4var ogmetalang = \u0026#34;\u0026#34;; 5var ogmeta = \u0026#34;\u0026#34;; 6var postdate = \u0026#34;\u0026#34;; 7 8for (var i=0; i\u0026lt;alength; i++) { 9 10 if (msg.posts[i].language == \u0026#34;de\u0026#34;) { 11 ogmetalang = \u0026#34;de_DE\u0026#34; 12 } else { 13 ogmetalang = \u0026#34;en_US\u0026#34; 14 } 15 16 img = msg.posts[i].imgurl.split(\u0026#39;.\u0026#39;)[0]+\u0026#34;.webp\u0026#34;; 17 18 19 ogmeta = ` 20 \u0026lt;meta property=\u0026#34;og:type\u0026#34; content=\u0026#34;website\u0026#34;\u0026gt; 21 \u0026lt;meta property=\u0026#34;og:locale\u0026#34; content=\u0026#34;`+ogmetalang+`\u0026#34;\u0026gt; 22 \u0026lt;meta property=\u0026#34;og:site_name\u0026#34; content=\u0026#34;niklas-stephan.de\u0026#34;\u0026gt; 23 \u0026lt;link rel=\u0026#34;canonical\u0026#34; href=\u0026#34;`+msg.baseurl+`/posts/`+msg.posts[i].language+`/`+msg.posts[i].distFilename+`\u0026#34;\u0026gt; 24 \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;`+msg.posts[i].excerpt+`\u0026#34;\u0026gt; 25 \u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;`+msg.posts[i].title+`\u0026#34;\u0026gt; 26 \u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;`+msg.posts[i].excerpt+`\u0026#34;\u0026gt; 27 \u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;`+msg.baseurl+`/posts/`+msg.posts[i].language+`/`+msg.posts[i].distFilename+`\u0026#34;\u0026gt; 28 \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/media/full/`+img+`\u0026#34;\u0026gt; 29 \u0026lt;meta property=\u0026#34;og:image:secure_url\u0026#34; content=\u0026#34;`+msg.baseurl+`/media/full/`+img+`\u0026#34;\u0026gt; 30 \u0026lt;meta name=\u0026#34;twitter:card\u0026#34; content=\u0026#34;summary\u0026#34;\u0026gt; 31 \u0026lt;meta name=\u0026#34;twitter:description\u0026#34; content=\u0026#34;`+msg.posts[i].excerpt+`\u0026#34;\u0026gt; 32 \u0026lt;meta name=\u0026#34;twitter:title\u0026#34; content=\u0026#34;`+msg.posts[i].title+`\u0026#34;\u0026gt; 33 \u0026lt;meta name=\u0026#34;twitter:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/media/full/`+img+`\u0026#34;\u0026gt;` 34 35 postdate = \u0026#39;\u0026lt;small class=\u0026#34;c-gray pb-3\u0026#34; id=\u0026#34;postdate\u0026#34;\u0026gt;\u0026#39;+msg.posts[i].date+\u0026#39;\u0026lt;/small\u0026gt;\u0026#39;; 36 37 data = \u0026#34;\u0026#34;; 38 data = msg.templates[\u0026#34;post.html\u0026#34;]; 39 40 41 data = data.replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 42 data = data.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 43 data = data.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 44 data = data.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 45 46 data = data.replace(\u0026#34;\u0026lt;!-- mardown content from posts --\u0026gt;\u0026#34;,msg.posts[i].distContent); 47 data = data.replace(\u0026#34;\u0026lt;!-- Post Headline --\u0026gt;\u0026#34;, msg.posts[i].title); 48 data = data.replace(\u0026#34;\u0026lt;!-- postdate --\u0026gt;\u0026#34;, postdate); 49 data = data.replace(\u0026#34;\u0026lt;!-- Post Image --\u0026gt;\u0026#34;, \u0026#39;\u0026lt;img src=\u0026#34;/media/thumb/\u0026#39;+img+\u0026#39;\u0026#34; class=\u0026#34;img-fluid mb-2\u0026#34; alt=\u0026#34;postImage\u0026#34;\u0026gt;\u0026#39;); 50 data = data.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,msg.posts[i].title); 51 data = data.replace(\u0026#34;\u0026lt;!-- meta tags --\u0026gt;\u0026#34;,ogmeta); 52 53 msg.dist.posts[msg.posts[i].distFilename] = data; 54} 55 56return msg; create blog.html Nun unser letzter vorbereitender Streich, die Erzeungung der Übersicht aller Posts im Objekt msg.dist.blog. Es ist am Umfang der Funktion ersichtlich, dass hier etwas mehr als bei den anderen Dateien passiert.\nZuerst sammeln wir alle definierten Kategorien aus den einzelnen Posts über eine for Schleife ein um dann doppelt vorhandene Werte wieder aus dem erzeugtem Array zu löschen. Das brauchen wir damit die Besucher unserer Website später über Kategorien filtern können. Jede Kategorie bekommt außerdem eine eindeutige Farbe, die einer CSS Klasse in unserem Stylesheet entspricht, zugeordnet.\nAls nächstes extrahieren wir aus den Metadaten der Posts noch Titel, Kurzbeschreibung, Datum und Bild. Während dieser for-Schleife verhindert eine if-Bedingung, dass wir unveröffentliche Posts zur Auswahl aufbereiten.\nAbschließend erzeugen wir aus den ermittelten Werten den entsprechenden HTML Code, fügen die Daten der Snippets ein, ergänzen die Meta Informationen zur Seite und schreiben das Ganze in das Objekt msg.dist.blog.\n1//get categories from all posts and extract unique ones 2var categories = []; 3for (var i=0 ; i\u0026lt;msg.posts.length;i++) { 4 if (msg.posts[i].published == true) { 5 for (var j = 0; j \u0026lt; msg.posts[i].keywords.length; j++) { 6 categories.push(msg.posts[i].keywords[j]); 7 } 8 } 9} 10var uniqueCategories = [...new Set(categories)]; 11 12// define a color to stick for each category 13const colorcat = {}; 14var catcolors = [\u0026#34;green\u0026#34;, \u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;yellow\u0026#34;, \u0026#34;pink\u0026#34;, \u0026#34;purple\u0026#34;,\u0026#34;indigo\u0026#34;]; 15var c=0; 16 for (const key of uniqueCategories) { 17 colorcat[key] = catcolors[c]; 18 c = c+1; 19 } 20 21 22//generate and set html for categorie selection 23var cathtml = \u0026#34;\u0026#34;; 24for (var k = 0; k \u0026lt; uniqueCategories.length; k++) { 25 26 cathtml = cathtml + `\u0026lt;button data-filter=\u0026#34;.cat-`+uniqueCategories[k]+`\u0026#34; type=\u0026#34;button\u0026#34; 27 onclick=\u0026#34;sort()\u0026#34; class=\u0026#34;btn bg-` + catcolors[k] + ` c-white me-2\u0026#34;\u0026gt;` + uniqueCategories[k]+`\u0026lt;/button\u0026gt;`; 28} 29msg.dist.blog = msg.templates[\u0026#34;blog.html\u0026#34;].replace(\u0026#34;\u0026lt;!-- CATEGORIES --\u0026gt;\u0026#34;, cathtml); 30 31 32// get card content from all posts and generate html 33var posthtml = \u0026#34;\u0026#34;; 34for (var l = 0; l \u0026lt; msg.posts.length; l++) { 35 if (msg.posts[l].published == true) { 36 37 var link = msg.posts[l].filename.slice(0, -3)+\u0026#34;.html\u0026#34;; 38 39 //get color for current post 40 var postcolor = \u0026#34;\u0026#34;; 41 for (const key in colorcat) { 42 if (key == msg.posts[l].keywords[0]) { 43 postcolor = colorcat[key]; 44 } 45 } 46 47 imgurl = msg.posts[l].imgurl.split(\u0026#39;.\u0026#39;)[0]+\u0026#34;.webp\u0026#34;; 48 49 50 posthtml = posthtml + ` 51 \u0026lt;div class=\u0026#34;col-sm-6 col-lg-4 my-4 filterDiv cat-`+msg.posts[l].keywords[0]+` lang-`+ msg.posts[l].language + `\u0026#34;\u0026gt; 52 \u0026lt;span class=\u0026#34;date hidden d-none\u0026#34;\u0026gt;`+ msg.posts[l].date + `\u0026lt;/span\u0026gt; 53 \u0026lt;span class=\u0026#34;name hidden d-none\u0026#34;\u0026gt;`+ msg.posts[l].title + `\u0026lt;/span\u0026gt; 54 \u0026lt;div onclick=\u0026#34;goto(\u0026#39;`+ link+ `\u0026#39;,\u0026#39;blog\u0026#39;)\u0026#34; class=\u0026#34;card h-100 d-flex align-items-center bo-`+postcolor+`\u0026#34;\u0026gt; 55 \u0026lt;div class=\u0026#34;card-header bg-`+ postcolor + `\u0026#34;\u0026gt;` + msg.posts[l].keywords[0] + `\u0026lt;/div\u0026gt; 56 \u0026lt;div class=\u0026#34;card-img-wrapper d-flex align-items-center\u0026#34;\u0026gt; 57 \u0026lt;img src=\u0026#34;media/thumb/`+ imgurl + `\u0026#34; 58 class=\u0026#34;card-img-top\u0026#34; alt=\u0026#34;iot\u0026#34;\u0026gt; 59 \u0026lt;/div\u0026gt; 60 \u0026lt;div class=\u0026#34;card-body\u0026#34;\u0026gt; 61 \u0026lt;h5 class=\u0026#34;card-title\u0026#34;\u0026gt;`+ msg.posts[l].title + `\u0026lt;/h5\u0026gt; 62 \u0026lt;p class=\u0026#34;card-text\u0026#34;\u0026gt; 63 `+ msg.posts[l].excerpt + ` 64 \u0026lt;/p\u0026gt; 65 \u0026lt;/div\u0026gt; 66 \u0026lt;div class=\u0026#34;card-footer small text-center c-gray pdate\u0026#34;\u0026gt; 67 `+msg.posts[l].date+` 68 \u0026lt;/div\u0026gt; 69 \u0026lt;/div\u0026gt; 70 \u0026lt;/div\u0026gt; 71 `; 72 } 73} 74 75msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- POSTS --\u0026gt;\u0026#34;, posthtml); 76 77var ogmetalang = \u0026#34;de_DE\u0026#34;; 78var ogmeta = ` 79\u0026lt;meta property=\u0026#34;og:type\u0026#34; content=\u0026#34;website\u0026#34;\u0026gt; 80\u0026lt;meta property=\u0026#34;og:locale\u0026#34; content=\u0026#34;`+ogmetalang+`\u0026#34;\u0026gt; 81\u0026lt;meta property=\u0026#34;og:site_name\u0026#34; content=\u0026#34;niklas-stephan.de\u0026#34;\u0026gt; 82\u0026lt;link rel=\u0026#34;canonical\u0026#34; href=\u0026#34;`+msg.baseurl+`/blog.html\u0026#34;\u0026gt; 83\u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 84\u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;Projects \u0026amp; Blog - niklas-stephan.de\u0026#34;\u0026gt; 85\u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 86\u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;`+msg.baseurl+`/blog.html\u0026#34;\u0026gt; 87\u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt; 88\u0026lt;meta property=\u0026#34;og:image:secure_url\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt; 89\u0026lt;meta name=\u0026#34;twitter:card\u0026#34; content=\u0026#34;summary\u0026#34;\u0026gt; 90\u0026lt;meta name=\u0026#34;twitter:description\u0026#34; content=\u0026#34;Projekte und Posts aus der Welt von IoT, Musik und mehr\u0026#34;\u0026gt; 91\u0026lt;meta name=\u0026#34;twitter:title\u0026#34; content=\u0026#34;Projects \u0026amp; Blog - niklas-stephan.de\u0026#34;\u0026gt; 92\u0026lt;meta name=\u0026#34;twitter:image\u0026#34; content=\u0026#34;`+msg.baseurl+`/assets/img/me_logo.webp\u0026#34;\u0026gt;` 93 94msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- meta tags --\u0026gt;\u0026#34;,ogmeta); 95msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- html head from head.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;head.html\u0026#34;]); 96msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- Top Navigation from navbar.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;navbar.html\u0026#34;]); 97msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- footer navigation from footer.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;footer.html\u0026#34;]); 98msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- Javascript from script.html snipppet --\u0026gt;\u0026#34;,msg.snippets[\u0026#34;script.html\u0026#34;]); 99msg.dist.blog = msg.dist.blog.replace(\u0026#34;\u0026lt;!-- PAGE TITLE --\u0026gt;\u0026#34;,\u0026#34;Projekte \u0026amp; Blog\u0026#34;); 100 101return msg; write files Jetzt wollen wir endlich unsere mühevoll erzeugten Objekte in msg.dist als reale Dateien festschreiben. In unserer Funktion “write files” binden wir dafür wieder das npm modul fs-extra als fse ein.\nAlle erzeugten Dateien sollem im Verzeichnis /dist/ landen. Gleichzeitig wollen wir auch alle spuren vorheriger Deployments löschen, damit uns “Dateileichen” und ähnliche nicht zu Inkonsitenzen führen.\nDas den fse Funktionen vorangestellte await stellt eine sequentielle Ausführung der einzelnen Schritte sicher, in dem es wartet bis der jeweilige Aufruf auch komplett abgeschlossen ist. Also wird in der Funktion im ersten Schritt das Verzeichnes /dist komplett geleert und dann alle benötigten Unterordner wieder leer erstellt.\nAls nächstes kopieren wir unsere Assets von /src/assets/ nach /dist/assets/ und machen das Gleiche mit der robots.txt Datei.\nAnschließend schreiben wir die Inhalte der Objekte in msg.dist in die jeweilige Datei fest, um dann in einer Schleife durch alle Posts zu gehen und diese ebenfalls in das Dateisystem zu schreiben.\n1await fse.emptyDir(\u0026#39;/dist\u0026#39;).then(() =\u0026gt; { 2 fse.mkdirSync(\u0026#39;/dist/media\u0026#39;); 3 fse.mkdirSync(\u0026#39;/dist/media/full\u0026#39;); 4 fse.mkdirSync(\u0026#39;/dist/media/thumb\u0026#39;); 5 fse.mkdirSync(\u0026#39;/dist/posts\u0026#39;); 6}); 7 8await fse.copySync(\u0026#39;/src/assets/\u0026#39;, \u0026#39;/dist/assets/\u0026#39;); 9await fse.copySync(\u0026#39;/src/templates/robots.txt\u0026#39;, \u0026#39;/dist/robots.txt\u0026#39;); 10 11await fse.writeFileSync(\u0026#39;/dist/index.html\u0026#39;, msg.dist.index); 12await fse.writeFileSync(\u0026#39;/dist/blog.html\u0026#39;, msg.dist.blog); 13await fse.writeFileSync(\u0026#39;/dist/404.html\u0026#39;, msg.dist.errorpage); 14await fse.writeFileSync(\u0026#39;/dist/privacy-policy.html\u0026#39;, msg.dist.privacy); 15await fse.writeFileSync(\u0026#39;/dist/search.html\u0026#39;, msg.dist.search); 16await fse.writeFileSync(\u0026#39;/dist/sitemap.xml\u0026#39;, msg.dist.sitemap); 17await fse.writeFileSync(\u0026#39;/dist/searchindex.json\u0026#39;, msg.dist.searchindex); 18 19const postfilefolder = \u0026#34;/dist/posts/\u0026#34; 20 21await Object.entries(msg.dist.posts).forEach(item =\u0026gt; { 22 var [key, value] = item; 23 fse.writeFileSync(postfilefolder+key, value) 24}); 25 26return msg; convert media files Noch etwas komplexer ist die Erzeugung der Mediendatein. Die Dateien aus dem Verzeichnis /src/media/ und einer Unterordnerebene tiefer, wollen wir ins Speicher sparende .webp Format konvertieren und zusätzlichen jeweils einen Thumbnail in geringerer Auflösung generieren. Außerdem wollen wir das nur Medien mit den Quellformaten .jpg, .jpeg, .png oder .gif konvertiert werden. Alle anderen Dateien werden ohne Änderung direkt nach /dist/media/full/ kopiert. Um uns die Arbeit zu erleichten greifen wir auf die npm Module fs-extra als fse und sharp zurück.\nMan beachte auch, das ich das await Kommando bewusst bei der Erzeugung der Dateien ausspare, so dass die Schleifen bereits mit der nächsten Datei aus ihren Arrays starten, bevor die Schreiboperation abgeschlossen ist. Das beschleunigt das Deployment um einen sehr großen Faktor, bei dem geringen Risiko bzw. dem akzeptieren Umstand, dass eine Mediendatei noch nicht geschrieben/verfügbar ist, wenn das Deplyoment abgeschlossen ist.\n1var srcpath = \u0026#39;/src/media/\u0026#39;; 2var fullpath = \u0026#39;/dist/media/full/\u0026#39;; 3var thumbpath = \u0026#39;/dist/media/thumb/\u0026#39;; 4const filetypes = [\u0026#34;jpg\u0026#34;, \u0026#34;jpeg\u0026#34;, \u0026#34;png\u0026#34;, \u0026#34;gif\u0026#34;, \u0026#34;webp\u0026#34;]; 5const mediafiles = fse.readdirSync(srcpath); 6const alength = mediafiles.length; 7var mediafile = \u0026#34;\u0026#34;; 8var targetfile = \u0026#34;\u0026#34;; 9 10for (var i=0; i\u0026lt;alength; i++) { 11 mediafile = mediafiles[i]; 12 13 if (mediafile.includes(\u0026#34;.\u0026#34;)) { 14 targetfile = mediafile.split(\u0026#39;.\u0026#39;)[0]; 15 if (filetypes.includes(mediafile.split(\u0026#39;.\u0026#39;)[1]) ) { 16 sharp(srcpath+mediafile) 17 .toFile(fullpath+targetfile+\u0026#39;.webp\u0026#39;); 18 sharp(srcpath+mediafile).resize({ width: 440 }) 19 .toFile(thumbpath+targetfile+\u0026#39;.webp\u0026#39;); 20 } else { 21 fse.copySync(srcpath+mediafile, fullpath+mediafile) 22 } 23 24 } else { 25 var subsrcpath = srcpath+mediafile+\u0026#34;/\u0026#34;; 26 var subfullpath = fullpath+mediafile+\u0026#34;/\u0026#34;; 27 var subthumbpath = thumbpath+mediafile+\u0026#34;/\u0026#34;; 28 var submediafiles = fse.readdirSync(subsrcpath); 29 var blength = submediafiles.length; 30 await fse.mkdirSync(subfullpath); 31 await fse.mkdirSync(subthumbpath); 32 33 for (var j=0; j\u0026lt;blength; j++) { 34 submediafile = submediafiles[j]; 35 subtargetfile = submediafile.split(\u0026#39;.\u0026#39;)[0]; 36 37 if (filetypes.includes(submediafile.split(\u0026#39;.\u0026#39;)[1]) ) { 38 sharp(subsrcpath+submediafile) 39 .toFile(subfullpath+subtargetfile+\u0026#39;.webp\u0026#39;); 40 41 sharp(subsrcpath+submediafile).resize({ width: 440 }) 42 .toFile(subthumbpath+subtargetfile+\u0026#39;.webp\u0026#39;); 43 } else { 44 fse.copySync(subsrcpath+submediafile, subfullpath+submediafile) 45 } 46 } 47 } 48} 49 50return msg; finish Unser Deployment nähert sich dem Ende. Wir errechnen noch die Dauer des Deployments und setzen einen Zeitstempel um diese Informationen in der Datei /dist/deploy.log festzuhalten. Die Datei wird dann wieder mit dem npm Modul fs-extra als fse geschrieben. Je nach Auslöser des Depyloyments, also manuell vs. webhook, wird dann abschließnd an ein Debug Node oder an den Debug Node und einen http-out Node weitergeleitet.\n1var endtime = Date.now(); 2var duration = endtime-msg.starttime; 3duration = duration; 4let yourDate = new Date() 5var depdate = yourDate.toISOString(); 6 7var log = \u0026#34;Deployment duration: \u0026#34;+duration+\u0026#34; ms \\n\u0026#34;; 8log = log+\u0026#34;Deployment timestamp: \u0026#34;+depdate; 9msg.statusCode = 200; 10 11fse.writeFileSync(\u0026#39;/dist/deploy.log\u0026#39;, log); 12msg.payload = log; 13 14if (msg.type != \u0026#34;manual\u0026#34;) { 15 return [msg,msg]; 16} else { 17 return [msg,null]; 18} msg \u0026amp; http Fertig! Die Beiden Nodes msg und http dienen zum sauberen Abschluss unseres Deployments. Der “http out” Node liefert den zuvor definierten Body als Nachricht zurück an den aufrufenden Webhook.\nDer “msg” Debug Node zeigt uns den kompletten Inhalt des Deployments im Debugger von Node-Red an, wenn aktiviert.\nDen kompletten Node-Red Flow könnt ihr euch auch hier herunterladen https://niklas-stephan.de/media/orig/ncms/flow.json (Version 0.60).\nFazit Zugegeben, hätte ich eine Stoppuhr genutzt um aufzuzeichnen wie lange die Entwicklung von nCMS gedauert hat, vielleicht hätte ich irgendwann abgebrochen. Aber: von Start bis Ende des Projekts, ganz wie bei einem rundenbasierten Strategiespiel, war regelmäßig der “nur diese eine Sache noch” Moment da. Eine Menge Spaß hat es außerdem gemacht, mit Hilfe von Node-Red immer ausgefeiltere JavaScript Funktionen zu entwickeln. Der fest integrierte Debugger war dabei eine fast genauso große Hilfe, wie die Möglichkeit in Node-Red javascript Funktionen maximal einfach auf weitere node.js Module zuzugreifen. Falls ihr mal eine ähnliches Vorhaben umsetzen möchtet, könnt ihr gerne meine Quellen Auf Github forken.\nEinige verbesserungswürdige Schwachstellen gibt es natürlich auch noch.\nZum Beispiel an den Stellen, bei denen ich im Node-Red Backend auf das Frontend referenziere. Das macht das Ganze etwas weniger flexibel, denn wenn wirklich mal jemand meine Quellen nutzen möchte, müsste sie/er sich entsprechend noch in das Frontend einarbeiten.\nUnd überhaupt bin ich hier im Artikel nicht auf das HTML5 Frontend mit javascript Funktionen, CSS und HTML weiter eingegangen.\nVielleicht folgt das ein andernmal.\nQuellen / Weiterführende Links Hier nochmal alle Quellen, Links und Dateien aus dem Artikel zusammen aufgeführt:\nDie Quellen von NCMS bei Github: https://github.com/handtrixx/ncms Der Node-Red Flow von NCMS: https://niklas-stephan.de/media/full/ncms/ncms_flow.json Das fantastastische Node-Red: https://nodered.org/ Node-Red als Container bei Docker Hub: https://hub.docker.com/r/nodered/node-red Ngnix Proxy Manager: https://nginxproxymanager.com/ Das famose Bootstrap HTML5 Template von Twitter: https://getbootstrap.com/ markdown-it – von Markup nach HTML in einfach: https://markdown-it.github.io/ Dateisystemoperation in node.js mit fs-extra: https://github.com/jprichardson/node-fs-extra Rasend schnelle Konvertierung von Bildern in node.js mit sharp: https://sharp.pixelplumbing.com/ ","tags":["deutsch"],"section":"blog"},{"date":"1670025600","url":"/blog/ir-remote/","title":"IR-Remote Control 4.0 – Alles ins IoT!","summary":"Universalfernbedienung? Mehr 90er geht es ja nicht! Was aber wenn die alte Infrarot Fernbedienung alle Lichter im Raum ein- und ausschalten, die Heizung oder Rollos bedienen und das Heimkino gleichzeitig steuern kann? Dieser Beitrag hat es in sich, denn auf diese Art lässt sich jedes irgendwie auslesbare/anbindbare Gerät in euer Alexa, HomeKit oder nahezu jede andere Smart Home Lösung integrieren! Das alles ohne Abos, versteckte Kosten und auf Wunsch auch ganz ohne die Cloud irgendeines großen Konzerns.\n","content":"Universalfernbedienung? Mehr 90er geht es ja nicht! Was aber wenn die alte Infrarot Fernbedienung alle Lichter im Raum ein- und ausschalten, die Heizung oder Rollos bedienen und das Heimkino gleichzeitig steuern kann? Dieser Beitrag hat es in sich, denn auf diese Art lässt sich jedes irgendwie auslesbare/anbindbare Gerät in euer Alexa, HomeKit oder nahezu jede andere Smart Home Lösung integrieren! Das alles ohne Abos, versteckte Kosten und auf Wunsch auch ganz ohne die Cloud irgendeines großen Konzerns.\nZielsetzung In dieser kleinen Aufbauanleitung beschreibe ich wie ich aus mehreren Komponenten ein IoT (Internet of Things) Szenario aufgebaut habe. Ein grundsätzliches technisches Verständnis hilft dabei die einzelnen Schritte nachzuvollziehen, ein IT-Studium oder eine ähnliche Ausbildung ist aber definitiv nicht erforderlich!\nDas Ziel ist es mit einer handelsüblichen (gerne auch aus dem untersten Preissegment, oder noch besser: Elektroschrott) Infrarot Fernbedienung verschiedene Aktionen in einem Smart Home auszulösen. Ich will von einer einfachen Hardwareumgebung aus, beim Eintreffen eines Infrarotsignals, dessen Eingabewerte an meine Heimautomatisierung übergeben.\nAls Hardware nutzen wir den kleinen Bruder des weltberühmten Raspberry Pi, den Zero W und wenig weiteres Zubehör. Alles davon kann auch ohne Mühen für andere Zwecke genutzt werden. Dank tausenden von verfügbaren “Bastelanleitungen” im Netz ist die eventuelle Neuanschaffung auf keinen Fall umsonst.\nBei der Software wähle ich wie immer möglichst ausschließlich Open Source Quellen, wie z.B. Linux und Node-RED. Die gigantische Community dahinter leistet uns eine 24/7 Unterstützung bei jeglicher Art von Problemstellung.\nHardware Viel mehr als in der Zielsetzung beschrieben braucht es wirklich nicht. Falls du die Komponenten nicht herumliegen hast, hier zunächst ein paar Kaufempfehlungen.\nAb ca. 16 € gibt es den Kleinstcomputer Raspberry Pi Zero W mit passendem Gehäuse z.B. hier: https://www.berrybase.de/raspberry-pi/raspberry-pi-computer/kits/raspberry-pi-zero-w-zusammenstellung-zero-43-case. Der Zero ist für unseren Zweck besonders gut geeignet, weil wir zum einen keine besonders große Leistung benötigen, zum anderen aber Strom sparen wollen. Denn wir planen einen Dauerbetrieb. Mit einer Spannung von 2,3 bis 5,0 V und einer Stromstärke von 100 bis 140 mA, müsste der tatsächliche Stromverbrauch zwischen 0,23 Watt und 0,7 Watt liegen. Das ist weniger als so mancher Fernseher im Standby “frisst”. Das klingt vielleicht alles super “billig”, winzig und schwach, aber: Es ist 2021 und das bisschen Hardware hat die gleiche Rechenleistung wie ein Spitzen-Heimcomputer kurz nach der Jahrtausendwende. Und Windows XP wollen wir ja gar nicht installieren…\nDamit der Raspberry funktioniert braucht es noch Strom, am besten aus einem alten Handynetzteil mit Micro-USB Anschluss. Falls gerade keines verfügbar ist, bekommt man so etwas Überall für wenige Euro, so z.B. auch im Supermarkt. Hinzu kommt noch eine Micro-SD Karte, die vielleicht auch noch irgendwo zuhause herumliegt. Alles ab 8GB Größe ist OK, nur sollte sie über den Zusatz “XD” oder “HC” verfügen, damit man sicher ist nicht eine “lahme Ente” zu verwenden. Da geht es bei ca. 3 € los und auch hier wird man in den meisten Supermärkten fündig.\nEin kleines Adapterkabel hilft uns Geräte mit normalgroßem (Typ A) USB-Anschluss, wie den Infrarotempfänger, mit dem Raspberry Pi Zero zu verbinden. Das wiederum wird man eher selten im Supermarkt finden, aber z.B. hier: https://www.berrybase.de/raspberry-pi/raspberry-pi-computer/kabel-adapter/usb-kabel-adapter/usb-2.0-hi-speed-otg-adapterkabel-a-buchse-micro-b-stecker-0-15m-wei-223. Und damit ist man weitere 1,80 € ärmer. Wer nun über gar keine dieser Komponenten verfügt, kann sich auch ein Komplettset besorgen, z.B. hier: https://www.berrybase.de/raspberry-pi/raspberry-pi-computer/kits/raspberry-pi-zero-w-zusammenstellung-full-starter-kit Um die SD-Karte später mit dem Betriebssystem bespielen zu können, brauchen wir noch ein SD-Kartenleser. Die meisten Laptops haben einen solchen integriert. Aber falls nicht, gibt es die auch günstig ab 5€ fast überall und z.B. hier: https://www.heise.de/preisvergleich/sandisk-mobilemate-single-slot-cardreader-sddr-b531-gn6nn-a2356696.html\nBei der Fernbedienung hat man die Qual der Wahl. Wirklich jegliche Art von Gerät, das ein Infrarotsignal sendet, kommt hier in Frage. Es sollte natürlich keine genommen werden, die zu einem aktiv benutzten Gerät im gleichen Raum gehört, aber das versteht sich hoffentlich von selbst. Ich habe eine alte Universalfernbedienung benutzt, die ich noch übrighatte. Damit habe ich es mir selbst einen kleinen Tick schwieriger gemacht als nötig, da ich diese nun auf ein nicht vorhandenes Empfangsgerät voreinstellen musste. Einfacher wäre es gewesen, wirklich eine ganz “normale” zu verwenden.\nZum Schluss noch der Infrarotempfänger. Das ist tatsächlich fast die größte Investition hier im Projekt. Ich hatte noch den, qualitativ hochwertigen und mit cleverer Softwarelösung kommenden, FLIRC USB-Empfänger. Den bekommt man z.B. hier: https://www.heise.de/preisvergleich/flirc-usb-rev-2-flirc-v2-a1621664.html, ab ca. 24 €\nAlles in allem also eine Investition zwischen 0€ und maximal 65€ Euro.\nSoftware Nun zum zumindest für Anfänger komplizierten Teil, aber immerhin: viel Software braucht es nicht. Eigentlich nur ein Betriebssystem und zwei zusätzliche Programme/Tools. Da sich die Fernbedienung am einfachsten über eine Desktopumgebung konfigurieren lassen wird, installieren wir diese auf unsere SD-Karte. Das geht mittlerweile maximal einfach. Unter https://www.raspberrypi.org/software/ gibt es das Programm “Raspberry Pi Imager”, welches ihr dort für euer Betriebssystem herunterladen könnt. Vor dem Öffnen des heruntergeladenen und installiertem Programms steckt ihr noch die SD-Karte in euren SD-Karten-Lesegerät bzw. -slot. Im Programm wählt ihr dann das Betriebssystem “Raspberry PI OS (32-Bit)” aus, und als SD-Karte die eurige. Mit einem Klick auf “SCHREIBEN” wird das Betriebssystem auf eure Karte kopiert.\nSobald der Vorgang abgeschlossen ist, entfernt ihr die Karte bitte noch NICHT. Denn wir wollen zuvor noch zwei Dinge erledigen, die uns das Leben anschließend erleichtern. Falls schon, vor Ungeduld oder welchem Grund auch immer, herausgezogen ist das auch kein Problem, einfach wieder rein damit. Zunächst erstellen wir im Hauptverzeichnis der SD-Karte eine, gerne auch leere, Datei mit dem Namen “ssh”. Bitte achtet darauf, dass die Datei keine versteckte Endung wie z.B. “.txt” hat. Das passiert gerne unter den Standardeinstellungen von Windows. Mac und Linux Anwender haben dieses Problem in der Regel nicht. Wem das zu ungenau beschrieben ist, der guckt bitte hier: https://www.shellhacks.com/raspberry-pi-enable-ssh-headless-without-monitor/.\nJetzt möchten wir noch gleich die Verbindungsdaten zu unserem WLAN angeben, damit die Netzwerkverbindung von Anfang an sichergestellt ist. Dazu legen wir eine zweite Datei Namens “wpa_supplicant.conf” in das Gleiche Verzeichnis. Windows Benutzer passen wieder bei der Dateiendung auf. Dieses Mal kommt es aber auf den Inhalt der Datei an:\n1country=DE 2ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev 3update_config=1 4network={ 5 ssid=\u0026#34;DerNameDeinesWLANs\u0026#34; 6 psk=\u0026#34;DeinWLANPasswort\u0026#34; 7 key_mgmt=WPA-PSK 8} Das ist nur ein Beispiel, und wir müssen die Werte in den Klammern bei den Zeilen “ssid” und “psk” auf die Werte unseres WLANs setzen.\nJetzt sind wir aber fertig und die Karte kann entfernt und in den Raspberry gesetzt werden. Nun steckt ihr alle Komponenten zusammen und bringt das Ganze an seinen Bestimmungsort. Sobald ihr das Gerät mit dem Netzteil verbunden habt, seht ihr:\nErstmal nicht viel. Kein Grund zur Panik, das ist normal und ihr habt vermutlich alles richtig gemacht. Wir machen einfach weiter.\nWir sind jetzt in der Lage uns per SSH auf dem kleinen Raspberry anzumelden, um dort ein paar Dinge einzustellen. Um sich kurz auf Kommandozeilenebene die Finger schmutzig zu machen, kommen wir leider nicht herum und leider lässt sich dafür im Falle des Raspberry Pi Zeros auch nicht Visual Studio Code verwenden. Ein Terminal muss her.\nUnter MacOS und Linux ist eine Terminal Applikation für die Kommandozeile seit je her eingebaut und heißt dort auch so. Soweit ich weiß, steht das SSH Kommando auch in Windows 10 zur Verfügung, kann das aber leider gerade nicht testen. Allen die damit nicht vertraut sind, hilft Google gerne weiter. Die Verbindung zu unserem neuen Kleinstcomputer stellen wir dann wie folgt her:\n1ssh pi@192.168.178.22 Die IP-Adresse muss natürlich auf die eures Raspi angepasst werden. Die bekommt ihr am schnellsten über das Webinterface eures Routers heraus. Das Standardpasswort nachdem ihr nun gefragt werdet ist: raspberry . Wenn ihr das nach der Anmeldung gleich ändern möchtet, geht das mit dem Befehl “passwd”. Es empfiehlt sich Betriebssystem und Software regelmäßig auf den neuesten Stand zu bringen, das macht man mit den Befehlen:\n1sudo apt-get update um aktuell verfügbare Pakete zu ermitteln, um diese dann wiederum mit:\n1sudo apt-get upgrade zu installieren. Jetzt installieren wir Node-RED mit dem Befehl:\n1bash \u0026lt;(curl -sL https://raw.githubusercontent.com/node-red/linux-installers/master/deb/update-nodejs-and-nodered) und wenn wir schon dabei sind, bestimmen wir noch, dass Node-RED bei jedem Systemstart automatisch gestartet wird:\n1sudo systemctl enable nodered.service Nun starten wir das System am besten einmal neu. Das geht mit:\n1sudo reboot Nach spätestens 3-4 Minuten sollte alles wieder hochgefahren sein und wir müssten in der Lage sein auf Node-RED, über den Browser eines beliebigen Gerätes in eurem Netzwerk, zuzugreifen:\nhttps://192.168.178.22:1880 Auch hier die IP-Adresse durch die eures Raspis ersetzen. Wenn sich nun die Weboberfläche von Node-RED öffnet, können wir unseren ersten Erfolg feiern. Wir haben aus unserem kleinen Raspi ein mächtiges IoT Werkzeug gemacht!\nJetzt wollen wir uns daran machen unsere Fernbedienung zu konfigurieren. Dafür müssen wir noch einmal die Kommandozeile bemühen und uns wie zuvor beschrieben auf den Raspi schalten. Als Erstes prüfen wir, ob uns der Desktop des Raspis später zur Verfügung stehen wird. Wie das geht, ist perfekt hier: https://www.raspberrypi.org/documentation/remote-access/vnc/ beschrieben. Dort auf den Abschnitt “Enabling VNC Server at the command line” fokussieren, den Rest können wir für den Moment ignorieren.\nAuch auf Kommandozeilenebene installieren wir die grafische Oberfläche für unseren Flirc Infrarotempfänger. Das geht mit (copy \u0026amp; paste):\n1curl apt.flirc.tv/install.sh | sudo bash Jetzt haben wir alles zusammen was wir brauchen und können endlich mit dem richtigen Spaß beginnen.\nKonfiguration Verglichen mit der Softwareinstallation, ist die eigentliche Konfiguration nun ein “Klacks”.\nIm ersten Schritt lernen wir unsere Fernbedienung an. Dazu schalten wir uns per VNC auf den Desktop des Raspberry PI Zero. Dort öffnen wir dann das Programm Flirc, welches wir im Startmenü finden. Bevor es losgeht, stellen wir die Ansicht noch auf “Keyboard” um. Dadurch stehen uns schon ohne Tastenkombinationen mehr als 127 Kommandos (Tasten) zur Verfügung, die wir auf unserer Fernbedienung belegen können. Da wir niemals eine echte Tastatur an den Raspi anschließen werden, ist das Einzige, worauf wir achten müssen, dass wir keine Sondertasten belegen. Die “Drucken” Taste zu belegen ist also keine gute Idee, alle Zahlen und Buchstaben: kein Problem. Um die Belegung zu starten, klicken wir im Programm auf eine der angezeigten Tasten, also z.B. “F”. Dadurch werden wir nun aufgefordert unsere Fernbedienung in Richtung des Infrarot Empfänger zu halten und darauf ebenfalls einen Knopf zu drücken. Dieses Prozedere spielen wir für alle Knöpfe auf unserer Fernbedienung durch, die wir später mit einer Aktion hinterlegen wollen. Jetzt ist unsere Fernbedienung “angelernt” und wir können die VNC Session wieder beenden. Das FLIRC Programm kann vorher auch beendet werden, es läuft im Hintergrund weiter und unsere Kommandos stehen auch nach einem Neustart weiterhin zur Verfügung.\nAb hier darf jetzt Node-RED zeigen was es kann. Und das Programm macht es uns maximal einfach! Wir öffnen wieder im Browser eines beliebigen Endgeräts unseres Netzwerks die Node-RED GUI:\nhttps://192.168.178.22:1880 Unser Flow (Ablauf) benötigt nur zwei Knoten aus dem “Standardsortiment”. Als erstes ziehen wir aus der linken Leiste einen “rpi – keyboard” Knoten in den Flow. Der macht nichts anderes als, wie der Name es erahnen lässt, auf Tastatureingaben am Raspi zu lauschen. Durch unsere Konfiguration von FLIRC hört er so automatisch auch auf die Eingaben unserer Fernbedienung. Wer das Testen oder einfach sehen möchte, ob es funktioniert, fügt einen Debug nun in den Flow ein und verbindet diesen mit dem Ausgang unseres “rpi – keyboard” Nodes. Nach einem Klick auf Deploy oben rechts sollte nun nach jedem druck auf der Fernbedienung auf einen angelernten Knopf, eine Nachricht im Debug Fenster auftauchen. So weit so gut.\nNun wollen wir uns mit der Außenwelt verbinden. Dafür stehen uns unterschiedlichste Möglichkeiten zur Auswahl. So können wir die Ereignisse bereits mit Node-Red Bordmitteln in eine Datei schreiben, mit einem Webservice Aufruf koppeln oder auch eine Nachricht über einen Websocket übertragen. Auch stehen MQTT, TCP, UDP und sogar die serielle Ausgabe als Übertragungsmöglichkeit im Standard offen. Damit sollte eigentlich schon jeder Wunsch abgedeckt sein, aber noch mehr Komfort zur Integration in bestehende Smart Home Netze gibt es durch optionale installierbare Module. So machen wir unser Setup mit dem Modul “node-red-contrib-homekit-bridged” z.B. zu einem HomeKit Endgerät nach Apple Standard. Das gleiche funktioniert auch mit OpenHab https://flows.nodered.org/node/node-red-contrib-openhab3 oder sogar Alexa https://flows.nodered.org/node/node-red-contrib-virtual-smart-home und eigentlich allem was es so auf dem Markt gibt und über ein Netzwerk kommuniziert. Ich persönlich habe mich für die Übertragung via Websockets ohne zusätzliches Modul entschieden.\nHalt, Stop! Warum auf einmal jetzt Websockets? In meinem ersten Aufbau hatte ich die Kommandos der Fernbedienung über eine “normale” API zu übertragen versucht. Das funktioniert zwar ebenso prächtig ist aber zu langsam! Beim Drücken eines Knopfes auf einer Fernbedienung erwartet jeder eine sofortige Reaktion bzw. Aktion. 1 Sekunde Verzögerung fühlt sich unnatürlich an, 2 Sekunden sind bereits unerträglich. Man stelle sich vor man will z.B. die Lautstärke des Fernsehers stück für stück erhöhen. Ganz schnell habe ich 20-mal oder öfter gedrückt und die API-Kommandos werden langsam abgearbeitet. So arbeitet man erfolgreich an einem Hörsturz. Jeder der jetzt gerade Kopfkino spielt: Ja, genau so ist es mir ergangen…\nVor ähnlichen Erfahrungen möchte ich euch schützen und deswegen greifen wir gleich auf Websockets zurück. Ohne zu tief in die Grundlagen der Informatik eingehen zu wollen, handelt es sich bei Websockets um eine Interprozesskommunikation (IPC) über das HTTP (WEB) Protokoll. Anders formuliert: Mit Websockets bauen wir einen Tunnel zwischen zwei oder mehr Geräten zur Echtzeitkommunikation auf. Genau was wir hier brauchen!\nDer zweite Knoten, den wir in den Flow ziehen ist also ein “Websocket out” aus dem Netzwerkbereich. Auf diesen hört wiederum ein „Websocket in“ in meiner Heimautomatisierungslösung, die ebenfalls mit Node-Red realisiert ist. Wenn ihr ein anderes System zur Heimautomatisierung einsetzt, wäre die Eingabe des “rpi – keyboard” Nodes/Knotens an das entsprechende Modul weiterzuleiten. So lässt sich eben auch, wie in der Einleitung angekündigt, jedes andere Gerät in euer IoT einbinden.\nFazit In unserem kleinen Versuch haben wir spielerisch aus einer alten Infrarot Fernbedienung eine sinnvolle Erweiterung unserer Smart Homes geschaffen. Da unser Raspi Zero nur wenig Strom verbraucht, wäre theoretisch auch eine Energieversorgung über einen Akku oder eine Powerbank denkbar, womit das ganze Setup sogar mobil werden würde.\nDer ein oder andere “Profi” wird darüber geschimpft haben, warum ich auf dem kleinen Raspberry Pi Zero eine Desktopumgebung installiert habe, anstelle darauf der geringen Ressourcen zuliebe zu verzichten. Das habe ich probiert und das Anlernen der Fernbedienung über FLIRC hat auf diese Art reichlich wenig Spaß gemacht. Außerdem sagt uns htop, dass auf dem Raspi auch bei laufendem Desktop nur ca. 100 von 512 MB Arbeitsspeicher belegt sind.\nIch hoffe der Artikel war interessant und hat zu der ein oder anderen neuen Idee für private oder auch berufliche Projekte geführt!\n","tags":["deutsch"],"section":"blog"},{"date":"1667260800","url":"/blog/boinc/","title":"Rechenkraft für die Wissenschaft mit BOINC","summary":"Wenn man sich zum Einem für wissenschaftliche Projekte begeistern kann und zum Anderen ein wenig Rechenleistung übrig hat, dann sollte man sich das Open Source Projekt und Werkzeug Boinc einmal genauer anschauen. In diesem Artikel berichte ich darüber, was Boinc eigentlich ist und wie man es einfach installieren kann.\nVolunteer-Computing / Ehrenamtliches Rechnen? Volunteer-Computing (zu deutsch: ehrenamtliches / freiwilliges Rechnen) beschreibt eine Technik der Anwendungsprogrammierung bei der einzelne Computernutzer Rechnerkapazitäten wie Rechenzeit und Speicherplatz auf freiwilliger Basis einem bestimmten Projekt zur Verfügung stellen, um unter Anwendung des verteilten Rechnens ein gemeinsames Ergebnis zu berechnen.\n","content":"Wenn man sich zum Einem für wissenschaftliche Projekte begeistern kann und zum Anderen ein wenig Rechenleistung übrig hat, dann sollte man sich das Open Source Projekt und Werkzeug Boinc einmal genauer anschauen. In diesem Artikel berichte ich darüber, was Boinc eigentlich ist und wie man es einfach installieren kann.\nVolunteer-Computing / Ehrenamtliches Rechnen? Volunteer-Computing (zu deutsch: ehrenamtliches / freiwilliges Rechnen) beschreibt eine Technik der Anwendungsprogrammierung bei der einzelne Computernutzer Rechnerkapazitäten wie Rechenzeit und Speicherplatz auf freiwilliger Basis einem bestimmten Projekt zur Verfügung stellen, um unter Anwendung des verteilten Rechnens ein gemeinsames Ergebnis zu berechnen.\n— Wikipedia Das heisst, dass jeder der ein wenig Rechenleistung entbehren kann, sich an wissenschaftlichen Projekten beteiligen kann, um diesen dabei zu helfen bestimmte Problemstellungen zu lösen. Um das zu tun, muss man sich bloß ein bestimmtes Programm installieren, dass auf einem Computer läuft der möglichst oft/dauerhaft betrieben wird. Wer also z.B. zuhause einen Raspberry Pi oder so wie ich, sowieso einen Server betreibt, kann wissenschaftliche Arbeit leisten in dem er teile seiner freien Prozessorkapazität zur Verfügung stellt.\nHinweis Vor einem Betrieb auf einem Laptop oder einem klassischen PC sollte man bedenken, dass durch erhöhten CPU-Verbrauch die Lebensdauer, wenn vielleicht auch nicht signifikant, heruntergesetzt bzw. der Stromverbrauch gesteigert wird. Von einem Betrieb auf Cloud-Server Angeboten wie Microsoft Azure oder Amazon AWS, ist wegen der auf CPU-Leistung und Datenübertragung basierenden Abrechnungsmodelle, ebenfalls abzuraten.\nSonst kann es teuer werden!\nWas ist Boinc? Die Schaffer des Tools Boinc haben es sich zur Aufgabe gemacht ca. 30 wissenschaftliche Volunteer-Computing Projekte über eine Oberfläche verfügbar und konfigurierbar zu machen. Auch Einstellungen wie die zur Verfügung zu stellenden Ressourcen und Reports über bereits geleistete Arbeit stellt das Tool zur Verfügung. Eine genaue Liste mit Beschreibung der Projekte findet sich hier: https://boinc.berkeley.edu/projects.php . Dort wird auch schon ersichtlich, dass nicht jedes Projekt für jede Computerplattform existiert und andere nicht für jede Art von Plattform geeignet sind. So benötigen einige von Ihnen z.B. eine schnelle dezidierte Grafikkarte, da die angestellten Berechnung auf diesen Prozessortyp (GPU) optimiert sind.\nOK, gefällt mir. Wie starte ich? Die Installation kann entweder manuell und direkt über das auf der Boinc Homepage verfügbare Installationsprogramm erfolgen oder, wie von mir bevorzugt und folgend beschrieben, als Docker Container mit Docker Compose.\nDie manuelle Installation funktioniert auf jeder Art von Rechner und kann zum Beispiel so konfiguriert werden, das BOINC als Bildschirmschoner läuft und so nur dann Ressourcen des PCs beansprucht, wenn dieser gerade nicht genutzt wird. Die Installation kann man unter https://boinc.berkeley.edu/download.php herunterladen.\nFür den Betrieb auf einem Server in einem Container hier eine beispielhafte compose.yml Datei:\n1version: \u0026#34;2.1\u0026#34; 2services: 3 boinc: 4 image: lscr.io/linuxserver/boinc 5 hostname: \u0026#34;boinc\u0026#34; 6 container_name: boinc 7 environment: 8 - PUID=1000 9 - PGID=1000 10 - TZ=Europe/Berlin 11 - PASSWORD=xxx 12 volumes: 13 - ./config:/config 14 restart: unless-stopped 15 #ports: 16 # - 8080:8080 17 logging: 18 options: 19 max-size: \u0026#34;10m\u0026#34; 20 max-file: \u0026#34;3\u0026#34; 21 networks: 22 - dmz 23 deploy: 24 resources: 25 limits: 26 cpus: 1.00 27 memory: 2048M 28networks: 29 dmz: 30 external: true Die Angabe der zur Verfügung gestellten Ressourcen über deploy – resources – limits, ist optional. Eine Beschränkung der dem Program bereitgestellten Ressourcen lässt sich, wie schon zuvor erwähnt, auch direkt im Programm konfigurieren. Ich gehe hier auf Nummer sicher und schränke den Zugriff von vorn herein ein. Ein CPU Wert von 1 entspricht einem CPU-Thread. Bei z.B. vorhandenen 8 Threads, kann der hier definierte Container also auf 12,5% der verfügbaren CPU-Leistung zugreifen. Genau das von mir gewünschte “Grundrauschen”. Die Angabe des Ports ist nur erforderlich, wenn wir keinen Reverse Proxy nutzen. Dann wäre unsere BOINC Instanz auf dem Host über den entsprechenden Port erreichbar. Das Passwort sollte auf jeden Fall gesetzt werden und ist dann zur Anmeldung am virtuellen Desktop über den Browser erforderlich. Der Benutzer ist übrigens fest auf abc eingestellt.\nAnwendung und Fazit Entweder durch starten der Anwendung oder durch Zugriff auf das Webinterface im Conatinerbetrieb, können wir uns nun im Program für die Projekte die uns intressieren anmelden und die Prozesse starten. Danach passiert eigentlich nichts spannendes mehr und eine weiteres Eingreifen unserseits ist nicht erfordlich. Unseren Beitrag zum jeweiligen Projekt können wir über die im Program integrierten Berichte oder auf der jeweiligen Projektseite jederzeit einsehen. Dort oder unter können wir uns auch ein Zertifkat gennerieren lassen, wenn wir das möchten.\nEinfacher geht es nicht, viel Spaß beim rechnen (lassen)!\n","tags":["deutsch"],"section":"blog"},{"date":"1664668800","url":"/blog/headless-cms-2022/","title":"Headless CMS, APIs und IoT für die Website","summary":"Wie im Artikel “Die (beinahe) perfekte Website – Teil 1: Anforderungen” bereits angerissen, kommen bei Gestaltung und Umsetzung eines modernen Internetauftritts eine Vielzahl von Faktoren zusammen. Für alle denen eine einfache WordPress Installation nicht reicht, oder für die die möglichen Alternativen kennen lernen möchten, stelle ich in diesem Post meine Umsetzung mit Hilfe der Tools Cockpit, Node-RED und weiteren Helferlein vor.\nWas ist das Ziel? Ich wollte erreichen dass meine Seite(n) beim Besucher weit unterhalb von 2 Sekunden geladen werden und dort hübsch und zeitgemäß dargestellt werden. Außerdem wollte ich dazu moderne Techniken und Tools verwenden. Soviel vorab: das Meiste davon ist mir gelungen :-). Bei der Auswahl der Tools habe ich größtenteils mir vertraute Lösungen genommen, mit dem headless CMS Cockpit aber auch etwas Neues ausprobiert. Aber der Reihe nach.\n","content":"Wie im Artikel “Die (beinahe) perfekte Website – Teil 1: Anforderungen” bereits angerissen, kommen bei Gestaltung und Umsetzung eines modernen Internetauftritts eine Vielzahl von Faktoren zusammen. Für alle denen eine einfache WordPress Installation nicht reicht, oder für die die möglichen Alternativen kennen lernen möchten, stelle ich in diesem Post meine Umsetzung mit Hilfe der Tools Cockpit, Node-RED und weiteren Helferlein vor.\nWas ist das Ziel? Ich wollte erreichen dass meine Seite(n) beim Besucher weit unterhalb von 2 Sekunden geladen werden und dort hübsch und zeitgemäß dargestellt werden. Außerdem wollte ich dazu moderne Techniken und Tools verwenden. Soviel vorab: das Meiste davon ist mir gelungen :-). Bei der Auswahl der Tools habe ich größtenteils mir vertraute Lösungen genommen, mit dem headless CMS Cockpit aber auch etwas Neues ausprobiert. Aber der Reihe nach.\nEs ist beinahe lustig dass das 2022 (wieder) der Fall ist, aber Microsoft Visual Studio Code(VS Code) ist tatsächlich beinahe das Wichtigste dieser Werkzeuge. Back to the Roots und ein klares Nein an die Generatoren von chaotischem und langsamem Quellcode wie WordPress, Jimdo usw. es nun mal sind. Außerdem ist VS Code weit mehr als nur ein erweiterter Text-Editor, sondern auch Open Source Tool zur Administration ganzer Containerlandschaften und natürlich vorzüglich zum Debugging. Vor einer Weile hatte ich dazu auch folgendes Anwendungsbeispiel geschrieben: Quellcode auf GitHub verwalten mit Visual Studio Code .\nSeit Veröffentlichung von Version 5 des Open Source Frontend Toolkits Bootstrap von Twitter, ist dieses schneller den je zuvor. Man hat sich endlich des unnötigen Overheads des jQuery Frameworks entledigt und setzt nun vollständig auf natives “Vanilla” Javascript. Das gleiche habe ich bei den wenigen selbst geschriebenen Funktionen im Frontend natürlich auch gemacht. Bootstrap selbst stellt CSS Klassen, Icons, Komponenten und vieles mehr zur Verfügung, die bei der Entwicklung von UI/UX unwahrscheinlich viel Zeit und Aufwand sparen.\nMit den Open Source Web Content Management System (WCMS) Cockpit von Agentejo, gelingt die saubere Trennung von Back- und Frontend. Neben den für ein CMS üblichen Funktionalitäten zur Pflege- und Verwaltung von Web-Content und einer Medienverwaltung macht die integrierte API den Unterschied. Dazu später etwas mehr.\nDas ursprünglich von IBM initiierte Open Source Projekt Node-RED ist vielleicht die zur Zeit beste Implementierung einer Internet of Things (IoT) Low-Code Platform. Sowohl die Einschränkung “Low-Code” als auch auf IoT, werden dem Tool aber nicht ganz gerecht. Zwar ist es mal für diese Szenarien entwickelt worden, lässt sich aber ohne Mühen auch für ganz viel Code und zur Abbildung aller möglichen vorstellbaren Prozesse nutzen. Ein Beispiel folgt.\nEinige weitere benötigte Werkzeuge und Helfer wir den Nginx Reverse Proxy Manager, Let’s Encrypt SSL Zetifikate, Matomo Analytics, Docker(-Compose) und weitere würden hier vom Fokus ablenken, weshalb ich sie erstmal außen vor lasse.\nWarum? Die Frage zum warum ist einfach zu beantworten; vor allem beruflich setze ich mich täglich mit den Hürden und Schwierigkeiten beim Umgang mit monolithischen Systemen auseinander. Andererseits propagieren Unternehmen wie die SAP neue Headless Ansätze. Das und meine innere Neugier haben mich angetrieben, einfach für mich selbst solch ein Setup vorzunehmen um evtl. neue Risiken, aber vor allem Möglichkeiten kennen zu lernen. Oder in kurz: Weil ich es können will und es gar nicht so schwierig ist.\nWie genau? Die Installation von Cockpit CMS und Node-RED in eigenständigen Containern geht schnell von der Hand und sollte in maximal 20 Minuten erledigt sein. Im Appendix, als Hilfestellung, meine Docker-Compose Dateien.\nJetzt geht es ans “Eingemachte”. Nach dem Entwurf des Layouts und der Umsetzung des Prototypen mit Bootstrap, lagere ich sämtlichen statischen HTML Content nach Cockpit CMS in Form von dort sogenannten “Singletons” aus. Die haben den Vorteil, dass sie mit Attributen wie Sprache und Version, etc. versehen werden können. Theoretisch könnte man das auch mit den CSS und Javascript Dateien machen, aber die bearbeite ich dann doch irgendwie lieber in Visual Studio Code. Als nächstes erstellen wir nun eine sogenannte “Collection” in Cockpit, die sozusagen den Rahmen um unsere Posts bildet. So ließe sich z.B. mit einer Cockpit Installation der Content für viele Websites verwalten. Klasse! Bilder, Grafiken und der Gleichen lassen sich auch mit Cockpit komfortabel verwalten und vor allem von dort aus auch automatisch als Thumbnails umwandeln. Noch besser! Der Zugriff auf das Ganze erfolgt dann über Web APIs, auf die ich gleich weiter eingehe. Und noch etwas; über sogenannte Webhooks lassen sich auch genau andersherum von Cockpit aus Aktionen starten. So starte ich z.B. bei jedem speichern/ändern der Collection automatisch ein Deployment der kompletten Website. Wow! Eigentlich war Cockpit einfach nur mein erster Google Treffer bei der Suche nach einem Headless CMS, aber mir fällt gerade kein Grund mehr ein warum ich wechseln bzw. mir andere ansehen sollte.\nUnd nun kommt Node-RED ins spiel, denn ich muss meine Inhalte ja nun irgendwie unter unter niklas-stephan.de verfügbar machen. Eigentlich geht es mir darum, mit dem Ziel maximaler Performanz, statische Seiten zu generieren. Bei einer Google Suche nach “Static Site Generators” ist Node-RED garantiert nicht auf der ersten Seite der Suchergebnisse zu finden. Dafür gibt es unzählige andere Tools, die aber auch wieder eine Einarbeitung und meistens auch das Erlernen einer eigenen Syntax erfordern. Durch meine umtriebigen Aktivitäten im Umfeld der Heimautomatisierung, ist mir aber nun Node-RED bestens vertraut und eigentlich will ich doch nur einen relativ einfachen Prozess abbilden. Und der sieht wie folgt aus:\nStart des Deplyoments manuell oder durch Webhook Sammeln aller statischen Quellen Sammeln aller vorhanden Singletons und Posts Dynamische Generierung von Meta-Daten Rendern der einzelnen Seiten Speichern der Seiten auf Platte und im Arbeitsspeicher Wer sich die Mühe machen möchte mal kurz im Console Log dieser Seite zu gucken sieht, dass ich bei jedem Deployment Zeitpunkt und Dauer protokollieren lasse. Da sehen wir dann auch dass der beschriebene Prozess insgesamt 0,099 Sekunden dauert. Und das auf einem Server der nebenbei u.a. noch Monero schürft und meine Cloud Umgebung bereit stellt. Also eigentlich schon recht fix, aber um noch “eins Drauf” zu setzen lege ich die erzeugten Dateien im letzten Schritt im “/dev/shm”, sprich dem Arbeitsspeicher des Servers ab, von wo aus sie der Reverse Proxy beim Zugriff zur Verfügung stellt. Da braucht’s vorerst kein CDN mehr und mein Ziel ist erreicht. 🙂\nAppendix compose.yml für Cockpit CMS\n1services: 2 cms-db: 3 image: \u0026#39;mongo:latest\u0026#39; 4 restart: always 5 volumes: 6 - \u0026#39;./db:/data/db\u0026#39; 7 networks: 8 - default 9 logging: 10 options: 11 max-size: \u0026#34;10m\u0026#34; 12 max-file: \u0026#34;3\u0026#34; 13 cms: 14 image: agentejo/cockpit 15 restart: always 16 volumes: 17 - \u0026#39;./cms/config.php:/var/www/html/config/config.php\u0026#39; 18 - \u0026#39;./cms/defines.php:/var/www/html/config/defines.php\u0026#39; 19 - \u0026#39;./storage:/var/www/html/storage\u0026#39; 20 environment: 21 COCKPIT_SESSION_NAME: SESSION 22 COCKPIT_SALT: SALT 23 COCKPIT_DATABASE_SERVER: \u0026#39;mongodb://cms-db:27017\u0026#39; 24 COCKPIT_DATABASE_NAME: DBNAME 25 depends_on: 26 - cms-db 27 ports: 28 - \u0026#34;8080:80\u0026#34; 29 networks: 30 - default 31 logging: 32 options: 33 max-size: \u0026#34;10m\u0026#34; 34 max-file: \u0026#34;3\u0026#34; 35volumes: 36 mongo-vol: null compose.yml für Node-RED\n1services: 2 node-red: 3 image: nodered/node-red:latest 4 restart: always 5 environment: 6 - TZ=Europe/Berlin 7 ports: 8 - \u0026#34;1880:1880\u0026#34; 9 networks: 10 - default 11 logging: 12 options: 13 max-size: \u0026#34;10m\u0026#34; 14 max-file: \u0026#34;3\u0026#34; 15 volumes: 16 - ./data:/data 17 - ./templates:/templates 18 - ./static:/static 19 - /etc/localtime:/etc/localtime:ro 20 - /dev/shm/nsde:/ramdisk 21 - ./development:/development ","tags":["deutsch"],"section":"blog"},{"date":"1656806400","url":"/blog/perfect-website-2022/","title":"Die (beinahe) perfekte Website - Anforderungen","summary":"Willkommen zum ersten, wirklich langen Beitrag meiner Website, in dem ich darauf eingehe, was alles erforderlich ist um einen Internetauftritt besonders gut zu gestalten. Um zu beweisen, dass die gestellten Anforderungen auch erfüllt werden können, nutzen wir meine Domain niklas-stephan.de um das Gelernte, soweit möglich, auch gleich direkt umzusetzen.\nWas ist perfekt? Was ist schon perfekt und wer definiert das? Im Falle eines Internetauftritts ist diese Frage vielleicht einfacher zu beantworten als in anderen Fällen. Denn, egal wie gut Gestaltung und Umsetzung gelungen sind, muss die Seite erst einmal von einem Besucher gefunden werden. Mit einem Markanteil von über 90% (https://blog.hubspot.de/marketing/google-trends-suche) ist da die Google Suchmaschine eigentlich der einzige ausschlaggebende Faktor. Als Schlussfolgerung kann mann also sagen, dass die Optimierung einer Website auf von Google definierte Faktoren gleichzeitig nah an eine perfekte Website führt. Welche Faktoren das sind und welche Weiteren noch eine wichtige Rolle spielen, sehen wir uns nun im ersten Teil unserer Artikelserie an.\n","content":"Willkommen zum ersten, wirklich langen Beitrag meiner Website, in dem ich darauf eingehe, was alles erforderlich ist um einen Internetauftritt besonders gut zu gestalten. Um zu beweisen, dass die gestellten Anforderungen auch erfüllt werden können, nutzen wir meine Domain niklas-stephan.de um das Gelernte, soweit möglich, auch gleich direkt umzusetzen.\nWas ist perfekt? Was ist schon perfekt und wer definiert das? Im Falle eines Internetauftritts ist diese Frage vielleicht einfacher zu beantworten als in anderen Fällen. Denn, egal wie gut Gestaltung und Umsetzung gelungen sind, muss die Seite erst einmal von einem Besucher gefunden werden. Mit einem Markanteil von über 90% (https://blog.hubspot.de/marketing/google-trends-suche) ist da die Google Suchmaschine eigentlich der einzige ausschlaggebende Faktor. Als Schlussfolgerung kann mann also sagen, dass die Optimierung einer Website auf von Google definierte Faktoren gleichzeitig nah an eine perfekte Website führt. Welche Faktoren das sind und welche Weiteren noch eine wichtige Rolle spielen, sehen wir uns nun im ersten Teil unserer Artikelserie an.\nAus Sicht des Besuchers Neben viel Literatur und Informationen gibt Google mit dem Werkzeug “Lighthouse”, welches in jede Installation des Browsers Chrome integriert ist, einen Einblick auf die Kriterien die sie an eine Website Stellen. Diese sind:\nPerformance (Geschwindigkeit) Accessibility (Erreichbarkeit/Zugiffsfähigkeit) Best Practices (Empfohlene Maßnahmen) Search Engine Optimization (SEO – Suchmaschinenoptimierung) Es taucht noch ein weiter Punkt “Progressive Web App” (PWA) in Lighthouse auf. Um aus HTML5 eine echte WebApp zu machen müssen nämlich eine Menge von Anforderungen erfüllt werden. Dazu später mehr in einem anderen Beitrag zu meiner App zur Hausautomatisierung. Für eine Website im eigentlichen Sinn ist der Punkt PWA jedenfalls irrelevant. Der Screenshot hier zeigt eine Lighthouse Bewertung einer im Internet tausendfach frequentierten Seite, die aber offensichtlich nicht die gestellten Kriterien zu Googles voller Zufriedenheit erfüllt. Dies ist durchaus üblich bzw. nicht ungewöhnlich, denn noch viele weitere Faktoren entscheiden über den Erfolg eines Internetauftritts.\nWen es interessiert wie diese Bewertung zustande kommt, der kann wie gesagt über die Entwicklertools von Google Chrome (Öffnen sich mit Taste F12) im Tab “Lighthouse” eine beliebige Seite bewerten lassen und sich das Ergebnis ansehen. Praktischerweise gibt Lighthouse auch gleich noch Tipps dazu aus, wie sich die Punktzahl weiter erhöhen bzw. die Seite optimieren lässt.\nDeshalb orientieren wir uns in den nächsten Kapiteln genau an diesen Faktoren.\nPerformance Seit dem die Übertragungsgeschwindigkeit der Endgeräte des Webseitenbesuchers nicht mehr der limitierende Faktor sind, müssen sich Webseiten mindestens genauso “schnell” anfühlen wie eine Applikation die auf unserem Endgerät fest installiert ist. Google gibt in seinem Playbook so den dringlichen Rat dazu, dass eine Seite innerhalb von 2 Sekunden geladen sein muss um die Benutzererfahrung nicht zu schmälern. Um unter diesen 2 Sekunden zu bleiben kann man sich vieler Stellschrauben bedienen:\nStarke Infrastruktur / Hardware mit einer guten Netzwerkanbindung Ein möglichst geringes Datenvolumen einer Seite Kompression von Dateien und während der Datenübertraung Reverse-Proxy Konfiguration und Caching Content Delivery Networks (CDN) für lokales Caching Kein/wenig Server-Side Rendering “Sauberer” und sparsamer Quellcode Auf das (nach)laden, besonders von externen Inhalten, sollte möglichst verzichtet werden Jeder der aufgelisteten Punkte ist mindestens ein eigenes Kapitel Wert und wir werden in unsere Kapitelserie dementsprechend darauf eingehen.\nAccessibility Egal wie schnell sich die von uns aufgesuchte Seite auch aufgebaut hat, wenn der Autor meinte es sei eine gute Idee alles in einer Schriftgröße von 5 Pixeln in hellgrauer Schrift unter Verwendung eines Comic Sans Fonts auf weißem Hintergrund anzuzeigen, ist der Spaß vorbei. Auch Laufschriften mit Telefonnummern zum Abschreiben und großzügige Nutzung von Neon-Farben sind Effekte die in einem höchstens die Gewaltbereitschaft steigern. Vielleicht nicht ganz so übertrieben aber ähnlich hat sicherlich jeder von uns schon einmal Erfahrungen im Internet gemacht. Um solchen Situationen entgegenzuwirken achtet man als Webseitenbetreiber, neben der generellen User Experience (UX – Berufserfahrung), auch auf die Accessibility, sprich die Erreichbarkeit / Zugriffsfähigkeit seiner Inhalte. Dabei gilt es zu bedenken:\nNicht jeder Mensch kann gut sehen Manche Menschen können überhaupt nicht sehen und lassen sich die Inhalte von einer Maschine vorlesen Menschen nutzen unterschiedliche Endgeräte mit unterschiedlichen Auflösungen Auf die Verwendung evtl. verwirrender Effekte sollte man möglichst verzichten, wenn sie nicht ein Kernelement der Seite darstellen. Die Seitenstruktur sollte klar und hierarchisch sein, z.B. nicht am Anfang der Seite mit einer h4 Überschrift anfangen und dann mitten im Text h2 verwenden. Es sollten möglichst keine oder zumindest nur wenige Maßnahmen getroffen werden, die den Nutzer daran hindern die Darstellung seinen Bedürfnissen nach anzupassen. Wenn mann sich daran hält, wird man auf modernen Endgeräten/Browsern automatisch durch die Bereitstellung seiner Seite mit der “Reader” Option belohnt, die ein eBook-artiges lesen der Artikel ermöglicht.\nBest Practices Es gibt eine Reihe von empfohlenen Maßnahmen um die korrekte Darstellung der Seite beim Besucher zu Gewährleisten und um bestimmte Sicherheitsstandards einzuhalten. Außerdem prüft Lighthouse hier noch, dass die Seite keine offensichtlichen Fehler in der Programmierung enthält und gängige sowie akutelle Standardtechniken umgesetzt wurden. Insgesamt werden bis zu 17 Kriterien geprüft, von denen ich hier die wichtigsten Aufliste.\nVerwendung von https zur Verschlüsselung der Datenübertragung zwischen Nutzer und Betreiber. Schon vor 2021 eigentlich ein absolutes Muss.\nVerzicht auf nicht unbedingt erforderliche JavaScript Features Keine JavaScript Fehler auf der Seite. Korrekte Formatierung des HTML Codes bezüglich wesentlicher Merkmale wie z.B. der eingesetzten Sprache. Das klingt eigentlich nach nicht viel und leicht umzusetzen. Wer sich aber einmal die Mühe macht sich die Fehlerprotokolle durchzulesen die von einer großen Anzahl gängiger Websites “ausgespuckt” werden, staunt nicht schlecht. Offenbar ist es selbst großen Unternehmen nur schwer möglich, ihre Entwickler dahingehend zu motivieren, dass solche Fehler ausgeschlossen sind. Zur Verteidigung der Entwickler aber vor allem deren Motivation, muss man allerdings sagen dass verschiedene Browser gerade mit JavaScript verschieden umgehen und manchmal Kompromisse geschlossen werden müssen.\nSEO Wie schon zu Beginn des Artikels erläutert, bringt uns die schönste Website nichts, wenn sie niemand findet. Falls wir dann auch noch nicht gewillt sind tief in unsere Taschen zu greifen, um Werbekampagnen zu finanzieren, dann aber eigentlich auch immer müssen wir uns mit Search Engine Optimization (SEO) beschäftigen. Früher bestand die Optimierung für Suchmaschinen zum großen Teil darin, brav ein paar Schlüsselwörter (Keywords) in die Meta Informationen unseres Auftritts einzubauen. Das reicht heute bei weitem nicht mehr aus und die vergebenen Keywords werden sogar in der Regel von den gängigen Suchmaschinen gar nicht mehr beachtet. Dafür sind nun eine Vielzahl anderer Faktoren entscheidend dafür ob wir (gut) gefunden werden oder nicht. Einige der wichtigsten davon sind hier aufgelistet:\nDie Seite enthält Metainformationen, wie: Beschreibung, Titel, Bild, Link, usw. Die Seite hat einen längeren wirklichen Inhalt in Textform Möglichst alle Links auf der Seite haben eine Beschreibung (Text) Alle Bilder auf der Seite haben eine Bezeichnung (alt) die Seite ist z.B. über eine robots.txt Datei so eingestellt, dass sie von Suchmaschinen indiziert werden darf die robots.txt enthält, wenn vorhanden, keine Fehler Es werden keine “Mini” Schriftgrößen verwendet um Inhalte vorzutäuschen (eine Zwischenzeitlich verbreitete Masche unseriöser Anbieter) Die Seite / Der Beitrag enthält interne und externe Links zu anderen Seiten Wie man sieht zielen diese Kriterien darauf ab, dass neben den Metainformationen die auch für Soziale Netzwerke relevant sind, der komplette Seiteninhalt bei der Indizierung der Seite durch Suchmaschinen berücksichtigt wird. Das ist auch ein weiteres Argument, neben der Performance, Inhalte nicht dynamisch nachzuladen, weil eben diese dann von Google\u0026amp;Co nicht gefunden werden.\nSocial Media Aus Sicht eines Unternehmens heute beinahe überlebensnotwendig, aus Sicht einer Privatperson einfach eine hilfreiche Unterstützung um Inhalte bekannt zu machen und mit der Welt zu teilen, das ist Social Media Integration für eine Website. Die schon zuvor genannten Meta-Informationen ermöglichen es, dass beim Teilen eines Links automatisch ein Bild, eine Überschrift und eine Kurzbeschreibung generiert werden. Außerdem ermutigen Schaltflächen mit den Symbolen der verschiedenen Netzwerke in der Nähe eines Beitrags zum Teilen. Man kann auch berücksichtigen, dass je nach Endgerät unterschiedliche Kanäle genutzt werden. So macht ein Share Button zu WhatsApp in der Regel nur für mobile Endgeräte Sinn. Auch werden von unterschiedlichen Altersgruppen unterschiedliche Netzwerke bevorzugt. Dabei kann man sich hieran orientieren:\nFacebook: Wird primär von Menschen mittleren bis höheren Alters genutzt, ist aber wohl jedem als Pionier bekannt. Instagram: Junge Erwachsene bis Menschen mittleren Alters nutzen dieses Netzwerk hauptsächlich zum Teilen von Bildern und Geschichten dazu.\nTikTok: Für die ganz jungen Mitmenschen. Da wird wohl hauptsächlich getanzt, geklatscht und dazu gesungen, um das Ganze dann auch noch als Video zu teilen.\nTwitter(X): zum Austausch von Kurznachrichten und so richtig im sogenannten Arabischen Frühling weltweit bekannt geworden. Bis vor kurzem auch Stammsitz einer orangen Ente. Zielpublikum sind inzwischen eher Menschen mittleren bis höheren Alters.\nYouTube: Mit etwas Aufwand auch als Soziales Netzwerk nutzbar, wird dafür aber altersgruppenübergreifend genutzt\nWhatsApp: eher zum persönlichen Austausch in kleineren Zielgruppen, dafür aber ebenfalls altersgruppenübergreifend.\nNatürlich gibt es auch regionale Unterschiede bei der Nutzung der Netzwerke, so sieht in China oder Indien das Bild schon wieder ganz anders aus. Z.B. tritt dort der Anbieter WeChat mit seiner App als eierlegende Wollmilchsau auf.\nBrowser Kompatibilität Hier muss man eine Entscheidung treffen. Leider nutzen nach wie vor viele Menschen den völlig veralteten Internetbrowser “Internet Explorer” (IE11) von Microsoft. Dieser wird schon seit längerem nicht mehr von Microsoft weiterentwickelt und wurde durch “Edge” ersetzt. Da nun aber viele trotzdem noch den IE11 nutzen, ist die zu treffende Entscheidung: Will ich diese Besucher ebenfalls mit einer fehlerfreien Darstellung bedienen oder schließe ich sie aus, damit ich mich bei der Erstellung der Seite an aktuelle Standards halten kann und diverse Altlasten nicht berücksichtigen muss. Ich habe mich gegen die Unterstützung des IE11 entschieden, da selbst Microsoft nach Supportende von Windows 7, an welches der IE11 gekoppelt ist, diesen als Sicherheitsrisiko einzustufen plant. Die anderen üblichen Browser wie Google Chrome, Chromium und Microsoft Edge oder auch Apple Safari liefern stets gute Ergebnisse. Das Open Source Projekt Firefox der Mozilla Gruppe ist die Wahl des Nutzers mit Wunsch nach Unabhängigkeit. Auch Firefox stellt die meisten Inhalte des aktuellen HTML Standards korrekt dar und unterstützt nur z.B. einige exotischere Parameter in CSS nicht.\nUser Experience und Layout Für ein Unternehmen sicherlich nicht nur ein Unterpunkt für mich als Privatperson mehr eine Hilfe. Unter User Experience (UX) wird die komplette Berufserfahrung die der Besucher während des Besuchs unserer Website macht zusammengefasst. Dazu gehört das Layout oder auch User Interface (UI) unserer Seite als Kernelement. Sich wiederholende Elemente sollten auf allen (Unter)Seiten gleich aussehen und generell sollte während des kompletten Besuchs eine Wiedererkennungswert geschaffen werden. So sind Buttons immer an der gleichen Stelle zu finden, verhalten sich identisch und sehen gleich aus. Auch das Farbschema sowie die komplette Formatierung der Inhalte sollten durchgängig und ansprechend sein. Ich vertraue hierzu als Einstieg schon seit einigen Jahren auf das Open Source Projekt “Bootstrap” (Link) von Twitter. Dieses beinhaltet im wesentlichen vorgefertigte CCS (Styles) die nach belieben verwendet werden und ggfs. auch angepasst werden können. Bootstrap hatte zwischenzeitlich etwas an seinem guten Ruf eingebüßt, weil es lange auf das JavaScript Framework “jQuery” gesetzt hat. Mit der aktuellen Version 5 ist das Geschichte und wir freuen uns über einen kleineren Quellcode bei der Einbindung.\nAus Sicht des Betreibers In den vorherigen Kapiteln haben wir herausgefunden, was aus Sicht von Google bei der Ausspielung einer Website relevant ist. Natürlich haben wir aber neben den Anforderungen aus Besuchersicht auch noch sowohl als Autor als auch als Administrator weitere Anforderungen. Einige davon werde ich hier anführen, andere später in der Artikelserie auch noch tiefgreifender ausführen.\nServer Wie zuvor im Punkt Performance schon beschrieben ist ein schneller Server mit einer sehr guten Netzwerkanbindung ans Internet eines der Kernkriterien für eine performante Website. Anbieter für virtuelle oder physische Server gibt es viele. Einige der Bekanntesten sind Hetzner, 1blu, Netcup oder Strato. Man hat also die Qual der Wahl. Heute immer mehr üblich ist anstelle des Betriebs eines eigenen Servers, die Nutzung von Ressourcen eines Cloud Anbieters. Hier Stellen Google, Amazon und Microsoft die Platzhirsche dar. Aus Sicht des Datenschutzes stellen alle drei Anbieter mittlerweile auch Ressourcen aus Deutschland bereit und halten sich dementsprechend an die hier gültigen Regulierungen. Ich persönlich bin aber aufgrund der variablen Abrechnung kein Fan der Cloud Anbieter und bezahle lieber einen monatlichen fixen Betrag der sämtlichen Traffic sowie die Hardwarekosten beinhaltet.\nMonitoring \u0026amp; Alerting Das Monitoring, also die Überwachung des Serves ist nicht nur in Problemfällen wichtig, sondern hilft auch dabei Problemen bereits entgegen zur wirken bevor sie auftreten. Ich nutze zum Monitoring das mit dem CentOS Betriebsystem ausgelieferte Open Source Tool Cockpit, welches eine schicke Weboberfläche für die meisten anliegen liefert. So lassen sich dort automatische Updates und eine Langzeit-Ressourcenüberwachung inkl. Dashboard mit ein paar Mausklicks aktivieren.\nViele setzen ein Alerting, also die Alarmierung im Fehlerfall, mit einem Monitoring gleich. Das ist so nicht ganz korrekt. Ein Alerting kann durchaus völlig unabhängig vom Monitoring konfiguriert und genutzt werden. So nutze ich z.B. nur Alerts die mir eine E-Mail senden wenn meine Website keinen gültigen Inhalt zurück liefert. Wem das reicht, der braucht keine schwergewichtige Softwarelösung zu installieren. Wichtig ist nur: Die Überwachung sollte auf keinen Fall auf dem gleichen Serversystem wie unsere Website laufen. Der Grund ist klar: Wenn der Server ausfällt kann er mir auch keine E-Mails mit der Info über den Ausfall liefern… Deshalb kommen meine Mails von einem kleinen Raspberry PI der zuhause auch schon vorher seinen 24-Stunden Dienst verrichtet hat. Und im Normalfall kommen eigentlich auch gar keine Mails, weil der Server einfach nicht ausfällt 🙂\nDatensicherung Das eine Datensicherung zwingend erforderlich ist, braucht man heute glücklicherweise niemanden mehr zu erklären. Ich nutze für die Website genauso wie vor andere Komponenten auf meinem Server eine mehrschichtige Backupstrategie: Eine tägliche Spiegelung der Inhalte innerhalb der Servers lässt mich versehentlich gelöschte Inhalte sehr schnell wiederherstellen. Diese Spiegelung ist dann auch die Quelle um die Daten inkrementell abgesichert an den zuvor erwähnten Raspberry Pi zuhause zu übertragen. Genau genommen an ein dahinter liegendes Hardware RAID Plattensystem. So habe ich zumindest alles mir mögliche getan um sowohl das versehentliche Löschen, einen Datenverlust oder einem Datenträgerausfall entgegen zu wirken.\nDomain und DNS Die Adresse(n) unter denen wir im Internet erreichbar sein wollen werden als Domains bezeichnet. Diese sichert man sich für geringes Geld bei einer Vielzahl von Anbietern, wenn man einen kreativen und nicht bereits belegten Namen gefunden hat. Und genau da liegt die Schwierigkeit, die meisten “hübschen” Namen sind bereits vergeben und man muss wirklich kreativ werden. Eine Nacht darüber zu schlafen bevor man die Registrierung anstößt kann wirklich helfen.\nDer DNS Eintrag ist das Bindeglied im Internet zwischen unseren registrierten Domains und der IP-Adresse unseres Servers. Oft übernehmen Anbieter von Domains auch gleichzeitig die Registrierung und Verteilung des DNS Eintrags für genauso geringes Geld.\nSicherheit Neben dem eigentlich schon verpflichtenden Einsatz von Verschlüsselung über https://, welche auf meinem Server über das Open Source Tool “Nginx Proxy Manager” (Link) einrichte, kann man noch mehr für die Sicherheit von Besuchern und uns als Betreibern tun. Ein großer Mehrwert zur Sicherheit ist während der Umsetzung als Nebenprodukt entstanden. Da ich ausschließlich statische Seiten generiere und auf den Einsatz von Cookies verzichte, beschränkt sich der Besucher dieser Seiten fast vollständig auf den bloßen Download von Dateien ohne Nachladen weiterer Inhalte. Eine Ausnahme spielen die Assets (Bilder, Dokumente, etc.), aber dazu später mehr.\nAußerdem sollte natürlich der Zugang zum Server gesichert werden. So ist eine Firewall zu aktivieren und Standardports z.B. für SSH sind zu vermeiden. Auch empfiehlt es sich z.B. mit dem Open Source Tool Google Authenticator eine 2-Faktor Anmeldung am Server zu erzwingen. Eine Verschlüsselung der Daten auf dem Datenträgern des Servers ist, wenn möglich, sicherlich auch keine schlechte Idee. Für den Root user deaktivieren wir noch die Möglichkeit sich aus der Ferne anzumelden und führen selbst unsere Kommandos niemals direkt als dieser aus.\nContent Mangagment System Eine immer populärer werdende Praxis ist der Einsatz eines sogenannten Headless Content Management Systems (CMS) zur Datenhaltung, Verwaltung und ggfs. zum editieren von Beitragen wie diesem hier. Headless bedeutet dass das CMS die in ihm gepflegten Daten nicht direkt sondern nur über Webschnittstellen bereit stellt. Das hat, neben den Vorteilen der Modularisierung und Unabhängigkeit, den Vorzug dass wir unsere Inhalte zur Generierung von statischen Seiten an jedem Punkt unserer Prozesses abfragen können ohne dabei auf Limitierungen des CMS selbst gefesselt zu sein. Ich nutze hierfür das Open Source Tool “Cockpit” (Link), und bin nach einer kurzen Einarbeitungszeit von vielleicht 6 Stunden hellauf begeistert. Das einzig verwirrende ist der Name. Wir erinnern uns: Unser Monitoring Tool heisst ebenfalls Cockpit. Die beiden haben so rein gar nichts miteinander zu tun, sondern teilen sich einfach den Namen.\nAsset Management Im Falle unserer Website sind unsere Assets vornehmlich Bilder. Und diese möchten wir gerne, ähnlich wie unsere Inhalte, autark gespeichert und verwaltet wissen. Außerdem soll unser Asset Management System dazu in der Lage sein, aus den von uns hochgeladenen Bildern automatisch Miniaturen (Thumbnails) zu erstellen. Glücklicherweise kann unser CMS System “Cockpit” (Link) das alles von Haus aus und bietet für den Zugriff auch noch Web Services, mit deren Verwertung wir bereits bekannt sind. In Cockpit ist das Asset Management ein vom CMS getrennter Bereich der aber von dort aus auch angesprochen werden kann, so dass wir Bilder beim Schreiben eines Artikels direkt hochladen und einfügen können.\nBackend Oft habe ich in diesem Artikel über die Generierung von statischen Seiten gesprochen. Aber womit machen wir das eigentlich? Hier bin ich einen unüblichen Weg gegangen, und verwende das Open Source Tool Node-RED (Link). Node-RED ist eine sogenannte Low-Code Programmierumgebung auf die über den Browser zugegriffen wird. Low-Code heisst das man Abläufe von einfacher bis mittlerer Komplexität ohne eigentliche Programmierung abbilden kann. Allerdings lassen sich auch jederzeit eigene Funktionen einbinden, wovon ich auch häufig gebrauch machen. Verschiedene Prozessschritte lassen sich grafisch miteinander verbinden, so dass als Nebenprodukt gleich noch ein Ablaufdiagramm entsteht. Ich nutze das Tool schon seit mehren Jahren z.B. zur Heimautomatisierung, was die Vielseitigkeit widerspiegelt. Es gibt aber auch noch eine Vielzahl anderer Tools die auf die Generierung von statischen Websites spezialisiert sind.\nReporting \u0026amp; Web Analytics Wenn man so viel Arbeit in die Erstellung von Inhalten und Struktur steckt, möchte man natürlich auch wissen ob und wie diese bei den Besuchern ankommen. Auch wie viele Besucher es überhaupt in welchem Zeitraum gibt und ob sie längere Zeit auf unseren Seiten bleiben interessiert uns. Wenn unsere Inhalte außerdem in mehreren Sprachen oder zumindest auch in Englisch oder Spanisch angeboten werden, dann möchten wir auch wissen woher die Besucher kommen. Um all diese Fragen zu beantworten gibt es sogenannte Web Analytics Tool. Solch ein Tool bereitet die Antworten auf diese Fragestellungen in der Regel auch gleich noch grafisch auf. Das bekannteste unter ihnen ist sicherlich Google Analytics, welches sich kostenfrei in eine Website einbinden lässt. Allerdings ist Google Analytics aus Gesichtspunkten des Datenschutzes nicht besonders sparsam und lässt sich auch nicht selbst betreiben, sondern greift auf die Google Cloud Services zurück. Als Freund des Datenschützers hat sich hingegen das Open Source Tool “Matomo” (Link) erwiesen. Außerdem lässt es sich selbst betreiben, ist flink und geht auf alle genannten Bedürfnisse ein. In Matomo lässt sich auch einstellen, dass Daten nur anonymisiert erhoben werden und z.B. keine IP-Adressen der Besucher gespeichert werden.\nDatenschutz \u0026amp; Rechtliche Bestimmungen Wo wir schon das Thema Datenschutz angesprochen haben, ohne darauf gegenüber dem Webseitenbenutzer einzugehen geht es in Deutschland bzw. der EU nicht. Das ich mich gegen den Einsatz jeglicher Art von Cookies entscheide, vereinfacht diesen Teil aber ungemein (Stichwort: Cookie-Madness). So muss auf der gesamten Webseite aber trotzdem ein stets und einfach auffindbarer Bereich/Link zu den Datenschutzbestimmungen verfügbar gemacht werden. Das gute ist aber: Datenschützer sind keine Unmenschen, im Gegenteil sogar. Sie wollen uns nur vor unlauteren Geschäftspraktiken der Betreiber schützen und sind gegen lange komplizierte “Paragraphenauflisterei”. Das heisst, dass die Datenschutzbestimmungen möglichst kurz und verständlich formuliert sein sollen. Sie sollten außerdem keine Abschnitte enthalten die für die Website gar nicht relevant sind und im Wesentlichen folgendes enthalten:\nWelche persönlichen Daten werden gespeichert? Wo werden die Daten gespeichert? Von wem werden die Daten gespeichert? Wer ist Verantwortlicher Ansprechpartner? Wer darf die Daten auswerten? Welche Drittparteien sind evtl. involviert? Wie kann eine Löschung meiner evtl. vom Betreiber gespeicherten Daten initiiert werden? Wie gesagt müssen aus der Liste nicht alle Punkte relevant sein, z.B. wenn überhaupt keine personalisierten Daten gespeichert werden. Das heisst dass vorgefertigte Texte nicht unbedingt gut sind und vor allem meistens Passagen enthalten die gekürzt/entfernt werden sollten. Natürlich bin ich kein Anwalt und die rechtlichen Grundlagen ändern sich zur Zeit quartalsweise, aber wenn man sich an diese Grundlagen zum Datenschutz hält ist man auf jeden Fall gute dabei und handelt bedachter als viele andere Webseitenbetreiber.\nAus rechtlicher Sicht muss ebenso immer eine Impressum verfügbar sein. Hier kann man sich durchaus an Vorlagen aus dem Internet bedienen, da es sich im Gegensatz zur Datenschutzerklärung hier eher um unsere Absicherung gegenüber dem Nutzer und keine Dienstleistung für ihn handelt. Pflichtangaben sind:\nVerantwortlicher für das Angebot gemäß § 5 TMG / § 18 MStV Inhaltlich Verantwortlicher gemäß § 18 Abs. 2 MStV Beide Angaben sollten den vollständigen Namen, ein Adresse und zumindest eine E-Mail Anschrift enthalten. Hier zählen leider keine Argumente aus Sicht des Datenschutzes für den Betreiber…\nFazit und Ausblick Wir kennen nun viele Kriterien die eine perfekte Website ausmachen und können bereits sagen, dass sich diese so mit einem Homepagebaukasten oder einem Web Content Managementsystem wie z.B. WordPress nicht umsetzen lassen. In den Folgeartikeln meiner Serie werden wir allerdings auch sehen, dass zur Umsetzung teilweise tiefgreifendes IT-Verständnis benötigt wird. Deshalb Rate ich allen die sich nicht so tief mit dieser Materie auseinander setzen möchten trotzdem zum Einsatz von WordPress. Bei WordPress handelt es sich um ein millionenfach eingesetztes WCMS mit dem man schnell zu passablen Ergebnissen kommen kann und der Umsetzungsaufwand sich auch von Laien bewältigen lässt.\nZusammengefasst ist folgender Anforderungskatalog:\nHosting (Bereitstellung) auf einem flinken Server als Basis Eine performante Netzwerkanbindung dieses Servers an das Internet DNS Einträge unser Domain(s)beim einem vertrauenswürdigem Anbieter Virtualisierung einzelner Komponenten in sogenannten Containern Server Monitoring mit Cockpit (nicht das CMS Tool) Alerting mit einer geeigneten schlanken Lösung Eine Reverse Proxy Konfiguration inkl. Zertifikatsmanagement mit NGINX Proxy Manager Ein Backend auf Basis von Node-Red zur Generierung statischer Seiten Content und Asset Management mit Cockpit (nicht das Monitoring Tool) Web Analytics mit Matomo Einen Texteditor und Zugriff auf eine Shell mit Microsoft Visual Studio Code HTML5 Quellcode mit Unterstützung von Twitter Bootstrap 5 (also ohne jQuery) Komponenten Individuelle Stylesheets mit CSS Variablen Vanilla (keine Framework wie jQuery oder Angular) Javascript im Frontend In dieser Reihenfolge werde ich die Themen in den folgenden Artikeln auch beschreiben. Um noch etwas Spannung aufzubauen: Eine Lighthouse Berwertung von 100% Punkten in allen Bereichen lässt sich umsetzen, diese Seite hier ist der Beweis :-).\n","tags":["deutsch"],"section":"blog"},{"date":"1652313600","url":"/blog/move-3-trackpad/","title":"Reanimation: Auf MacOS mit 3 Fingern Fenster schieben","summary":"Aus einem mir nicht erklärlichen Grund, ist auf dem Mac seit geraumer Zeit das verschieben von Fenstern und Objekten mittels 3-Finger-Geste nicht mehr im Standard aktiviert. Das scheint nicht nicht nur mich gestört zu haben, da Apple einen relativ frisch aktualiserten Artikel zur Wiederherstellung dieses Verhaltens bereit stellt. Inhalt siehe hier:\nAnleitung Wähle das Apple-Menü () \u0026gt; „Systemeinstellungen“. Klicke auf „Bedienungshilfen“. Klicke auf „Zeigersteuerung“ oder „Maus \u0026amp; Trackpad“. Klicke auf die Taste „Trackpad-Optionen“. Aktiviere die Option „Trackpad zum Ziehen verwenden“ oder „Bewegen aktivieren“. Wähle im Einblendmenü für den Zugstil die Option „Mit drei Fingern bewegen“ aus. Klicke auf „OK“. ","content":"Aus einem mir nicht erklärlichen Grund, ist auf dem Mac seit geraumer Zeit das verschieben von Fenstern und Objekten mittels 3-Finger-Geste nicht mehr im Standard aktiviert. Das scheint nicht nicht nur mich gestört zu haben, da Apple einen relativ frisch aktualiserten Artikel zur Wiederherstellung dieses Verhaltens bereit stellt. Inhalt siehe hier:\nAnleitung Wähle das Apple-Menü () \u0026gt; „Systemeinstellungen“. Klicke auf „Bedienungshilfen“. Klicke auf „Zeigersteuerung“ oder „Maus \u0026amp; Trackpad“. Klicke auf die Taste „Trackpad-Optionen“. Aktiviere die Option „Trackpad zum Ziehen verwenden“ oder „Bewegen aktivieren“. Wähle im Einblendmenü für den Zugstil die Option „Mit drei Fingern bewegen“ aus. Klicke auf „OK“. ","tags":["deutsch"],"section":"blog"},{"date":"1641081600","url":"/blog/ks-four-seasons/","title":"Vier Jahreszeiten im Kasseler Bergpark","summary":"Seit nun über einem Jahr leben wir mit dem Kasseler Bergpark direkt nebenan. In diesem Beitrag möchte ich gerne die Impressionen und Sehenswürdigkeiten, die sich im Lauf der Jahreszeiten bieten, teilen.\nDas im Artikel beschriebene Gebiet bezieht sich auf ein größeres Areal im äußeren Westen Kassels, welches sich, auch wenn man alle Highlights sehen möchte, gut in einem oder zwei Tagesausflügen fußläufig erkunden lässt. Eine Übersicht über das Gebiet lässt sich hier finden: Link zu OpenStreetMap.\n","content":"Seit nun über einem Jahr leben wir mit dem Kasseler Bergpark direkt nebenan. In diesem Beitrag möchte ich gerne die Impressionen und Sehenswürdigkeiten, die sich im Lauf der Jahreszeiten bieten, teilen.\nDas im Artikel beschriebene Gebiet bezieht sich auf ein größeres Areal im äußeren Westen Kassels, welches sich, auch wenn man alle Highlights sehen möchte, gut in einem oder zwei Tagesausflügen fußläufig erkunden lässt. Eine Übersicht über das Gebiet lässt sich hier finden: Link zu OpenStreetMap.\nDer Wikipedia Artikel zum Bergpark Wilhelmshöhe liefert Interessierten viele weitere Details, zu dem seit 2013 als UNESCO Welterbe definierten Areal. Hier möchte ich aber mehr auf die persönlichen und subjektiven Eindrücke, als auf objektive Fakten eingehen.\nEinmal alles neu im Frühling Wenn Mutter Natur langsam wieder aus dem Winterschlaf erwacht, wird auch das Treiben im Bergpark wieder lauter. An jeder Ecke sieht man das Grün sprießen und die Tiere aus ihren Verstecken hervorkommen. Auch die Menschen scheinen frische Energie getankt zu haben und Ihre guten Vorsätze für das Neue Jahr sind wohl auch noch nicht ganz beiseite gelegt. Man sieht es an Ihren Gesichtsausdrücken, wenn sie einem beim Spaziergang den Weg kreuzen. Das Bild zum Abschnitt ist auch gleich eine der häufigst gemachten Aufnahmen im Bergpark: der Blick vom Herkules herunter auf die Stadt. Die Stufen sind wohl auch der Ort im Park, an dem man den anderen Besuchern gezwungenermaßen am nächsten kommt. Wer das oder generell Treppen nicht mag, kann diese beim Abstieg auch meiden, in dem er den Seitenpfad von oben aus rechts gesehen wählt.\nSommer, Sonne, Sonnenschein Warme Temperaturen, lebendige Gerüche und reges Treiben sorgen auch im Bergpark für gute Laune. Zu dieser Jahreszeit sieht man auch die meisten Touristen hier, welche aus aller Welt herbeiströmen. So erkennt man beim Lauschen deren Unterhaltungen vornehmlich französisch, spanisch, englisch, türkisch und viele der deutschen Dialekte. Im frühen Sommer lohnt es sich besonders eine Picknickdecke mitzubringen, da die Gräser auf den vielen Wiesen noch nicht so hoch sind und der Schäfer seine Schafe noch nicht regelmäßig durch den Park führt. Ein ruhiges Plätzchen ohne Passanten findet sich dann recht schnell. Das mit den Schafen hat insofern mit der Gastlichkeit der Wiesen zu tun, als dass die Tiere neben interessanten Gerüchen auch diverse Ausscheidungen hinerlassen. Das viele der Wiesen nicht gemäht werden, dient natürlich dem Naturschutz. Vor dem Schloß liegt aber eine sehr große und zentrale Grasfläche, die den ganzen Sommer über zum Sonnen und Liegen einlädt. Dort ist es dann aber natürlich nicht ganz so still und es ist dementsprechend schwerer der Natur zuzuhören. Das Bild ist auf dem Weg vom Schloß, links vorbei am Steinhöfer Wasserfall, auf dem Weg zur Löwenburg entstanden.\nMelancholischer Herbstblues Wenn die Tage wieder kürzer werden, sind die meisten Parkbesucher entweder einheimische oder Menschen aus der näheren Umgebung. Zu dieser Jahreszeit erinnern mich viele Teile des Bergparks an ein irisches Landschaftsbild. So z.B. auch der auf dem Bild zu sehende Wasserverlauf, wo eigentlich nur noch ein paar Kobolde fehlen. Auch im späteren Herbst verschwindet das Grün des Parks nie vollständig. Entsprechende Kleidung vorausgesetzt, kann einem ein Besuch einer der Wasserfälle, gerade bei Regen, die Natur und ein ganz besonderes Gefühl der Verbundenheit näher gebracht werden. Die im deutschen Herbst üblichen Farbenspiele gibt es natürlich trotzdem, besonders in den Bereichen in denen viele Laubbäume stehen (wo auch sonst).\nIm Winterschlaf Wenn man sich trotz Lethargie und Müdigkeit zu einem Spaziergang im Park aufraffen kann, wird man dafür stets belohnt. Es ist interessant zu sehen, dass eben doch nicht alle Tiere in wärmere Regionen gezogen sind und auch die Pflanzenwelt nicht völlig ruht. Aufgrund der höheren Lagen im Vergleich zum Rest Kassels, stehen die Chancen auf Schnee bei entsprechenden Temperaturen nicht schlecht. Einmal ins Winterkleid eingehüllt erscheinen uns die vorher noch so anders bekannten Szenenbilder auf einmal so als seien sie aus einer finsteren mittelalterlichen TV-Serie wie “Game of Thrones” entnommen. Die Baukräne, zuerst am Herkules und später an der Löwenburg, muss man sich dafür natürlich weg denken.\nFazit Ich hoffe ich konnte evtl. einige noch unbekannte Aspekte unseres schönen Berkparks aufzeigen und werde diesen Beitrag in Zukunft noch um eine Bilderserie ergänzen. Zum Schluss noch die Bitte um einen kleinen Gefallen: Wer sich zufällig vor Ort wiederfindet, der möge doch bitte kurz Laut “Max” und/oder “Sarina” rufen. Im Falle einer Antwort, bitte ich um eine kurze, sowie persönliche Nachricht an mich.\n","tags":["andere"],"section":"blog"}]